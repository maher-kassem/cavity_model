{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from Bio.PDB.Polypeptide import index_to_one\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from cavity_model import (\n",
    "    CavityModel,\n",
    "    ResidueEnvironment,\n",
    "    ResidueEnvironmentsDataset,\n",
    ")\n",
    "from helpers import (\n",
    "    _augment_with_reverse_mutation,\n",
    "    _populate_dfs_with_nlls_and_nlfs,\n",
    "    _populate_dfs_with_resenvs,\n",
    "    _train_loop,\n",
    "    _train_val_split,\n",
    "    _get_ddg_training_dataloaders,\n",
    "    _get_ddg_validation_dataloaders,\n",
    "    _train_downstream_and_evaluate,\n",
    ")\n",
    "from visualization import scatter_pred_vs_true, plot_validation_performance\n",
    "\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cavity Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and process Cavity Model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hack to find the conda activate path since bash scripts \n",
    "# don't necessarily work with the conda activate command\n",
    "conda_path = !which conda\n",
    "conda_path = list(conda_path)[0]\n",
    "conda_activate_path = pathlib.Path(conda_path).parent.parent / \"bin\" / \"activate\"\n",
    "if not conda_activate_path.is_file():\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find your conda activate path needed for running bash scripts.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run shell script that takes a .txt file with PDBIDs as input.\n",
    "!./get_and_parse_pdbs_for_cavity_model.sh $conda_activate_path data/pdbids_2336.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main parameters\n",
    "DEVICE = \"cuda\"  # \"cpu\" or \"cuda\"\n",
    "TRAIN_VAL_SPLIT = 0.8\n",
    "BATCH_SIZE = 100\n",
    "SHUFFLE_PDBS = True\n",
    "LEARNING_RATE = 3e-4\n",
    "EPOCHS = 10\n",
    "PATIENCE_CUTOFF = 1\n",
    "EPS = 1e-9\n",
    "\n",
    "# Parameters specific to downstream model\n",
    "BATCH_SIZE_DDG = 40\n",
    "SHUFFLE_DDG = True\n",
    "LEARNING_RATE_DDG = 1e-3\n",
    "EPOCHS_DDG = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Parsed PDBs and perform train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parsed_pdb_filenames = sorted(glob.glob(\"data/pdbs/parsed/*coord*\"))\n",
    "if SHUFFLE_PDBS:\n",
    "    random.shuffle(parsed_pdb_filenames)\n",
    "dataloader_train, dataset_train, dataloader_val, dataset_val = _train_val_split(\n",
    "    parsed_pdb_filenames, TRAIN_VAL_SPLIT, DEVICE, BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the cavity model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "cavity_model_net = CavityModel(DEVICE).to(DEVICE)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cavity_model_net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Create directory for model files\n",
    "models_dirpath = \"cavity_models/\"\n",
    "if not os.path.exists(models_dirpath):\n",
    "    os.mkdir(models_dirpath)\n",
    "\n",
    "# Train loop\n",
    "best_model_path = _train_loop(\n",
    "    dataloader_train,\n",
    "    dataloader_val,\n",
    "    cavity_model_net,\n",
    "    loss_function,\n",
    "    optimizer,\n",
    "    EPOCHS,\n",
    "    PATIENCE_CUTOFF,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ddG Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse PDBs for DMS, Guerois and Protein G data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse PDBs for which we have ddG data\n",
    "!./get_and_parse_pdbs_for_dowstream_task.sh $conda_activate_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make dict for residue environments for easy look up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary residue environment datasets as dicts to more easily match ddG data\n",
    "parsed_pdbs_wildcards = {\n",
    "    \"dms\": \"data/data_dms/pdbs_parsed/*coord*\",\n",
    "    \"protein_g\": \"data/data_protein_g/pdbs_parsed/*coord*\",\n",
    "    \"guerois\": \"data/data_guerois/pdbs_parsed/*coord*\",\n",
    "    \"symmetric\": \"data/data_symmetric/pdbs_parsed/*coord*\",\n",
    "}\n",
    "\n",
    "resenv_datasets_look_up = {}\n",
    "for dataset_key, pdbs_wildcard in parsed_pdbs_wildcards.items():\n",
    "    parsed_pdb_filenames = sorted(glob.glob(pdbs_wildcard))\n",
    "    dataset = ResidueEnvironmentsDataset(parsed_pdb_filenames, transformer=None)\n",
    "    dataset_look_up = {}\n",
    "    for resenv in dataset:\n",
    "        key = (\n",
    "            f\"{resenv.pdb_id}{resenv.chain_id}_{resenv.pdb_residue_number}\"\n",
    "            f\"{index_to_one(resenv.restype_index)}\"\n",
    "        )\n",
    "        dataset_look_up[key] = resenv\n",
    "    resenv_datasets_look_up[dataset_key] = dataset_look_up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ddG data to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddg_data_dict = OrderedDict()\n",
    "ddg_data_dict = {\n",
    "    \"dms\": pd.read_csv(\"data/data_dms/ddgs_parsed.csv\"),\n",
    "    \"protein_g\": pd.read_csv(\"data/data_protein_g/ddgs_parsed.csv\"),\n",
    "    \"guerois\": pd.read_csv(\"data/data_guerois/ddgs_parsed.csv\"),\n",
    "    \"symmetric_direct\": pd.read_csv(\"data/data_symmetric/ddgs_parsed_direct.csv\"),\n",
    "    \"symmetric_inverse\": pd.read_csv(\"data/data_symmetric/ddgs_parsed_inverse.csv\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate dataframes with wt ResidueEnvironment objects and wt and mt restype indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_populate_dfs_with_resenvs(ddg_data_dict, resenv_datasets_look_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate dataframes with predicted NLLs and isolated WT and MT predicted NLLs as well as NLFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load best performing cavity model from previos training\n",
    "cavity_model_infer_net = CavityModel(DEVICE).to(DEVICE)\n",
    "cavity_model_infer_net.load_state_dict(torch.load(best_model_path))\n",
    "cavity_model_infer_net.eval()\n",
    "\n",
    "_populate_dfs_with_nlls_and_nlfs(\n",
    "    ddg_data_dict, cavity_model_infer_net, DEVICE, BATCH_SIZE, EPS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results without downstream model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors = [\"steelblue\", \"firebrick\", \"forestgreen\", \"black\", \"black\"]\n",
    "for color, data_key in zip(colors, ddg_data_dict.keys()):\n",
    "    fig, ax = scatter_pred_vs_true(\n",
    "        ddg_data_dict[data_key][\"ddg\"],\n",
    "        ddg_data_dict[data_key][\"ddg_pred_no_ds\"],\n",
    "        color=color,\n",
    "        title=data_key,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge direct and inverse dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rename columns so they specify if it is the direct or inverse direction\n",
    "symmetric_direct_df = ddg_data_dict[\"symmetric_direct\"]\n",
    "symmetric_direct_df.columns = [\n",
    "    name + \"_dir\" if \"_dir\" not in name else name\n",
    "    for name in symmetric_direct_df.columns\n",
    "]\n",
    "symmetric_inverse_df = ddg_data_dict[\"symmetric_inverse\"]\n",
    "symmetric_inverse_df.columns = [\n",
    "    name + \"_inv\" if \"_inv\" not in name else name\n",
    "    for name in symmetric_inverse_df.columns\n",
    "]\n",
    "\n",
    "# Inner merge both dataframes\n",
    "ddg_data_dict[\"symmetric_both\"] = pd.merge(\n",
    "    symmetric_direct_df,\n",
    "    symmetric_inverse_df,\n",
    "    how=\"inner\",\n",
    "    left_on=\"merge_column_dir\",\n",
    "    right_on=\"merge_column_inv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make prediction based on both structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symmetric_both_df = ddg_data_dict[\"symmetric_both\"]\n",
    "symmetric_both_df[\"ddg_pred_no_ds_both_dir\"] = symmetric_both_df.apply(\n",
    "    lambda row: 0.5 * (row[\"ddg_pred_no_ds_dir\"] - row[\"ddg_pred_no_ds_inv\"]), axis=1\n",
    ")\n",
    "symmetric_both_df[\"ddg_pred_no_ds_both_inv\"] = symmetric_both_df.apply(\n",
    "    lambda row: 0.5 * (row[\"ddg_pred_no_ds_inv\"] - row[\"ddg_pred_no_ds_dir\"]), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot prediction using both structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction for ddg direct using both structures\n",
    "fig, ax = scatter_pred_vs_true(\n",
    "    ddg_data_dict[\"symmetric_both\"][\"ddg_dir\"],\n",
    "    ddg_data_dict[\"symmetric_both\"][\"ddg_pred_no_ds_both_dir\"],\n",
    "    color=\"black\",\n",
    "    title=\"Both structure (Direct)\",\n",
    ")\n",
    "\n",
    "# Plot prediction for ddg inverse using both structures\n",
    "fig, ax = scatter_pred_vs_true(\n",
    "    ddg_data_dict[\"symmetric_both\"][\"ddg_inv\"],\n",
    "    ddg_data_dict[\"symmetric_both\"][\"ddg_pred_no_ds_both_inv\"],\n",
    "    color=\"black\",\n",
    "    title=\"Both structure (Inverse)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downstream model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance without augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training dataloader and eval dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddg_dataloaders_train_dict = _get_ddg_training_dataloaders(\n",
    "    ddg_data_dict, BATCH_SIZE_DDG, SHUFFLE_DDG\n",
    ")\n",
    "ddg_dataloaders_val_dict = _get_ddg_validation_dataloaders(ddg_data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and report on the data that is not used during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pearsons_r_results_dict = _train_downstream_and_evaluate(\n",
    "    ddg_dataloaders_train_dict,\n",
    "    ddg_dataloaders_val_dict,\n",
    "    DEVICE,\n",
    "    LEARNING_RATE_DDG,\n",
    "    EPOCHS_DDG,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in pearsons_r_results_dict.keys():\n",
    "    _ = plot_validation_performance(key, pearsons_r_results_dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance with augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training we \"augment\" our dataset simply by adding the reverse mutation with -ddG value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ddg_data_dict_augmented = _augment_with_reverse_mutation(ddg_data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training dataloader (augmented data) and eval dataloaders (original data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddg_dataloaders_train_dict = _get_ddg_training_dataloaders(\n",
    "    ddg_data_dict_augmented, BATCH_SIZE_DDG, SHUFFLE_DDG\n",
    ")\n",
    "ddg_dataloaders_val_dict = _get_ddg_validation_dataloaders(ddg_data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and report on the data that is not used during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pearsons_r_results_dict = _train_downstream_and_evaluate(\n",
    "    ddg_dataloaders_train_dict,\n",
    "    ddg_dataloaders_val_dict,\n",
    "    DEVICE,\n",
    "    LEARNING_RATE_DDG,\n",
    "    EPOCHS_DDG,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in pearsons_r_results_dict.keys():\n",
    "    _ = plot_validation_performance(key, pearsons_r_results_dict[key])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
