{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"import glob\\nimport os\\nimport random\\nfrom typing import Callable, List, Union\\n\\nimport numpy as np\\nimport torch\\nfrom torch.utils.data import DataLoader, Dataset\\n\\nfrom cavity_model import (\\n    ResidueEnvironment,\\n    ResidueEnvironmentsDataset,\\n    ToTensor,\\n    CavityModel,\\n)\\n\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"import glob\\nimport os\\nimport random\\nfrom typing import Callable, List, Union\\n\\nimport numpy as np\\nimport torch\\nfrom torch.utils.data import DataLoader, Dataset\\n\\nfrom cavity_model import (\\n    ResidueEnvironment,\\n    ResidueEnvironmentsDataset,\\n    ToTensor,\\n    CavityModel,\\n)\\n\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "from typing import Callable, List, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from cavity_model import (\n",
    "    ResidueEnvironment,\n",
    "    ResidueEnvironmentsDataset,\n",
    "    ToTensor,\n",
    "    CavityModel,\n",
    ")\n",
    "\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and process Cavity Model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded 4X2U.pdb to data/pdbs/raw/4X2U.pdb. 1/10.\n",
      "Successfully downloaded 2X96.pdb to data/pdbs/raw/2X96.pdb. 2/10.\n",
      "Successfully downloaded 4MXD.pdb to data/pdbs/raw/4MXD.pdb. 3/10.\n",
      "Successfully downloaded 3E9L.pdb to data/pdbs/raw/3E9L.pdb. 4/10.\n",
      "Successfully downloaded 1UWC.pdb to data/pdbs/raw/1UWC.pdb. 5/10.\n",
      "Successfully downloaded 4BGU.pdb to data/pdbs/raw/4BGU.pdb. 6/10.\n",
      "Successfully downloaded 2YSW.pdb to data/pdbs/raw/2YSW.pdb. 7/10.\n",
      "Successfully downloaded 4OW4.pdb to data/pdbs/raw/4OW4.pdb. 8/10.\n",
      "Successfully downloaded 2V5E.pdb to data/pdbs/raw/2V5E.pdb. 9/10.\n",
      "Successfully downloaded 1IXH.pdb to data/pdbs/raw/1IXH.pdb. 10/10.\n",
      "Successfully cleaned data/pdbs/raw/1IXH.pdb and added it to data/pdbs/cleaned/. 1/10.\n",
      "Successfully cleaned data/pdbs/raw/1UWC.pdb and added it to data/pdbs/cleaned/. 2/10.\n",
      "Successfully cleaned data/pdbs/raw/2V5E.pdb and added it to data/pdbs/cleaned/. 3/10.\n",
      "Successfully cleaned data/pdbs/raw/2X96.pdb and added it to data/pdbs/cleaned/. 4/10.\n",
      "Successfully cleaned data/pdbs/raw/2YSW.pdb and added it to data/pdbs/cleaned/. 5/10.\n",
      "Successfully cleaned data/pdbs/raw/3E9L.pdb and added it to data/pdbs/cleaned/. 6/10.\n",
      "Successfully cleaned data/pdbs/raw/4BGU.pdb and added it to data/pdbs/cleaned/. 7/10.\n",
      "Successfully cleaned data/pdbs/raw/4MXD.pdb and added it to data/pdbs/cleaned/. 8/10.\n",
      "Successfully cleaned data/pdbs/raw/4OW4.pdb and added it to data/pdbs/cleaned/. 9/10.\n",
      "Successfully cleaned data/pdbs/raw/4X2U.pdb and added it to data/pdbs/cleaned/. 10/10.\n",
      "Successfully parsed 1IXH_clean.pdb and moved parsed file to data/pdbs/parsed. Finished 1/10.\n",
      "Successfully parsed 1UWC_clean.pdb and moved parsed file to data/pdbs/parsed. Finished 2/10.\n",
      "Successfully parsed 2V5E_clean.pdb and moved parsed file to data/pdbs/parsed. Finished 3/10.\n",
      "Successfully parsed 2X96_clean.pdb and moved parsed file to data/pdbs/parsed. Finished 4/10.\n",
      "Successfully parsed 2YSW_clean.pdb and moved parsed file to data/pdbs/parsed. Finished 5/10.\n",
      "Successfully parsed 3E9L_clean.pdb and moved parsed file to data/pdbs/parsed. Finished 6/10.\n",
      "Successfully parsed 4BGU_clean.pdb and moved parsed file to data/pdbs/parsed. Finished 7/10.\n",
      "Successfully parsed 4MXD_clean.pdb and moved parsed file to data/pdbs/parsed. Finished 8/10.\n",
      "Successfully parsed 4OW4_clean.pdb and moved parsed file to data/pdbs/parsed. Finished 9/10.\n",
      "Successfully parsed 4X2U_clean.pdb and moved parsed file to data/pdbs/parsed. Finished 10/10.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# Run shell script that takes a .txt file with PDBIDs as input.\\n!./get_parse_pdbs_cavity_model.sh data/pdbids_010.txt\";\n",
       "                var nbb_formatted_code = \"# Run shell script that takes a .txt file with PDBIDs as input.\\n!./get_parse_pdbs_cavity_model.sh data/pdbids_010.txt\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run shell script that takes a .txt file with PDBIDs as input.\n",
    "!./get_parse_pdbs_cavity_model.sh data/pdbids_010.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables for Cavity Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"DEVICE = \\\"cuda\\\"  # \\\"cpu\\\" or \\\"cuda\\\"\\nTRAIN_VAL_SPLIT = 0.8\\nBATCH_SIZE = 100\\nLEARNING_RATE = 3e-4\\nEPOCHS = 10\\nPATIENCE_CUTOFF = 2\";\n",
       "                var nbb_formatted_code = \"DEVICE = \\\"cuda\\\"  # \\\"cpu\\\" or \\\"cuda\\\"\\nTRAIN_VAL_SPLIT = 0.8\\nBATCH_SIZE = 100\\nLEARNING_RATE = 3e-4\\nEPOCHS = 10\\nPATIENCE_CUTOFF = 2\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DEVICE = \"cuda\"  # \"cpu\" or \"cuda\"\n",
    "TRAIN_VAL_SPLIT = 0.8\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 3e-4\n",
    "EPOCHS = 10\n",
    "PATIENCE_CUTOFF = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse PDBs and train/val split for Cavity Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data set includes 8 pdbs with 4405 environments.\n",
      "Validation data set includes 2 pdbs with 853 environments.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"parsed_pdb_filenames = sorted(glob.glob(\\\"data/pdbs/parsed/*coord*\\\"))\\nrandom.shuffle(parsed_pdb_filenames)\\n\\nn_train_pdbs = int(len(parsed_pdb_filenames) * TRAIN_VAL_SPLIT)\\nfilenames_train = parsed_pdb_filenames[:n_train_pdbs]\\nfilenames_val = parsed_pdb_filenames[n_train_pdbs:]\\n\\nto_tensor_transformer = ToTensor(DEVICE)\\n\\ndataset_train = ResidueEnvironmentsDataset(\\n    filenames_train, transformer=to_tensor_transformer\\n)\\ndataset_val = ResidueEnvironmentsDataset(\\n    filenames_val, transformer=to_tensor_transformer\\n)\\n\\n\\ndataloader_train = DataLoader(\\n    dataset_train,\\n    batch_size=BATCH_SIZE,\\n    shuffle=True,\\n    collate_fn=to_tensor_transformer.collate_cat,\\n    drop_last=True,\\n)\\ndataloader_val = DataLoader(\\n    dataset_val,\\n    batch_size=BATCH_SIZE,\\n    shuffle=True,\\n    collate_fn=to_tensor_transformer.collate_cat,\\n    drop_last=True,\\n)\\n\\nprint(\\n    f\\\"Training data set includes {len(filenames_train)} pdbs with \\\"\\n    f\\\"{len(dataset_train)} environments.\\\"\\n)\\nprint(\\n    f\\\"Validation data set includes {len(filenames_val)} pdbs with \\\"\\n    f\\\"{len(dataset_val)} environments.\\\"\\n)\";\n",
       "                var nbb_formatted_code = \"parsed_pdb_filenames = sorted(glob.glob(\\\"data/pdbs/parsed/*coord*\\\"))\\nrandom.shuffle(parsed_pdb_filenames)\\n\\nn_train_pdbs = int(len(parsed_pdb_filenames) * TRAIN_VAL_SPLIT)\\nfilenames_train = parsed_pdb_filenames[:n_train_pdbs]\\nfilenames_val = parsed_pdb_filenames[n_train_pdbs:]\\n\\nto_tensor_transformer = ToTensor(DEVICE)\\n\\ndataset_train = ResidueEnvironmentsDataset(\\n    filenames_train, transformer=to_tensor_transformer\\n)\\ndataset_val = ResidueEnvironmentsDataset(\\n    filenames_val, transformer=to_tensor_transformer\\n)\\n\\n\\ndataloader_train = DataLoader(\\n    dataset_train,\\n    batch_size=BATCH_SIZE,\\n    shuffle=True,\\n    collate_fn=to_tensor_transformer.collate_cat,\\n    drop_last=True,\\n)\\ndataloader_val = DataLoader(\\n    dataset_val,\\n    batch_size=BATCH_SIZE,\\n    shuffle=True,\\n    collate_fn=to_tensor_transformer.collate_cat,\\n    drop_last=True,\\n)\\n\\nprint(\\n    f\\\"Training data set includes {len(filenames_train)} pdbs with \\\"\\n    f\\\"{len(dataset_train)} environments.\\\"\\n)\\nprint(\\n    f\\\"Validation data set includes {len(filenames_val)} pdbs with \\\"\\n    f\\\"{len(dataset_val)} environments.\\\"\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parsed_pdb_filenames = sorted(glob.glob(\"data/pdbs/parsed/*coord*\"))\n",
    "random.shuffle(parsed_pdb_filenames)\n",
    "\n",
    "n_train_pdbs = int(len(parsed_pdb_filenames) * TRAIN_VAL_SPLIT)\n",
    "filenames_train = parsed_pdb_filenames[:n_train_pdbs]\n",
    "filenames_val = parsed_pdb_filenames[n_train_pdbs:]\n",
    "\n",
    "to_tensor_transformer = ToTensor(DEVICE)\n",
    "\n",
    "dataset_train = ResidueEnvironmentsDataset(\n",
    "    filenames_train, transformer=to_tensor_transformer\n",
    ")\n",
    "dataset_val = ResidueEnvironmentsDataset(\n",
    "    filenames_val, transformer=to_tensor_transformer\n",
    ")\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=to_tensor_transformer.collate_cat,\n",
    "    drop_last=True,\n",
    ")\n",
    "dataloader_val = DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=to_tensor_transformer.collate_cat,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Training data set includes {len(filenames_train)} pdbs with \"\n",
    "    f\"{len(dataset_train)} environments.\"\n",
    ")\n",
    "print(\n",
    "    f\"Validation data set includes {len(filenames_val)} pdbs with \"\n",
    "    f\"{len(dataset_val)} environments.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Cavity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"def _train_step(\\n    cavity_model_net: CavityModel,\\n    optimizer: torch.optim.Adam,\\n    loss_function: torch.nn.CrossEntropyLoss,\\n) -> (torch.Tensor, float):\\n    \\\"\\\"\\\"\\n    Helper function to take a training step\\n    \\\"\\\"\\\"\\n    cavity_model_net.train()\\n    optimizer.zero_grad()\\n    batch_y_pred = cavity_model_net(batch_x)\\n    loss_batch = loss_function(batch_y_pred, torch.argmax(batch_y, dim=-1))\\n    loss_batch.backward()\\n    optimizer.step()\\n    return (batch_y_pred, loss_batch.detach().cpu().item())\\n\\n\\ndef _eval_loop(\\n    cavity_model_net: CavityModel,\\n    data_loader_val,\\n    loss_function: torch.nn.CrossEntropyLoss,\\n) -> (float, float):\\n    \\\"\\\"\\\"\\n    Helper function to perform an eval loop\\n    \\\"\\\"\\\"\\n    # Eval loop. Due to memory, we don't pass the whole eval set to the model\\n    labels_true_val = []\\n    labels_pred_val = []\\n    loss_batch_list_val = []\\n    for batch_x_val, batch_y_val in dataloader_val:\\n        cavity_model_net.eval()\\n        batch_y_pred_val = cavity_model_net(batch_x_val)\\n\\n        loss_batch_val = loss_function(\\n            batch_y_pred_val, torch.argmax(batch_y_val, dim=-1)\\n        )\\n        loss_batch_list_val.append(loss_batch_val.detach().cpu().item())\\n\\n        labels_true_val.append(torch.argmax(batch_y_val, dim=-1).detach().cpu().numpy())\\n        labels_pred_val.append(\\n            torch.argmax(batch_y_pred_val, dim=-1).detach().cpu().numpy()\\n        )\\n    acc_val = np.mean(\\n        (np.reshape(labels_true_val, -1) == np.reshape(labels_pred_val, -1))\\n    )\\n    loss_val = np.mean(loss_batch_list_val)\\n    return acc_val, loss_val\";\n",
       "                var nbb_formatted_code = \"def _train_step(\\n    cavity_model_net: CavityModel,\\n    optimizer: torch.optim.Adam,\\n    loss_function: torch.nn.CrossEntropyLoss,\\n) -> (torch.Tensor, float):\\n    \\\"\\\"\\\"\\n    Helper function to take a training step\\n    \\\"\\\"\\\"\\n    cavity_model_net.train()\\n    optimizer.zero_grad()\\n    batch_y_pred = cavity_model_net(batch_x)\\n    loss_batch = loss_function(batch_y_pred, torch.argmax(batch_y, dim=-1))\\n    loss_batch.backward()\\n    optimizer.step()\\n    return (batch_y_pred, loss_batch.detach().cpu().item())\\n\\n\\ndef _eval_loop(\\n    cavity_model_net: CavityModel,\\n    data_loader_val,\\n    loss_function: torch.nn.CrossEntropyLoss,\\n) -> (float, float):\\n    \\\"\\\"\\\"\\n    Helper function to perform an eval loop\\n    \\\"\\\"\\\"\\n    # Eval loop. Due to memory, we don't pass the whole eval set to the model\\n    labels_true_val = []\\n    labels_pred_val = []\\n    loss_batch_list_val = []\\n    for batch_x_val, batch_y_val in dataloader_val:\\n        cavity_model_net.eval()\\n        batch_y_pred_val = cavity_model_net(batch_x_val)\\n\\n        loss_batch_val = loss_function(\\n            batch_y_pred_val, torch.argmax(batch_y_val, dim=-1)\\n        )\\n        loss_batch_list_val.append(loss_batch_val.detach().cpu().item())\\n\\n        labels_true_val.append(torch.argmax(batch_y_val, dim=-1).detach().cpu().numpy())\\n        labels_pred_val.append(\\n            torch.argmax(batch_y_pred_val, dim=-1).detach().cpu().numpy()\\n        )\\n    acc_val = np.mean(\\n        (np.reshape(labels_true_val, -1) == np.reshape(labels_pred_val, -1))\\n    )\\n    loss_val = np.mean(loss_batch_list_val)\\n    return acc_val, loss_val\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _train_step(\n",
    "    cavity_model_net: CavityModel,\n",
    "    optimizer: torch.optim.Adam,\n",
    "    loss_function: torch.nn.CrossEntropyLoss,\n",
    ") -> (torch.Tensor, float):\n",
    "    \"\"\"\n",
    "    Helper function to take a training step\n",
    "    \"\"\"\n",
    "    cavity_model_net.train()\n",
    "    optimizer.zero_grad()\n",
    "    batch_y_pred = cavity_model_net(batch_x)\n",
    "    loss_batch = loss_function(batch_y_pred, torch.argmax(batch_y, dim=-1))\n",
    "    loss_batch.backward()\n",
    "    optimizer.step()\n",
    "    return (batch_y_pred, loss_batch.detach().cpu().item())\n",
    "\n",
    "\n",
    "def _eval_loop(\n",
    "    cavity_model_net: CavityModel,\n",
    "    data_loader_val,\n",
    "    loss_function: torch.nn.CrossEntropyLoss,\n",
    ") -> (float, float):\n",
    "    \"\"\"\n",
    "    Helper function to perform an eval loop\n",
    "    \"\"\"\n",
    "    # Eval loop. Due to memory, we don't pass the whole eval set to the model\n",
    "    labels_true_val = []\n",
    "    labels_pred_val = []\n",
    "    loss_batch_list_val = []\n",
    "    for batch_x_val, batch_y_val in dataloader_val:\n",
    "        cavity_model_net.eval()\n",
    "        batch_y_pred_val = cavity_model_net(batch_x_val)\n",
    "\n",
    "        loss_batch_val = loss_function(\n",
    "            batch_y_pred_val, torch.argmax(batch_y_val, dim=-1)\n",
    "        )\n",
    "        loss_batch_list_val.append(loss_batch_val.detach().cpu().item())\n",
    "\n",
    "        labels_true_val.append(torch.argmax(batch_y_val, dim=-1).detach().cpu().numpy())\n",
    "        labels_pred_val.append(\n",
    "            torch.argmax(batch_y_pred_val, dim=-1).detach().cpu().numpy()\n",
    "        )\n",
    "    acc_val = np.mean(\n",
    "        (np.reshape(labels_true_val, -1) == np.reshape(labels_pred_val, -1))\n",
    "    )\n",
    "    loss_val = np.mean(loss_batch_list_val)\n",
    "    return acc_val, loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0. Train loss: 2.493. Train Acc: 0.32. Val loss: 3.199. Val Acc 0.07\n",
      "Epoch  1. Train loss: 1.261. Train Acc: 0.77. Val loss: 2.729. Val Acc 0.18\n",
      "Epoch  2. Train loss: 0.696. Train Acc: 0.94. Val loss: 2.725. Val Acc 0.21\n",
      "Epoch  3. Train loss: 0.347. Train Acc: 0.99. Val loss: 2.815. Val Acc 0.21\n",
      "Epoch  4. Train loss: 0.176. Train Acc: 1.00. Val loss: 2.881. Val Acc 0.21\n",
      "Epoch  5. Train loss: 0.099. Train Acc: 1.00. Val loss: 2.899. Val Acc 0.20\n",
      "Early stopping activated. Best epoch idx: 2 with validation loss: 2.725 and \n",
      "model_path: cavity_models/model_epoch_02.pt\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"# Define model\\ncavity_model_net = CavityModel(DEVICE).to(DEVICE)\\nloss_function = torch.nn.CrossEntropyLoss()\\noptimizer = torch.optim.Adam(cavity_model_net.parameters(), lr=LEARNING_RATE)\\n\\n# Create directory for model files\\nmodels_dirpath = \\\"cavity_models/\\\"\\nif not os.path.exists(models_dirpath):\\n    os.mkdir(models_dirpath) \\n\\n# Train loop\\ncurrent_best_epoch_idx = -1\\ncurrent_best_loss_val = 1e4\\npatience = 0\\nepoch_idx_to_model_path = {}\\nfor epoch in range(EPOCHS):\\n    labels_true = []\\n    labels_pred = []\\n    loss_batch_list = []\\n    for batch_x, batch_y in dataloader_train:\\n        # Take train step\\n        batch_y_pred, loss_batch = _train_step(\\n            cavity_model_net, optimizer, loss_function\\n        )\\n        loss_batch_list.append(loss_batch)\\n\\n        labels_true.append(torch.argmax(batch_y, dim=-1).detach().cpu().numpy())\\n        labels_pred.append(torch.argmax(batch_y_pred, dim=-1).detach().cpu().numpy())\\n    \\n    # Train epoch metrics\\n    acc_train = np.mean((np.reshape(labels_true, -1) == np.reshape(labels_pred, -1)))\\n    loss_train = np.mean(loss_batch_list)\\n\\n    # Validation epoch metrics\\n    acc_val, loss_val = _eval_loop(cavity_model_net, dataloader_val, loss_function)\\n\\n    print(\\n        f\\\"Epoch {epoch:2d}. Train loss: {loss_train:5.3f}. \\\"\\n        f\\\"Train Acc: {acc_train:4.2f}. Val loss: {loss_val:5.3f}. \\\"\\n        f\\\"Val Acc {acc_val:4.2f}\\\"\\n    )\\n    \\n    # Save model\\n    model_path = f\\\"cavity_models/model_epoch_{epoch:02d}.pt\\\"\\n    epoch_idx_to_model_path[epoch] = model_path\\n    torch.save(cavity_model_net.state_dict(), model_path)\\n    \\n    # Early stopping\\n    if loss_val < current_best_loss_val:\\n        current_best_loss_val = loss_val\\n        current_best_epoch_idx = epoch\\n    else:\\n        patience += 1\\n    if patience > PATIENCE_CUTOFF:\\n        print(\\n            f\\\"Early stopping activated. Best epoch idx: {current_best_epoch_idx} \\\"\\n            f\\\"with validation loss: {current_best_loss_val:5.3f} and \\\\nmodel_path: \\\"\\n            f\\\"{epoch_idx_to_model_path[current_best_epoch_idx]}\\\"\\n        )\\n        break\";\n",
       "                var nbb_formatted_code = \"# Define model\\ncavity_model_net = CavityModel(DEVICE).to(DEVICE)\\nloss_function = torch.nn.CrossEntropyLoss()\\noptimizer = torch.optim.Adam(cavity_model_net.parameters(), lr=LEARNING_RATE)\\n\\n# Create directory for model files\\nmodels_dirpath = \\\"cavity_models/\\\"\\nif not os.path.exists(models_dirpath):\\n    os.mkdir(models_dirpath)\\n\\n# Train loop\\ncurrent_best_epoch_idx = -1\\ncurrent_best_loss_val = 1e4\\npatience = 0\\nepoch_idx_to_model_path = {}\\nfor epoch in range(EPOCHS):\\n    labels_true = []\\n    labels_pred = []\\n    loss_batch_list = []\\n    for batch_x, batch_y in dataloader_train:\\n        # Take train step\\n        batch_y_pred, loss_batch = _train_step(\\n            cavity_model_net, optimizer, loss_function\\n        )\\n        loss_batch_list.append(loss_batch)\\n\\n        labels_true.append(torch.argmax(batch_y, dim=-1).detach().cpu().numpy())\\n        labels_pred.append(torch.argmax(batch_y_pred, dim=-1).detach().cpu().numpy())\\n\\n    # Train epoch metrics\\n    acc_train = np.mean((np.reshape(labels_true, -1) == np.reshape(labels_pred, -1)))\\n    loss_train = np.mean(loss_batch_list)\\n\\n    # Validation epoch metrics\\n    acc_val, loss_val = _eval_loop(cavity_model_net, dataloader_val, loss_function)\\n\\n    print(\\n        f\\\"Epoch {epoch:2d}. Train loss: {loss_train:5.3f}. \\\"\\n        f\\\"Train Acc: {acc_train:4.2f}. Val loss: {loss_val:5.3f}. \\\"\\n        f\\\"Val Acc {acc_val:4.2f}\\\"\\n    )\\n\\n    # Save model\\n    model_path = f\\\"cavity_models/model_epoch_{epoch:02d}.pt\\\"\\n    epoch_idx_to_model_path[epoch] = model_path\\n    torch.save(cavity_model_net.state_dict(), model_path)\\n\\n    # Early stopping\\n    if loss_val < current_best_loss_val:\\n        current_best_loss_val = loss_val\\n        current_best_epoch_idx = epoch\\n    else:\\n        patience += 1\\n    if patience > PATIENCE_CUTOFF:\\n        print(\\n            f\\\"Early stopping activated. Best epoch idx: {current_best_epoch_idx} \\\"\\n            f\\\"with validation loss: {current_best_loss_val:5.3f} and \\\\nmodel_path: \\\"\\n            f\\\"{epoch_idx_to_model_path[current_best_epoch_idx]}\\\"\\n        )\\n        break\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define model\n",
    "cavity_model_net = CavityModel(DEVICE).to(DEVICE)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cavity_model_net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Create directory for model files\n",
    "models_dirpath = \"cavity_models/\"\n",
    "if not os.path.exists(models_dirpath):\n",
    "    os.mkdir(models_dirpath) \n",
    "\n",
    "# Train loop\n",
    "current_best_epoch_idx = -1\n",
    "current_best_loss_val = 1e4\n",
    "patience = 0\n",
    "epoch_idx_to_model_path = {}\n",
    "for epoch in range(EPOCHS):\n",
    "    labels_true = []\n",
    "    labels_pred = []\n",
    "    loss_batch_list = []\n",
    "    for batch_x, batch_y in dataloader_train:\n",
    "        # Take train step\n",
    "        batch_y_pred, loss_batch = _train_step(\n",
    "            cavity_model_net, optimizer, loss_function\n",
    "        )\n",
    "        loss_batch_list.append(loss_batch)\n",
    "\n",
    "        labels_true.append(torch.argmax(batch_y, dim=-1).detach().cpu().numpy())\n",
    "        labels_pred.append(torch.argmax(batch_y_pred, dim=-1).detach().cpu().numpy())\n",
    "    \n",
    "    # Train epoch metrics\n",
    "    acc_train = np.mean((np.reshape(labels_true, -1) == np.reshape(labels_pred, -1)))\n",
    "    loss_train = np.mean(loss_batch_list)\n",
    "\n",
    "    # Validation epoch metrics\n",
    "    acc_val, loss_val = _eval_loop(cavity_model_net, dataloader_val, loss_function)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:2d}. Train loss: {loss_train:5.3f}. \"\n",
    "        f\"Train Acc: {acc_train:4.2f}. Val loss: {loss_val:5.3f}. \"\n",
    "        f\"Val Acc {acc_val:4.2f}\"\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    model_path = f\"cavity_models/model_epoch_{epoch:02d}.pt\"\n",
    "    epoch_idx_to_model_path[epoch] = model_path\n",
    "    torch.save(cavity_model_net.state_dict(), model_path)\n",
    "    \n",
    "    # Early stopping\n",
    "    if loss_val < current_best_loss_val:\n",
    "        current_best_loss_val = loss_val\n",
    "        current_best_epoch_idx = epoch\n",
    "    else:\n",
    "        patience += 1\n",
    "    if patience > PATIENCE_CUTOFF:\n",
    "        print(\n",
    "            f\"Early stopping activated. Best epoch idx: {current_best_epoch_idx} \"\n",
    "            f\"with validation loss: {current_best_loss_val:5.3f} and \\nmodel_path: \"\n",
    "            f\"{epoch_idx_to_model_path[current_best_epoch_idx]}\"\n",
    "        )\n",
    "        break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
