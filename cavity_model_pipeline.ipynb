{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"import glob\\nimport os\\nimport random\\nfrom typing import Callable, List, Union\\n\\nimport numpy as np\\nimport pandas as pd\\nimport torch\\nfrom torch.utils.data import DataLoader, Dataset\\nfrom Bio.PDB.Polypeptide import index_to_one\\n\\nfrom cavity_model import (\\n    ResidueEnvironment,\\n    ResidueEnvironmentsDataset,\\n    ToTensor,\\n    CavityModel,\\n)\\n\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"import glob\\nimport os\\nimport random\\nfrom typing import Callable, List, Union\\n\\nimport numpy as np\\nimport pandas as pd\\nimport torch\\nfrom torch.utils.data import DataLoader, Dataset\\nfrom Bio.PDB.Polypeptide import index_to_one\\n\\nfrom cavity_model import (\\n    ResidueEnvironment,\\n    ResidueEnvironmentsDataset,\\n    ToTensor,\\n    CavityModel,\\n)\\n\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "from typing import Callable, List, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from Bio.PDB.Polypeptide import index_to_one\n",
    "\n",
    "from cavity_model import (\n",
    "    ResidueEnvironment,\n",
    "    ResidueEnvironmentsDataset,\n",
    "    ToTensor,\n",
    "    CavityModel,\n",
    ")\n",
    "\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cavity Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and process Cavity Model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# # Run shell script that takes a .txt file with PDBIDs as input.\\n# !./get_parse_pdbs_cavity_model.sh data/pdbids_010.txt\";\n",
       "                var nbb_formatted_code = \"# # Run shell script that takes a .txt file with PDBIDs as input.\\n# !./get_parse_pdbs_cavity_model.sh data/pdbids_010.txt\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Run shell script that takes a .txt file with PDBIDs as input.\n",
    "# !./get_parse_pdbs_cavity_model.sh data/pdbids_010.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables for Cavity Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"DEVICE = \\\"cuda\\\"  # \\\"cpu\\\" or \\\"cuda\\\"\\nTRAIN_VAL_SPLIT = 0.8\\nBATCH_SIZE = 100\\nLEARNING_RATE = 3e-4\\nEPOCHS = 10\\nPATIENCE_CUTOFF = 2\";\n",
       "                var nbb_formatted_code = \"DEVICE = \\\"cuda\\\"  # \\\"cpu\\\" or \\\"cuda\\\"\\nTRAIN_VAL_SPLIT = 0.8\\nBATCH_SIZE = 100\\nLEARNING_RATE = 3e-4\\nEPOCHS = 10\\nPATIENCE_CUTOFF = 2\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DEVICE = \"cuda\"  # \"cpu\" or \"cuda\"\n",
    "TRAIN_VAL_SPLIT = 0.8\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 3e-4\n",
    "EPOCHS = 10\n",
    "PATIENCE_CUTOFF = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Parsed PDBs and perform train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data set includes 8 pdbs with 4636 environments.\n",
      "Validation data set includes 2 pdbs with 622 environments.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"parsed_pdb_filenames = sorted(glob.glob(\\\"data/pdbs/parsed/*coord*\\\"))\\nrandom.shuffle(parsed_pdb_filenames)\\n\\nn_train_pdbs = int(len(parsed_pdb_filenames) * TRAIN_VAL_SPLIT)\\nfilenames_train = parsed_pdb_filenames[:n_train_pdbs]\\nfilenames_val = parsed_pdb_filenames[n_train_pdbs:]\\n\\nto_tensor_transformer = ToTensor(DEVICE)\\n\\ndataset_train = ResidueEnvironmentsDataset(\\n    filenames_train, transformer=to_tensor_transformer\\n)\\ndataset_val = ResidueEnvironmentsDataset(\\n    filenames_val, transformer=to_tensor_transformer\\n)\\n\\ndataloader_train = DataLoader(\\n    dataset_train,\\n    batch_size=BATCH_SIZE,\\n    shuffle=True,\\n    collate_fn=to_tensor_transformer.collate_cat,\\n    drop_last=True,\\n)\\ndataloader_val = DataLoader(\\n    dataset_val,\\n    batch_size=BATCH_SIZE,\\n    shuffle=True,\\n    collate_fn=to_tensor_transformer.collate_cat,\\n    drop_last=True,\\n)\\n\\nprint(\\n    f\\\"Training data set includes {len(filenames_train)} pdbs with \\\"\\n    f\\\"{len(dataset_train)} environments.\\\"\\n)\\nprint(\\n    f\\\"Validation data set includes {len(filenames_val)} pdbs with \\\"\\n    f\\\"{len(dataset_val)} environments.\\\"\\n)\";\n",
       "                var nbb_formatted_code = \"parsed_pdb_filenames = sorted(glob.glob(\\\"data/pdbs/parsed/*coord*\\\"))\\nrandom.shuffle(parsed_pdb_filenames)\\n\\nn_train_pdbs = int(len(parsed_pdb_filenames) * TRAIN_VAL_SPLIT)\\nfilenames_train = parsed_pdb_filenames[:n_train_pdbs]\\nfilenames_val = parsed_pdb_filenames[n_train_pdbs:]\\n\\nto_tensor_transformer = ToTensor(DEVICE)\\n\\ndataset_train = ResidueEnvironmentsDataset(\\n    filenames_train, transformer=to_tensor_transformer\\n)\\ndataset_val = ResidueEnvironmentsDataset(\\n    filenames_val, transformer=to_tensor_transformer\\n)\\n\\ndataloader_train = DataLoader(\\n    dataset_train,\\n    batch_size=BATCH_SIZE,\\n    shuffle=True,\\n    collate_fn=to_tensor_transformer.collate_cat,\\n    drop_last=True,\\n)\\ndataloader_val = DataLoader(\\n    dataset_val,\\n    batch_size=BATCH_SIZE,\\n    shuffle=True,\\n    collate_fn=to_tensor_transformer.collate_cat,\\n    drop_last=True,\\n)\\n\\nprint(\\n    f\\\"Training data set includes {len(filenames_train)} pdbs with \\\"\\n    f\\\"{len(dataset_train)} environments.\\\"\\n)\\nprint(\\n    f\\\"Validation data set includes {len(filenames_val)} pdbs with \\\"\\n    f\\\"{len(dataset_val)} environments.\\\"\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parsed_pdb_filenames = sorted(glob.glob(\"data/pdbs/parsed/*coord*\"))\n",
    "random.shuffle(parsed_pdb_filenames)\n",
    "\n",
    "n_train_pdbs = int(len(parsed_pdb_filenames) * TRAIN_VAL_SPLIT)\n",
    "filenames_train = parsed_pdb_filenames[:n_train_pdbs]\n",
    "filenames_val = parsed_pdb_filenames[n_train_pdbs:]\n",
    "\n",
    "to_tensor_transformer = ToTensor(DEVICE)\n",
    "\n",
    "dataset_train = ResidueEnvironmentsDataset(\n",
    "    filenames_train, transformer=to_tensor_transformer\n",
    ")\n",
    "dataset_val = ResidueEnvironmentsDataset(\n",
    "    filenames_val, transformer=to_tensor_transformer\n",
    ")\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=to_tensor_transformer.collate_cat,\n",
    "    drop_last=True,\n",
    ")\n",
    "dataloader_val = DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=to_tensor_transformer.collate_cat,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Training data set includes {len(filenames_train)} pdbs with \"\n",
    "    f\"{len(dataset_train)} environments.\"\n",
    ")\n",
    "print(\n",
    "    f\"Validation data set includes {len(filenames_val)} pdbs with \"\n",
    "    f\"{len(dataset_val)} environments.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Cavity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"def _train_step(\\n    cavity_model_net: CavityModel,\\n    optimizer: torch.optim.Adam,\\n    loss_function: torch.nn.CrossEntropyLoss,\\n) -> (torch.Tensor, float):\\n    \\\"\\\"\\\"\\n    Helper function to take a training step\\n    \\\"\\\"\\\"\\n    cavity_model_net.train()\\n    optimizer.zero_grad()\\n    batch_y_pred = cavity_model_net(batch_x)\\n    loss_batch = loss_function(batch_y_pred, torch.argmax(batch_y, dim=-1))\\n    loss_batch.backward()\\n    optimizer.step()\\n    return (batch_y_pred, loss_batch.detach().cpu().item())\\n\\n\\ndef _eval_loop(\\n    cavity_model_net: CavityModel,\\n    data_loader_val,\\n    loss_function: torch.nn.CrossEntropyLoss,\\n) -> (float, float):\\n    \\\"\\\"\\\"\\n    Helper function to perform an eval loop\\n    \\\"\\\"\\\"\\n    # Eval loop. Due to memory, we don't pass the whole eval set to the model\\n    labels_true_val = []\\n    labels_pred_val = []\\n    loss_batch_list_val = []\\n    for batch_x_val, batch_y_val in dataloader_val:\\n        cavity_model_net.eval()\\n        batch_y_pred_val = cavity_model_net(batch_x_val)\\n\\n        loss_batch_val = loss_function(\\n            batch_y_pred_val, torch.argmax(batch_y_val, dim=-1)\\n        )\\n        loss_batch_list_val.append(loss_batch_val.detach().cpu().item())\\n\\n        labels_true_val.append(torch.argmax(batch_y_val, dim=-1).detach().cpu().numpy())\\n        labels_pred_val.append(\\n            torch.argmax(batch_y_pred_val, dim=-1).detach().cpu().numpy()\\n        )\\n    acc_val = np.mean(\\n        (np.reshape(labels_true_val, -1) == np.reshape(labels_pred_val, -1))\\n    )\\n    loss_val = np.mean(loss_batch_list_val)\\n    return acc_val, loss_val\";\n",
       "                var nbb_formatted_code = \"def _train_step(\\n    cavity_model_net: CavityModel,\\n    optimizer: torch.optim.Adam,\\n    loss_function: torch.nn.CrossEntropyLoss,\\n) -> (torch.Tensor, float):\\n    \\\"\\\"\\\"\\n    Helper function to take a training step\\n    \\\"\\\"\\\"\\n    cavity_model_net.train()\\n    optimizer.zero_grad()\\n    batch_y_pred = cavity_model_net(batch_x)\\n    loss_batch = loss_function(batch_y_pred, torch.argmax(batch_y, dim=-1))\\n    loss_batch.backward()\\n    optimizer.step()\\n    return (batch_y_pred, loss_batch.detach().cpu().item())\\n\\n\\ndef _eval_loop(\\n    cavity_model_net: CavityModel,\\n    data_loader_val,\\n    loss_function: torch.nn.CrossEntropyLoss,\\n) -> (float, float):\\n    \\\"\\\"\\\"\\n    Helper function to perform an eval loop\\n    \\\"\\\"\\\"\\n    # Eval loop. Due to memory, we don't pass the whole eval set to the model\\n    labels_true_val = []\\n    labels_pred_val = []\\n    loss_batch_list_val = []\\n    for batch_x_val, batch_y_val in dataloader_val:\\n        cavity_model_net.eval()\\n        batch_y_pred_val = cavity_model_net(batch_x_val)\\n\\n        loss_batch_val = loss_function(\\n            batch_y_pred_val, torch.argmax(batch_y_val, dim=-1)\\n        )\\n        loss_batch_list_val.append(loss_batch_val.detach().cpu().item())\\n\\n        labels_true_val.append(torch.argmax(batch_y_val, dim=-1).detach().cpu().numpy())\\n        labels_pred_val.append(\\n            torch.argmax(batch_y_pred_val, dim=-1).detach().cpu().numpy()\\n        )\\n    acc_val = np.mean(\\n        (np.reshape(labels_true_val, -1) == np.reshape(labels_pred_val, -1))\\n    )\\n    loss_val = np.mean(loss_batch_list_val)\\n    return acc_val, loss_val\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _train_step(\n",
    "    cavity_model_net: CavityModel,\n",
    "    optimizer: torch.optim.Adam,\n",
    "    loss_function: torch.nn.CrossEntropyLoss,\n",
    ") -> (torch.Tensor, float):\n",
    "    \"\"\"\n",
    "    Helper function to take a training step\n",
    "    \"\"\"\n",
    "    cavity_model_net.train()\n",
    "    optimizer.zero_grad()\n",
    "    batch_y_pred = cavity_model_net(batch_x)\n",
    "    loss_batch = loss_function(batch_y_pred, torch.argmax(batch_y, dim=-1))\n",
    "    loss_batch.backward()\n",
    "    optimizer.step()\n",
    "    return (batch_y_pred, loss_batch.detach().cpu().item())\n",
    "\n",
    "\n",
    "def _eval_loop(\n",
    "    cavity_model_net: CavityModel,\n",
    "    data_loader_val,\n",
    "    loss_function: torch.nn.CrossEntropyLoss,\n",
    ") -> (float, float):\n",
    "    \"\"\"\n",
    "    Helper function to perform an eval loop\n",
    "    \"\"\"\n",
    "    # Eval loop. Due to memory, we don't pass the whole eval set to the model\n",
    "    labels_true_val = []\n",
    "    labels_pred_val = []\n",
    "    loss_batch_list_val = []\n",
    "    for batch_x_val, batch_y_val in dataloader_val:\n",
    "        cavity_model_net.eval()\n",
    "        batch_y_pred_val = cavity_model_net(batch_x_val)\n",
    "\n",
    "        loss_batch_val = loss_function(\n",
    "            batch_y_pred_val, torch.argmax(batch_y_val, dim=-1)\n",
    "        )\n",
    "        loss_batch_list_val.append(loss_batch_val.detach().cpu().item())\n",
    "\n",
    "        labels_true_val.append(torch.argmax(batch_y_val, dim=-1).detach().cpu().numpy())\n",
    "        labels_pred_val.append(\n",
    "            torch.argmax(batch_y_pred_val, dim=-1).detach().cpu().numpy()\n",
    "        )\n",
    "    acc_val = np.mean(\n",
    "        (np.reshape(labels_true_val, -1) == np.reshape(labels_pred_val, -1))\n",
    "    )\n",
    "    loss_val = np.mean(loss_batch_list_val)\n",
    "    return acc_val, loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0. Train loss: 2.484. Train Acc: 0.32. Val loss: 3.171. Val Acc 0.05\n",
      "Epoch  1. Train loss: 1.292. Train Acc: 0.76. Val loss: 2.696. Val Acc 0.22\n",
      "Epoch  2. Train loss: 0.726. Train Acc: 0.92. Val loss: 2.727. Val Acc 0.19\n",
      "Epoch  3. Train loss: 0.368. Train Acc: 0.99. Val loss: 2.755. Val Acc 0.21\n",
      "Epoch  4. Train loss: 0.186. Train Acc: 1.00. Val loss: 2.783. Val Acc 0.20\n",
      "Early stopping activated.\n",
      "Best epoch idx: 1 with validation loss: 2.696 and model_path: cavity_models/model_epoch_01.pt\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# Define model\\ncavity_model_net = CavityModel(DEVICE).to(DEVICE)\\nloss_function = torch.nn.CrossEntropyLoss()\\noptimizer = torch.optim.Adam(cavity_model_net.parameters(), lr=LEARNING_RATE)\\n\\n# Create directory for model files\\nmodels_dirpath = \\\"cavity_models/\\\"\\nif not os.path.exists(models_dirpath):\\n    os.mkdir(models_dirpath)\\n\\n# Train loop\\ncurrent_best_epoch_idx = -1\\ncurrent_best_loss_val = 1e4\\npatience = 0\\nepoch_idx_to_model_path = {}\\nfor epoch in range(EPOCHS):\\n    labels_true = []\\n    labels_pred = []\\n    loss_batch_list = []\\n    for batch_x, batch_y in dataloader_train:\\n        # Take train step\\n        batch_y_pred, loss_batch = _train_step(\\n            cavity_model_net, optimizer, loss_function\\n        )\\n        loss_batch_list.append(loss_batch)\\n\\n        labels_true.append(torch.argmax(batch_y, dim=-1).detach().cpu().numpy())\\n        labels_pred.append(torch.argmax(batch_y_pred, dim=-1).detach().cpu().numpy())\\n\\n    # Train epoch metrics\\n    acc_train = np.mean((np.reshape(labels_true, -1) == np.reshape(labels_pred, -1)))\\n    loss_train = np.mean(loss_batch_list)\\n\\n    # Validation epoch metrics\\n    acc_val, loss_val = _eval_loop(cavity_model_net, dataloader_val, loss_function)\\n\\n    print(\\n        f\\\"Epoch {epoch:2d}. Train loss: {loss_train:5.3f}. \\\"\\n        f\\\"Train Acc: {acc_train:4.2f}. Val loss: {loss_val:5.3f}. \\\"\\n        f\\\"Val Acc {acc_val:4.2f}\\\"\\n    )\\n\\n    # Save model\\n    model_path = f\\\"cavity_models/model_epoch_{epoch:02d}.pt\\\"\\n    epoch_idx_to_model_path[epoch] = model_path\\n    torch.save(cavity_model_net.state_dict(), model_path)\\n\\n    # Early stopping\\n    if loss_val < current_best_loss_val:\\n        current_best_loss_val = loss_val\\n        current_best_epoch_idx = epoch\\n        patience = 0\\n    else:\\n        patience += 1\\n    if patience > PATIENCE_CUTOFF:\\n        print(f\\\"Early stopping activated.\\\")\\n        break\\n\\nprint(\\n    f\\\"Best epoch idx: {current_best_epoch_idx} with validation loss: \\\"\\n    f\\\"{current_best_loss_val:5.3f} and model_path: \\\"\\n    f\\\"{epoch_idx_to_model_path[current_best_epoch_idx]}\\\"\\n)\";\n",
       "                var nbb_formatted_code = \"# Define model\\ncavity_model_net = CavityModel(DEVICE).to(DEVICE)\\nloss_function = torch.nn.CrossEntropyLoss()\\noptimizer = torch.optim.Adam(cavity_model_net.parameters(), lr=LEARNING_RATE)\\n\\n# Create directory for model files\\nmodels_dirpath = \\\"cavity_models/\\\"\\nif not os.path.exists(models_dirpath):\\n    os.mkdir(models_dirpath)\\n\\n# Train loop\\ncurrent_best_epoch_idx = -1\\ncurrent_best_loss_val = 1e4\\npatience = 0\\nepoch_idx_to_model_path = {}\\nfor epoch in range(EPOCHS):\\n    labels_true = []\\n    labels_pred = []\\n    loss_batch_list = []\\n    for batch_x, batch_y in dataloader_train:\\n        # Take train step\\n        batch_y_pred, loss_batch = _train_step(\\n            cavity_model_net, optimizer, loss_function\\n        )\\n        loss_batch_list.append(loss_batch)\\n\\n        labels_true.append(torch.argmax(batch_y, dim=-1).detach().cpu().numpy())\\n        labels_pred.append(torch.argmax(batch_y_pred, dim=-1).detach().cpu().numpy())\\n\\n    # Train epoch metrics\\n    acc_train = np.mean((np.reshape(labels_true, -1) == np.reshape(labels_pred, -1)))\\n    loss_train = np.mean(loss_batch_list)\\n\\n    # Validation epoch metrics\\n    acc_val, loss_val = _eval_loop(cavity_model_net, dataloader_val, loss_function)\\n\\n    print(\\n        f\\\"Epoch {epoch:2d}. Train loss: {loss_train:5.3f}. \\\"\\n        f\\\"Train Acc: {acc_train:4.2f}. Val loss: {loss_val:5.3f}. \\\"\\n        f\\\"Val Acc {acc_val:4.2f}\\\"\\n    )\\n\\n    # Save model\\n    model_path = f\\\"cavity_models/model_epoch_{epoch:02d}.pt\\\"\\n    epoch_idx_to_model_path[epoch] = model_path\\n    torch.save(cavity_model_net.state_dict(), model_path)\\n\\n    # Early stopping\\n    if loss_val < current_best_loss_val:\\n        current_best_loss_val = loss_val\\n        current_best_epoch_idx = epoch\\n        patience = 0\\n    else:\\n        patience += 1\\n    if patience > PATIENCE_CUTOFF:\\n        print(f\\\"Early stopping activated.\\\")\\n        break\\n\\nprint(\\n    f\\\"Best epoch idx: {current_best_epoch_idx} with validation loss: \\\"\\n    f\\\"{current_best_loss_val:5.3f} and model_path: \\\"\\n    f\\\"{epoch_idx_to_model_path[current_best_epoch_idx]}\\\"\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define model\n",
    "cavity_model_net = CavityModel(DEVICE).to(DEVICE)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cavity_model_net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Create directory for model files\n",
    "models_dirpath = \"cavity_models/\"\n",
    "if not os.path.exists(models_dirpath):\n",
    "    os.mkdir(models_dirpath)\n",
    "\n",
    "# Train loop\n",
    "current_best_epoch_idx = -1\n",
    "current_best_loss_val = 1e4\n",
    "patience = 0\n",
    "epoch_idx_to_model_path = {}\n",
    "for epoch in range(EPOCHS):\n",
    "    labels_true = []\n",
    "    labels_pred = []\n",
    "    loss_batch_list = []\n",
    "    for batch_x, batch_y in dataloader_train:\n",
    "        # Take train step\n",
    "        batch_y_pred, loss_batch = _train_step(\n",
    "            cavity_model_net, optimizer, loss_function\n",
    "        )\n",
    "        loss_batch_list.append(loss_batch)\n",
    "\n",
    "        labels_true.append(torch.argmax(batch_y, dim=-1).detach().cpu().numpy())\n",
    "        labels_pred.append(torch.argmax(batch_y_pred, dim=-1).detach().cpu().numpy())\n",
    "\n",
    "    # Train epoch metrics\n",
    "    acc_train = np.mean((np.reshape(labels_true, -1) == np.reshape(labels_pred, -1)))\n",
    "    loss_train = np.mean(loss_batch_list)\n",
    "\n",
    "    # Validation epoch metrics\n",
    "    acc_val, loss_val = _eval_loop(cavity_model_net, dataloader_val, loss_function)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:2d}. Train loss: {loss_train:5.3f}. \"\n",
    "        f\"Train Acc: {acc_train:4.2f}. Val loss: {loss_val:5.3f}. \"\n",
    "        f\"Val Acc {acc_val:4.2f}\"\n",
    "    )\n",
    "\n",
    "    # Save model\n",
    "    model_path = f\"cavity_models/model_epoch_{epoch:02d}.pt\"\n",
    "    epoch_idx_to_model_path[epoch] = model_path\n",
    "    torch.save(cavity_model_net.state_dict(), model_path)\n",
    "\n",
    "    # Early stopping\n",
    "    if loss_val < current_best_loss_val:\n",
    "        current_best_loss_val = loss_val\n",
    "        current_best_epoch_idx = epoch\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "    if patience > PATIENCE_CUTOFF:\n",
    "        print(f\"Early stopping activated.\")\n",
    "        break\n",
    "\n",
    "print(\n",
    "    f\"Best epoch idx: {current_best_epoch_idx} with validation loss: \"\n",
    "    f\"{current_best_loss_val:5.3f} and model_path: \"\n",
    "    f\"{epoch_idx_to_model_path[current_best_epoch_idx]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ddG Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse PDBs for DMS, Guerois and Protein G data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"# # Parse PDBs for which we have ddG data\\n# !./get_parse_pdbs_dowstream_task.sh\";\n",
       "                var nbb_formatted_code = \"# # Parse PDBs for which we have ddG data\\n# !./get_parse_pdbs_dowstream_task.sh\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Parse PDBs for which we have ddG data\n",
    "# !./get_parse_pdbs_dowstream_task.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create temporary residue environment datasets as dicts to more easily match ddG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"parsed_pdbs_wildcards = {\\n    \\\"dms\\\": \\\"data/data_dms/pdbs_parsed/*coord*\\\",\\n    \\\"protein_g\\\": \\\"data/data_protein_g/pdbs_parsed/*coord*\\\",\\n    \\\"guerois\\\": \\\"data/data_gueros/pdbs_parsed/*coord*\\\",\\n    #     \\\"symmetric\\\": \\\"data/data_symmetric/pdbs_parsed/*coord*\\\",\\n}\\n\\ndatasets_look_up = {}\\nfor dataset_key, pdbs_wildcard in parsed_pdbs_wildcards.items():\\n    parsed_pdb_filenames = sorted(glob.glob(pdbs_wildcard))\\n    dataset = ResidueEnvironmentsDataset(parsed_pdb_filenames, transformer=None)\\n    dataset_look_up = {}\\n    for resenv in dataset:\\n        key = (\\n            f\\\"{resenv.pdb_id}{resenv.chain_id}_{resenv.pdb_residue_number}\\\"\\n            f\\\"{index_to_one(resenv.restype_index)}\\\"\\n        )\\n        dataset_look_up[key] = resenv\\n    datasets_look_up[dataset_key] = dataset_look_up\";\n",
       "                var nbb_formatted_code = \"parsed_pdbs_wildcards = {\\n    \\\"dms\\\": \\\"data/data_dms/pdbs_parsed/*coord*\\\",\\n    \\\"protein_g\\\": \\\"data/data_protein_g/pdbs_parsed/*coord*\\\",\\n    \\\"guerois\\\": \\\"data/data_gueros/pdbs_parsed/*coord*\\\",\\n    #     \\\"symmetric\\\": \\\"data/data_symmetric/pdbs_parsed/*coord*\\\",\\n}\\n\\ndatasets_look_up = {}\\nfor dataset_key, pdbs_wildcard in parsed_pdbs_wildcards.items():\\n    parsed_pdb_filenames = sorted(glob.glob(pdbs_wildcard))\\n    dataset = ResidueEnvironmentsDataset(parsed_pdb_filenames, transformer=None)\\n    dataset_look_up = {}\\n    for resenv in dataset:\\n        key = (\\n            f\\\"{resenv.pdb_id}{resenv.chain_id}_{resenv.pdb_residue_number}\\\"\\n            f\\\"{index_to_one(resenv.restype_index)}\\\"\\n        )\\n        dataset_look_up[key] = resenv\\n    datasets_look_up[dataset_key] = dataset_look_up\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parsed_pdbs_wildcards = {\n",
    "    \"dms\": \"data/data_dms/pdbs_parsed/*coord*\",\n",
    "    \"protein_g\": \"data/data_protein_g/pdbs_parsed/*coord*\",\n",
    "    \"guerois\": \"data/data_gueros/pdbs_parsed/*coord*\",\n",
    "    \"symmetric\": \"data/data_symmetric/pdbs_parsed/*coord*\",\n",
    "}\n",
    "\n",
    "datasets_look_up = {}\n",
    "for dataset_key, pdbs_wildcard in parsed_pdbs_wildcards.items():\n",
    "    parsed_pdb_filenames = sorted(glob.glob(pdbs_wildcard))\n",
    "    dataset = ResidueEnvironmentsDataset(parsed_pdb_filenames, transformer=None)\n",
    "    dataset_look_up = {}\n",
    "    for resenv in dataset:\n",
    "        key = (\n",
    "            f\"{resenv.pdb_id}{resenv.chain_id}_{resenv.pdb_residue_number}\"\n",
    "            f\"{index_to_one(resenv.restype_index)}\"\n",
    "        )\n",
    "        dataset_look_up[key] = resenv\n",
    "    datasets_look_up[dataset_key] = dataset_look_up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ddG data as pandas.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"ddg_data_dict = {\\n    \\\"dms\\\": pd.read_csv(\\\"data/data_dms/ddgs_parsed.csv\\\"),\\n    \\\"protein_g\\\": pd.read_csv(\\\"data/data_protein_g/ddgs_parsed.csv\\\"),\\n    \\\"guerois\\\": pd.read_csv(\\\"data/data_guerois/ddgs_parsed.csv\\\"),\\n}\";\n",
       "                var nbb_formatted_code = \"ddg_data_dict = {\\n    \\\"dms\\\": pd.read_csv(\\\"data/data_dms/ddgs_parsed.csv\\\"),\\n    \\\"protein_g\\\": pd.read_csv(\\\"data/data_protein_g/ddgs_parsed.csv\\\"),\\n    \\\"guerois\\\": pd.read_csv(\\\"data/data_guerois/ddgs_parsed.csv\\\"),\\n}\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ddg_data_dict = {\n",
    "    \"dms\": pd.read_csv(\"data/data_dms/ddgs_parsed.csv\"),\n",
    "    \"protein_g\": pd.read_csv(\"data/data_protein_g/ddgs_parsed.csv\"),\n",
    "    \"guerois\": pd.read_csv(\"data/data_guerois/ddgs_parsed.csv\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdbid</th>\n",
       "      <th>chainid</th>\n",
       "      <th>variant</th>\n",
       "      <th>ddg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1D5R</td>\n",
       "      <td>A</td>\n",
       "      <td>M1V</td>\n",
       "      <td>-0.065143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1D5R</td>\n",
       "      <td>A</td>\n",
       "      <td>T2A</td>\n",
       "      <td>0.224462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1D5R</td>\n",
       "      <td>A</td>\n",
       "      <td>T2D</td>\n",
       "      <td>-0.190667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1D5R</td>\n",
       "      <td>A</td>\n",
       "      <td>T2E</td>\n",
       "      <td>0.333408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1D5R</td>\n",
       "      <td>A</td>\n",
       "      <td>T2G</td>\n",
       "      <td>0.001390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1D5R</td>\n",
       "      <td>A</td>\n",
       "      <td>T2K</td>\n",
       "      <td>0.052390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1D5R</td>\n",
       "      <td>A</td>\n",
       "      <td>T2N</td>\n",
       "      <td>-0.072988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1D5R</td>\n",
       "      <td>A</td>\n",
       "      <td>T2P</td>\n",
       "      <td>0.156690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1D5R</td>\n",
       "      <td>A</td>\n",
       "      <td>T2R</td>\n",
       "      <td>0.127170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1D5R</td>\n",
       "      <td>A</td>\n",
       "      <td>T2S</td>\n",
       "      <td>0.175641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pdbid chainid variant       ddg\n",
       "0  1D5R       A     M1V -0.065143\n",
       "1  1D5R       A     T2A  0.224462\n",
       "2  1D5R       A     T2D -0.190667\n",
       "3  1D5R       A     T2E  0.333408\n",
       "4  1D5R       A     T2G  0.001390\n",
       "5  1D5R       A     T2K  0.052390\n",
       "6  1D5R       A     T2N -0.072988\n",
       "7  1D5R       A     T2P  0.156690\n",
       "8  1D5R       A     T2R  0.127170\n",
       "9  1D5R       A     T2S  0.175641"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdbid</th>\n",
       "      <th>chainid</th>\n",
       "      <th>variant</th>\n",
       "      <th>ddg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1PGA</td>\n",
       "      <td>A</td>\n",
       "      <td>M1A</td>\n",
       "      <td>0.1407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1PGA</td>\n",
       "      <td>A</td>\n",
       "      <td>M1D</td>\n",
       "      <td>0.3795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1PGA</td>\n",
       "      <td>A</td>\n",
       "      <td>M1E</td>\n",
       "      <td>0.6414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1PGA</td>\n",
       "      <td>A</td>\n",
       "      <td>M1L</td>\n",
       "      <td>0.4573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1PGA</td>\n",
       "      <td>A</td>\n",
       "      <td>T2E</td>\n",
       "      <td>0.1299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1PGA</td>\n",
       "      <td>A</td>\n",
       "      <td>T2F</td>\n",
       "      <td>0.3008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1PGA</td>\n",
       "      <td>A</td>\n",
       "      <td>T2G</td>\n",
       "      <td>-0.6680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1PGA</td>\n",
       "      <td>A</td>\n",
       "      <td>T2H</td>\n",
       "      <td>-0.1303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1PGA</td>\n",
       "      <td>A</td>\n",
       "      <td>T2I</td>\n",
       "      <td>1.0040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1PGA</td>\n",
       "      <td>A</td>\n",
       "      <td>T2L</td>\n",
       "      <td>0.5417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pdbid chainid variant     ddg\n",
       "0  1PGA       A     M1A  0.1407\n",
       "1  1PGA       A     M1D  0.3795\n",
       "2  1PGA       A     M1E  0.6414\n",
       "3  1PGA       A     M1L  0.4573\n",
       "4  1PGA       A     T2E  0.1299\n",
       "5  1PGA       A     T2F  0.3008\n",
       "6  1PGA       A     T2G -0.6680\n",
       "7  1PGA       A     T2H -0.1303\n",
       "8  1PGA       A     T2I  1.0040\n",
       "9  1PGA       A     T2L  0.5417"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdbid</th>\n",
       "      <th>chainid</th>\n",
       "      <th>variant</th>\n",
       "      <th>ddg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>171L</td>\n",
       "      <td>A</td>\n",
       "      <td>A45E</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1A2P</td>\n",
       "      <td>A</td>\n",
       "      <td>Y103F</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1A2P</td>\n",
       "      <td>A</td>\n",
       "      <td>T105V</td>\n",
       "      <td>2.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1A2P</td>\n",
       "      <td>A</td>\n",
       "      <td>I109A</td>\n",
       "      <td>2.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1A2P</td>\n",
       "      <td>A</td>\n",
       "      <td>I109V</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1A2P</td>\n",
       "      <td>A</td>\n",
       "      <td>V10A</td>\n",
       "      <td>3.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1A2P</td>\n",
       "      <td>A</td>\n",
       "      <td>V10T</td>\n",
       "      <td>2.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1A2P</td>\n",
       "      <td>A</td>\n",
       "      <td>R110A</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1A2P</td>\n",
       "      <td>A</td>\n",
       "      <td>D12A</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1A2P</td>\n",
       "      <td>A</td>\n",
       "      <td>D12G</td>\n",
       "      <td>1.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pdbid chainid variant   ddg\n",
       "0  171L       A    A45E  0.01\n",
       "1  1A2P       A   Y103F  0.00\n",
       "2  1A2P       A   T105V  2.24\n",
       "3  1A2P       A   I109A  2.07\n",
       "4  1A2P       A   I109V  0.76\n",
       "5  1A2P       A    V10A  3.39\n",
       "6  1A2P       A    V10T  2.48\n",
       "7  1A2P       A   R110A  0.41\n",
       "8  1A2P       A    D12A  0.31\n",
       "9  1A2P       A    D12G  1.29"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"for df in ddg_data_dict.values():\\n    display(df.head(10))\";\n",
       "                var nbb_formatted_code = \"for df in ddg_data_dict.values():\\n    display(df.head(10))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for df in ddg_data_dict.values():\n",
    "    display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
