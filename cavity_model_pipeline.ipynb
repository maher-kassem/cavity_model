{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"import glob\\nimport os\\nimport random\\nfrom typing import Callable, List, Union\\n\\nimport numpy as np\\nimport pandas as pd\\nimport torch\\nfrom torch.utils.data import DataLoader, Dataset\\nfrom Bio.PDB.Polypeptide import index_to_one\\n\\nfrom cavity_model import (\\n    ResidueEnvironment,\\n    ResidueEnvironmentsDataset,\\n    ToTensor,\\n    CavityModel,\\n)\\n\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"import glob\\nimport os\\nimport random\\nfrom typing import Callable, List, Union\\n\\nimport numpy as np\\nimport pandas as pd\\nimport torch\\nfrom torch.utils.data import DataLoader, Dataset\\nfrom Bio.PDB.Polypeptide import index_to_one\\n\\nfrom cavity_model import (\\n    ResidueEnvironment,\\n    ResidueEnvironmentsDataset,\\n    ToTensor,\\n    CavityModel,\\n)\\n\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "from typing import Callable, List, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from Bio.PDB.Polypeptide import index_to_one\n",
    "\n",
    "from cavity_model import (\n",
    "    ResidueEnvironment,\n",
    "    ResidueEnvironmentsDataset,\n",
    "    ToTensor,\n",
    "    CavityModel,\n",
    ")\n",
    "\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cavity Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and process Cavity Model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# # Run shell script that takes a .txt file with PDBIDs as input.\\n# !./get_parse_pdbs_cavity_model.sh data/pdbids_010.txt\";\n",
       "                var nbb_formatted_code = \"# # Run shell script that takes a .txt file with PDBIDs as input.\\n# !./get_parse_pdbs_cavity_model.sh data/pdbids_010.txt\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Run shell script that takes a .txt file with PDBIDs as input.\n",
    "# !./get_parse_pdbs_cavity_model.sh data/pdbids_010.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables for Cavity Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"DEVICE = \\\"cpu\\\"  # \\\"cpu\\\" or \\\"cuda\\\"\\nTRAIN_VAL_SPLIT = 0.8\\nBATCH_SIZE = 100\\nLEARNING_RATE = 3e-4\\nEPOCHS = 10\\nPATIENCE_CUTOFF = 1\";\n",
       "                var nbb_formatted_code = \"DEVICE = \\\"cpu\\\"  # \\\"cpu\\\" or \\\"cuda\\\"\\nTRAIN_VAL_SPLIT = 0.8\\nBATCH_SIZE = 100\\nLEARNING_RATE = 3e-4\\nEPOCHS = 10\\nPATIENCE_CUTOFF = 1\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DEVICE = \"cpu\"  # \"cpu\" or \"cuda\"\n",
    "TRAIN_VAL_SPLIT = 0.8\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 3e-4\n",
    "EPOCHS = 10\n",
    "PATIENCE_CUTOFF = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Parsed PDBs and perform train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data set includes 8 pdbs with 4359 environments.\n",
      "Validation data set includes 2 pdbs with 899 environments.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"parsed_pdb_filenames = sorted(glob.glob(\\\"data/pdbs/parsed/*coord*\\\"))\\nrandom.shuffle(parsed_pdb_filenames)\\n\\nn_train_pdbs = int(len(parsed_pdb_filenames) * TRAIN_VAL_SPLIT)\\nfilenames_train = parsed_pdb_filenames[:n_train_pdbs]\\nfilenames_val = parsed_pdb_filenames[n_train_pdbs:]\\n\\nto_tensor_transformer = ToTensor(DEVICE)\\n\\ndataset_train = ResidueEnvironmentsDataset(\\n    filenames_train, transformer=to_tensor_transformer\\n)\\ndataset_val = ResidueEnvironmentsDataset(\\n    filenames_val, transformer=to_tensor_transformer\\n)\\n\\ndataloader_train = DataLoader(\\n    dataset_train,\\n    batch_size=BATCH_SIZE,\\n    shuffle=True,\\n    collate_fn=to_tensor_transformer.collate_cat,\\n    drop_last=True,\\n)\\ndataloader_val = DataLoader(\\n    dataset_val,\\n    batch_size=BATCH_SIZE,\\n    shuffle=True,\\n    collate_fn=to_tensor_transformer.collate_cat,\\n    drop_last=True,\\n)\\n\\nprint(\\n    f\\\"Training data set includes {len(filenames_train)} pdbs with \\\"\\n    f\\\"{len(dataset_train)} environments.\\\"\\n)\\nprint(\\n    f\\\"Validation data set includes {len(filenames_val)} pdbs with \\\"\\n    f\\\"{len(dataset_val)} environments.\\\"\\n)\";\n",
       "                var nbb_formatted_code = \"parsed_pdb_filenames = sorted(glob.glob(\\\"data/pdbs/parsed/*coord*\\\"))\\nrandom.shuffle(parsed_pdb_filenames)\\n\\nn_train_pdbs = int(len(parsed_pdb_filenames) * TRAIN_VAL_SPLIT)\\nfilenames_train = parsed_pdb_filenames[:n_train_pdbs]\\nfilenames_val = parsed_pdb_filenames[n_train_pdbs:]\\n\\nto_tensor_transformer = ToTensor(DEVICE)\\n\\ndataset_train = ResidueEnvironmentsDataset(\\n    filenames_train, transformer=to_tensor_transformer\\n)\\ndataset_val = ResidueEnvironmentsDataset(\\n    filenames_val, transformer=to_tensor_transformer\\n)\\n\\ndataloader_train = DataLoader(\\n    dataset_train,\\n    batch_size=BATCH_SIZE,\\n    shuffle=True,\\n    collate_fn=to_tensor_transformer.collate_cat,\\n    drop_last=True,\\n)\\ndataloader_val = DataLoader(\\n    dataset_val,\\n    batch_size=BATCH_SIZE,\\n    shuffle=True,\\n    collate_fn=to_tensor_transformer.collate_cat,\\n    drop_last=True,\\n)\\n\\nprint(\\n    f\\\"Training data set includes {len(filenames_train)} pdbs with \\\"\\n    f\\\"{len(dataset_train)} environments.\\\"\\n)\\nprint(\\n    f\\\"Validation data set includes {len(filenames_val)} pdbs with \\\"\\n    f\\\"{len(dataset_val)} environments.\\\"\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parsed_pdb_filenames = sorted(glob.glob(\"data/pdbs/parsed/*coord*\"))\n",
    "random.shuffle(parsed_pdb_filenames)\n",
    "\n",
    "n_train_pdbs = int(len(parsed_pdb_filenames) * TRAIN_VAL_SPLIT)\n",
    "filenames_train = parsed_pdb_filenames[:n_train_pdbs]\n",
    "filenames_val = parsed_pdb_filenames[n_train_pdbs:]\n",
    "\n",
    "to_tensor_transformer = ToTensor(DEVICE)\n",
    "\n",
    "dataset_train = ResidueEnvironmentsDataset(\n",
    "    filenames_train, transformer=to_tensor_transformer\n",
    ")\n",
    "dataset_val = ResidueEnvironmentsDataset(\n",
    "    filenames_val, transformer=to_tensor_transformer\n",
    ")\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=to_tensor_transformer.collate_cat,\n",
    "    drop_last=True,\n",
    ")\n",
    "dataloader_val = DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=to_tensor_transformer.collate_cat,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Training data set includes {len(filenames_train)} pdbs with \"\n",
    "    f\"{len(dataset_train)} environments.\"\n",
    ")\n",
    "print(\n",
    "    f\"Validation data set includes {len(filenames_val)} pdbs with \"\n",
    "    f\"{len(dataset_val)} environments.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Cavity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"def _train_step(\\n    cavity_model_net: CavityModel,\\n    optimizer: torch.optim.Adam,\\n    loss_function: torch.nn.CrossEntropyLoss,\\n) -> (torch.Tensor, float):\\n    \\\"\\\"\\\"\\n    Helper function to take a training step\\n    \\\"\\\"\\\"\\n    cavity_model_net.train()\\n    optimizer.zero_grad()\\n    batch_y_pred = cavity_model_net(batch_x)\\n    loss_batch = loss_function(batch_y_pred, torch.argmax(batch_y, dim=-1))\\n    loss_batch.backward()\\n    optimizer.step()\\n    return (batch_y_pred, loss_batch.detach().cpu().item())\\n\\n\\ndef _eval_loop(\\n    cavity_model_net: CavityModel,\\n    data_loader_val,\\n    loss_function: torch.nn.CrossEntropyLoss,\\n) -> (float, float):\\n    \\\"\\\"\\\"\\n    Helper function to perform an eval loop\\n    \\\"\\\"\\\"\\n    # Eval loop. Due to memory, we don't pass the whole eval set to the model\\n    labels_true_val = []\\n    labels_pred_val = []\\n    loss_batch_list_val = []\\n    for batch_x_val, batch_y_val in dataloader_val:\\n        cavity_model_net.eval()\\n        batch_y_pred_val = cavity_model_net(batch_x_val)\\n\\n        loss_batch_val = loss_function(\\n            batch_y_pred_val, torch.argmax(batch_y_val, dim=-1)\\n        )\\n        loss_batch_list_val.append(loss_batch_val.detach().cpu().item())\\n\\n        labels_true_val.append(torch.argmax(batch_y_val, dim=-1).detach().cpu().numpy())\\n        labels_pred_val.append(\\n            torch.argmax(batch_y_pred_val, dim=-1).detach().cpu().numpy()\\n        )\\n    acc_val = np.mean(\\n        (np.reshape(labels_true_val, -1) == np.reshape(labels_pred_val, -1))\\n    )\\n    loss_val = np.mean(loss_batch_list_val)\\n    return acc_val, loss_val\";\n",
       "                var nbb_formatted_code = \"def _train_step(\\n    cavity_model_net: CavityModel,\\n    optimizer: torch.optim.Adam,\\n    loss_function: torch.nn.CrossEntropyLoss,\\n) -> (torch.Tensor, float):\\n    \\\"\\\"\\\"\\n    Helper function to take a training step\\n    \\\"\\\"\\\"\\n    cavity_model_net.train()\\n    optimizer.zero_grad()\\n    batch_y_pred = cavity_model_net(batch_x)\\n    loss_batch = loss_function(batch_y_pred, torch.argmax(batch_y, dim=-1))\\n    loss_batch.backward()\\n    optimizer.step()\\n    return (batch_y_pred, loss_batch.detach().cpu().item())\\n\\n\\ndef _eval_loop(\\n    cavity_model_net: CavityModel,\\n    data_loader_val,\\n    loss_function: torch.nn.CrossEntropyLoss,\\n) -> (float, float):\\n    \\\"\\\"\\\"\\n    Helper function to perform an eval loop\\n    \\\"\\\"\\\"\\n    # Eval loop. Due to memory, we don't pass the whole eval set to the model\\n    labels_true_val = []\\n    labels_pred_val = []\\n    loss_batch_list_val = []\\n    for batch_x_val, batch_y_val in dataloader_val:\\n        cavity_model_net.eval()\\n        batch_y_pred_val = cavity_model_net(batch_x_val)\\n\\n        loss_batch_val = loss_function(\\n            batch_y_pred_val, torch.argmax(batch_y_val, dim=-1)\\n        )\\n        loss_batch_list_val.append(loss_batch_val.detach().cpu().item())\\n\\n        labels_true_val.append(torch.argmax(batch_y_val, dim=-1).detach().cpu().numpy())\\n        labels_pred_val.append(\\n            torch.argmax(batch_y_pred_val, dim=-1).detach().cpu().numpy()\\n        )\\n    acc_val = np.mean(\\n        (np.reshape(labels_true_val, -1) == np.reshape(labels_pred_val, -1))\\n    )\\n    loss_val = np.mean(loss_batch_list_val)\\n    return acc_val, loss_val\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _train_step(\n",
    "    cavity_model_net: CavityModel,\n",
    "    optimizer: torch.optim.Adam,\n",
    "    loss_function: torch.nn.CrossEntropyLoss,\n",
    ") -> (torch.Tensor, float):\n",
    "    \"\"\"\n",
    "    Helper function to take a training step\n",
    "    \"\"\"\n",
    "    cavity_model_net.train()\n",
    "    optimizer.zero_grad()\n",
    "    batch_y_pred = cavity_model_net(batch_x)\n",
    "    loss_batch = loss_function(batch_y_pred, torch.argmax(batch_y, dim=-1))\n",
    "    loss_batch.backward()\n",
    "    optimizer.step()\n",
    "    return (batch_y_pred, loss_batch.detach().cpu().item())\n",
    "\n",
    "\n",
    "def _eval_loop(\n",
    "    cavity_model_net: CavityModel,\n",
    "    data_loader_val,\n",
    "    loss_function: torch.nn.CrossEntropyLoss,\n",
    ") -> (float, float):\n",
    "    \"\"\"\n",
    "    Helper function to perform an eval loop\n",
    "    \"\"\"\n",
    "    # Eval loop. Due to memory, we don't pass the whole eval set to the model\n",
    "    labels_true_val = []\n",
    "    labels_pred_val = []\n",
    "    loss_batch_list_val = []\n",
    "    for batch_x_val, batch_y_val in dataloader_val:\n",
    "        cavity_model_net.eval()\n",
    "        batch_y_pred_val = cavity_model_net(batch_x_val)\n",
    "\n",
    "        loss_batch_val = loss_function(\n",
    "            batch_y_pred_val, torch.argmax(batch_y_val, dim=-1)\n",
    "        )\n",
    "        loss_batch_list_val.append(loss_batch_val.detach().cpu().item())\n",
    "\n",
    "        labels_true_val.append(torch.argmax(batch_y_val, dim=-1).detach().cpu().numpy())\n",
    "        labels_pred_val.append(\n",
    "            torch.argmax(batch_y_pred_val, dim=-1).detach().cpu().numpy()\n",
    "        )\n",
    "    acc_val = np.mean(\n",
    "        (np.reshape(labels_true_val, -1) == np.reshape(labels_pred_val, -1))\n",
    "    )\n",
    "    loss_val = np.mean(loss_batch_list_val)\n",
    "    return acc_val, loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0. Train loss: 2.459. Train Acc: 0.32. Val loss: 3.354. Val Acc 0.08\n",
      "Epoch  1. Train loss: 1.294. Train Acc: 0.76. Val loss: 2.789. Val Acc 0.17\n",
      "Epoch  2. Train loss: 0.747. Train Acc: 0.92. Val loss: 2.796. Val Acc 0.19\n",
      "Epoch  3. Train loss: 0.388. Train Acc: 0.99. Val loss: 2.884. Val Acc 0.17\n",
      "Early stopping activated.\n",
      "Best epoch idx: 1 with validation loss: 2.789 and model_path: cavity_models/model_epoch_01.pt\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# Define model\\ncavity_model_net = CavityModel(DEVICE).to(DEVICE)\\nloss_function = torch.nn.CrossEntropyLoss()\\noptimizer = torch.optim.Adam(cavity_model_net.parameters(), lr=LEARNING_RATE)\\n\\n# Create directory for model files\\nmodels_dirpath = \\\"cavity_models/\\\"\\nif not os.path.exists(models_dirpath):\\n    os.mkdir(models_dirpath)\\n\\n# Train loop\\ncurrent_best_epoch_idx = -1\\ncurrent_best_loss_val = 1e4\\npatience = 0\\nepoch_idx_to_model_path = {}\\nfor epoch in range(EPOCHS):\\n    labels_true = []\\n    labels_pred = []\\n    loss_batch_list = []\\n    for batch_x, batch_y in dataloader_train:\\n        # Take train step\\n        batch_y_pred, loss_batch = _train_step(\\n            cavity_model_net, optimizer, loss_function\\n        )\\n        loss_batch_list.append(loss_batch)\\n\\n        labels_true.append(torch.argmax(batch_y, dim=-1).detach().cpu().numpy())\\n        labels_pred.append(torch.argmax(batch_y_pred, dim=-1).detach().cpu().numpy())\\n\\n    # Train epoch metrics\\n    acc_train = np.mean((np.reshape(labels_true, -1) == np.reshape(labels_pred, -1)))\\n    loss_train = np.mean(loss_batch_list)\\n\\n    # Validation epoch metrics\\n    acc_val, loss_val = _eval_loop(cavity_model_net, dataloader_val, loss_function)\\n\\n    print(\\n        f\\\"Epoch {epoch:2d}. Train loss: {loss_train:5.3f}. \\\"\\n        f\\\"Train Acc: {acc_train:4.2f}. Val loss: {loss_val:5.3f}. \\\"\\n        f\\\"Val Acc {acc_val:4.2f}\\\"\\n    )\\n\\n    # Save model\\n    model_path = f\\\"cavity_models/model_epoch_{epoch:02d}.pt\\\"\\n    epoch_idx_to_model_path[epoch] = model_path\\n    torch.save(cavity_model_net.state_dict(), model_path)\\n\\n    # Early stopping\\n    if loss_val < current_best_loss_val:\\n        current_best_loss_val = loss_val\\n        current_best_epoch_idx = epoch\\n        patience = 0\\n    else:\\n        patience += 1\\n    if patience > PATIENCE_CUTOFF:\\n        print(f\\\"Early stopping activated.\\\")\\n        break\\n\\nprint(\\n    f\\\"Best epoch idx: {current_best_epoch_idx} with validation loss: \\\"\\n    f\\\"{current_best_loss_val:5.3f} and model_path: \\\"\\n    f\\\"{epoch_idx_to_model_path[current_best_epoch_idx]}\\\"\\n)\";\n",
       "                var nbb_formatted_code = \"# Define model\\ncavity_model_net = CavityModel(DEVICE).to(DEVICE)\\nloss_function = torch.nn.CrossEntropyLoss()\\noptimizer = torch.optim.Adam(cavity_model_net.parameters(), lr=LEARNING_RATE)\\n\\n# Create directory for model files\\nmodels_dirpath = \\\"cavity_models/\\\"\\nif not os.path.exists(models_dirpath):\\n    os.mkdir(models_dirpath)\\n\\n# Train loop\\ncurrent_best_epoch_idx = -1\\ncurrent_best_loss_val = 1e4\\npatience = 0\\nepoch_idx_to_model_path = {}\\nfor epoch in range(EPOCHS):\\n    labels_true = []\\n    labels_pred = []\\n    loss_batch_list = []\\n    for batch_x, batch_y in dataloader_train:\\n        # Take train step\\n        batch_y_pred, loss_batch = _train_step(\\n            cavity_model_net, optimizer, loss_function\\n        )\\n        loss_batch_list.append(loss_batch)\\n\\n        labels_true.append(torch.argmax(batch_y, dim=-1).detach().cpu().numpy())\\n        labels_pred.append(torch.argmax(batch_y_pred, dim=-1).detach().cpu().numpy())\\n\\n    # Train epoch metrics\\n    acc_train = np.mean((np.reshape(labels_true, -1) == np.reshape(labels_pred, -1)))\\n    loss_train = np.mean(loss_batch_list)\\n\\n    # Validation epoch metrics\\n    acc_val, loss_val = _eval_loop(cavity_model_net, dataloader_val, loss_function)\\n\\n    print(\\n        f\\\"Epoch {epoch:2d}. Train loss: {loss_train:5.3f}. \\\"\\n        f\\\"Train Acc: {acc_train:4.2f}. Val loss: {loss_val:5.3f}. \\\"\\n        f\\\"Val Acc {acc_val:4.2f}\\\"\\n    )\\n\\n    # Save model\\n    model_path = f\\\"cavity_models/model_epoch_{epoch:02d}.pt\\\"\\n    epoch_idx_to_model_path[epoch] = model_path\\n    torch.save(cavity_model_net.state_dict(), model_path)\\n\\n    # Early stopping\\n    if loss_val < current_best_loss_val:\\n        current_best_loss_val = loss_val\\n        current_best_epoch_idx = epoch\\n        patience = 0\\n    else:\\n        patience += 1\\n    if patience > PATIENCE_CUTOFF:\\n        print(f\\\"Early stopping activated.\\\")\\n        break\\n\\nprint(\\n    f\\\"Best epoch idx: {current_best_epoch_idx} with validation loss: \\\"\\n    f\\\"{current_best_loss_val:5.3f} and model_path: \\\"\\n    f\\\"{epoch_idx_to_model_path[current_best_epoch_idx]}\\\"\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define model\n",
    "cavity_model_net = CavityModel(DEVICE).to(DEVICE)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cavity_model_net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Create directory for model files\n",
    "models_dirpath = \"cavity_models/\"\n",
    "if not os.path.exists(models_dirpath):\n",
    "    os.mkdir(models_dirpath)\n",
    "\n",
    "# Train loop\n",
    "current_best_epoch_idx = -1\n",
    "current_best_loss_val = 1e4\n",
    "patience = 0\n",
    "epoch_idx_to_model_path = {}\n",
    "for epoch in range(EPOCHS):\n",
    "    labels_true = []\n",
    "    labels_pred = []\n",
    "    loss_batch_list = []\n",
    "    for batch_x, batch_y in dataloader_train:\n",
    "        # Take train step\n",
    "        batch_y_pred, loss_batch = _train_step(\n",
    "            cavity_model_net, optimizer, loss_function\n",
    "        )\n",
    "        loss_batch_list.append(loss_batch)\n",
    "\n",
    "        labels_true.append(torch.argmax(batch_y, dim=-1).detach().cpu().numpy())\n",
    "        labels_pred.append(torch.argmax(batch_y_pred, dim=-1).detach().cpu().numpy())\n",
    "\n",
    "    # Train epoch metrics\n",
    "    acc_train = np.mean((np.reshape(labels_true, -1) == np.reshape(labels_pred, -1)))\n",
    "    loss_train = np.mean(loss_batch_list)\n",
    "\n",
    "    # Validation epoch metrics\n",
    "    acc_val, loss_val = _eval_loop(cavity_model_net, dataloader_val, loss_function)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:2d}. Train loss: {loss_train:5.3f}. \"\n",
    "        f\"Train Acc: {acc_train:4.2f}. Val loss: {loss_val:5.3f}. \"\n",
    "        f\"Val Acc {acc_val:4.2f}\"\n",
    "    )\n",
    "\n",
    "    # Save model\n",
    "    model_path = f\"cavity_models/model_epoch_{epoch:02d}.pt\"\n",
    "    epoch_idx_to_model_path[epoch] = model_path\n",
    "    torch.save(cavity_model_net.state_dict(), model_path)\n",
    "\n",
    "    # Early stopping\n",
    "    if loss_val < current_best_loss_val:\n",
    "        current_best_loss_val = loss_val\n",
    "        current_best_epoch_idx = epoch\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "    if patience > PATIENCE_CUTOFF:\n",
    "        print(f\"Early stopping activated.\")\n",
    "        break\n",
    "\n",
    "print(\n",
    "    f\"Best epoch idx: {current_best_epoch_idx} with validation loss: \"\n",
    "    f\"{current_best_loss_val:5.3f} and model_path: \"\n",
    "    f\"{epoch_idx_to_model_path[current_best_epoch_idx]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ddG Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse PDBs for DMS, Guerois and Protein G data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 217;\n",
       "                var nbb_unformatted_code = \"# # Parse PDBs for which we have ddG data\\n# !./get_parse_pdbs_dowstream_task.sh\";\n",
       "                var nbb_formatted_code = \"# # Parse PDBs for which we have ddG data\\n# !./get_parse_pdbs_dowstream_task.sh\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Parse PDBs for which we have ddG data\n",
    "# !./get_parse_pdbs_dowstream_task.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 218;\n",
       "                var nbb_unformatted_code = \"# Create temporary residue environment datasets as dicts to more easily match ddG data\\nparsed_pdbs_wildcards = {\\n    \\\"dms\\\": \\\"data/data_dms/pdbs_parsed/*coord*\\\",\\n    \\\"protein_g\\\": \\\"data/data_protein_g/pdbs_parsed/*coord*\\\",\\n    \\\"guerois\\\": \\\"data/data_guerois/pdbs_parsed/*coord*\\\",\\n    \\\"symmetric\\\": \\\"data/data_symmetric/pdbs_parsed/*coord*\\\",\\n}\\n\\nresenv_datasets_look_up = {}\\nfor dataset_key, pdbs_wildcard in parsed_pdbs_wildcards.items():\\n    parsed_pdb_filenames = sorted(glob.glob(pdbs_wildcard))\\n    dataset = ResidueEnvironmentsDataset(parsed_pdb_filenames, transformer=None)\\n    dataset_look_up = {}\\n    for resenv in dataset:\\n        key = (\\n            f\\\"{resenv.pdb_id}{resenv.chain_id}_{resenv.pdb_residue_number}\\\"\\n            f\\\"{index_to_one(resenv.restype_index)}\\\"\\n        )\\n        dataset_look_up[key] = resenv\\n    resenv_datasets_look_up[dataset_key] = dataset_look_up\";\n",
       "                var nbb_formatted_code = \"# Create temporary residue environment datasets as dicts to more easily match ddG data\\nparsed_pdbs_wildcards = {\\n    \\\"dms\\\": \\\"data/data_dms/pdbs_parsed/*coord*\\\",\\n    \\\"protein_g\\\": \\\"data/data_protein_g/pdbs_parsed/*coord*\\\",\\n    \\\"guerois\\\": \\\"data/data_guerois/pdbs_parsed/*coord*\\\",\\n    \\\"symmetric\\\": \\\"data/data_symmetric/pdbs_parsed/*coord*\\\",\\n}\\n\\nresenv_datasets_look_up = {}\\nfor dataset_key, pdbs_wildcard in parsed_pdbs_wildcards.items():\\n    parsed_pdb_filenames = sorted(glob.glob(pdbs_wildcard))\\n    dataset = ResidueEnvironmentsDataset(parsed_pdb_filenames, transformer=None)\\n    dataset_look_up = {}\\n    for resenv in dataset:\\n        key = (\\n            f\\\"{resenv.pdb_id}{resenv.chain_id}_{resenv.pdb_residue_number}\\\"\\n            f\\\"{index_to_one(resenv.restype_index)}\\\"\\n        )\\n        dataset_look_up[key] = resenv\\n    resenv_datasets_look_up[dataset_key] = dataset_look_up\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create temporary residue environment datasets as dicts to more easily match ddG data\n",
    "parsed_pdbs_wildcards = {\n",
    "    \"dms\": \"data/data_dms/pdbs_parsed/*coord*\",\n",
    "    \"protein_g\": \"data/data_protein_g/pdbs_parsed/*coord*\",\n",
    "    \"guerois\": \"data/data_guerois/pdbs_parsed/*coord*\",\n",
    "    \"symmetric\": \"data/data_symmetric/pdbs_parsed/*coord*\",\n",
    "}\n",
    "\n",
    "resenv_datasets_look_up = {}\n",
    "for dataset_key, pdbs_wildcard in parsed_pdbs_wildcards.items():\n",
    "    parsed_pdb_filenames = sorted(glob.glob(pdbs_wildcard))\n",
    "    dataset = ResidueEnvironmentsDataset(parsed_pdb_filenames, transformer=None)\n",
    "    dataset_look_up = {}\n",
    "    for resenv in dataset:\n",
    "        key = (\n",
    "            f\"{resenv.pdb_id}{resenv.chain_id}_{resenv.pdb_residue_number}\"\n",
    "            f\"{index_to_one(resenv.restype_index)}\"\n",
    "        )\n",
    "        dataset_look_up[key] = resenv\n",
    "    resenv_datasets_look_up[dataset_key] = dataset_look_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 219;\n",
       "                var nbb_unformatted_code = \"# Load ddG data as pandas.DataFrame\\nddg_dataset_dict = {\\n    \\\"dms\\\": pd.read_csv(\\\"data/data_dms/ddgs_parsed.csv\\\"),\\n    \\\"protein_g\\\": pd.read_csv(\\\"data/data_protein_g/ddgs_parsed.csv\\\"),\\n    \\\"guerois\\\": pd.read_csv(\\\"data/data_guerois/ddgs_parsed.csv\\\"),\\n    \\\"symmetric_direct\\\": pd.read_csv(\\\"data/data_symmetric/ddgs_parsed_direct.csv\\\"),\\n    \\\"symmetric_inverse\\\": pd.read_csv(\\\"data/data_symmetric/ddgs_parsed_inverse.csv\\\"),\\n}\";\n",
       "                var nbb_formatted_code = \"# Load ddG data as pandas.DataFrame\\nddg_dataset_dict = {\\n    \\\"dms\\\": pd.read_csv(\\\"data/data_dms/ddgs_parsed.csv\\\"),\\n    \\\"protein_g\\\": pd.read_csv(\\\"data/data_protein_g/ddgs_parsed.csv\\\"),\\n    \\\"guerois\\\": pd.read_csv(\\\"data/data_guerois/ddgs_parsed.csv\\\"),\\n    \\\"symmetric_direct\\\": pd.read_csv(\\\"data/data_symmetric/ddgs_parsed_direct.csv\\\"),\\n    \\\"symmetric_inverse\\\": pd.read_csv(\\\"data/data_symmetric/ddgs_parsed_inverse.csv\\\"),\\n}\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load ddG data as pandas.DataFrame\n",
    "ddg_dataset_dict = {\n",
    "    \"dms\": pd.read_csv(\"data/data_dms/ddgs_parsed.csv\"),\n",
    "    \"protein_g\": pd.read_csv(\"data/data_protein_g/ddgs_parsed.csv\"),\n",
    "    \"guerois\": pd.read_csv(\"data/data_guerois/ddgs_parsed.csv\"),\n",
    "    \"symmetric_direct\": pd.read_csv(\"data/data_symmetric/ddgs_parsed_direct.csv\"),\n",
    "    \"symmetric_inverse\": pd.read_csv(\"data/data_symmetric/ddgs_parsed_inverse.csv\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped 1187 / 8096 data points from dataset dms due to missing residue definition in PDB structure.\n",
      "dropped    0 /  907 data points from dataset protein_g due to missing residue definition in PDB structure.\n",
      "dropped    0 /  911 data points from dataset guerois due to missing residue definition in PDB structure.\n",
      "dropped    0 /  342 data points from dataset symmetric_direct due to missing residue definition in PDB structure.\n",
      "dropped    1 /  342 data points from dataset symmetric_inverse due to missing residue definition in PDB structure.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 220;\n",
       "                var nbb_unformatted_code = \"# Add wt residue environments to dataframes\\nfor dataset_key in ddg_dataset_dict.keys():\\n    resenvs_ddg_data = []\\n    for idx, row in ddg_dataset_dict[dataset_key].iterrows():\\n        resenv_key = (\\n            f\\\"{row['pdbid']}{row['chainid']}_\\\"\\n            f\\\"{row['variant'][1:-1]}{row['variant'][0]}\\\"\\n        )\\n        try:\\n            if \\\"symmetric\\\" in dataset_key:\\n                dataset_key_adhoc_fix = \\\"symmetric\\\"\\n            else:\\n                dataset_key_adhoc_fix = dataset_key\\n            resenv = resenv_datasets_look_up[dataset_key_adhoc_fix][resenv_key]\\n            resenvs_ddg_data.append(resenv)\\n        except KeyError:\\n            resenvs_ddg_data.append(np.nan)\\n    ddg_dataset_dict[dataset_key][\\\"resenv\\\"] = resenvs_ddg_data\\n    n_datapoints_before = ddg_dataset_dict[dataset_key].shape[0]\\n    ddg_dataset_dict[dataset_key].dropna(inplace=True)\\n    n_datapoints_after = ddg_dataset_dict[dataset_key].shape[0]\\n    print(\\n        f\\\"dropped {n_datapoints_before - n_datapoints_after:4d} / \\\"\\n        f\\\"{n_datapoints_before:4d} data points from dataset {dataset_key} \\\"\\n        f\\\"due to missing residue definition in PDB structure.\\\"\\n    )\";\n",
       "                var nbb_formatted_code = \"# Add wt residue environments to dataframes\\nfor dataset_key in ddg_dataset_dict.keys():\\n    resenvs_ddg_data = []\\n    for idx, row in ddg_dataset_dict[dataset_key].iterrows():\\n        resenv_key = (\\n            f\\\"{row['pdbid']}{row['chainid']}_\\\"\\n            f\\\"{row['variant'][1:-1]}{row['variant'][0]}\\\"\\n        )\\n        try:\\n            if \\\"symmetric\\\" in dataset_key:\\n                dataset_key_adhoc_fix = \\\"symmetric\\\"\\n            else:\\n                dataset_key_adhoc_fix = dataset_key\\n            resenv = resenv_datasets_look_up[dataset_key_adhoc_fix][resenv_key]\\n            resenvs_ddg_data.append(resenv)\\n        except KeyError:\\n            resenvs_ddg_data.append(np.nan)\\n    ddg_dataset_dict[dataset_key][\\\"resenv\\\"] = resenvs_ddg_data\\n    n_datapoints_before = ddg_dataset_dict[dataset_key].shape[0]\\n    ddg_dataset_dict[dataset_key].dropna(inplace=True)\\n    n_datapoints_after = ddg_dataset_dict[dataset_key].shape[0]\\n    print(\\n        f\\\"dropped {n_datapoints_before - n_datapoints_after:4d} / \\\"\\n        f\\\"{n_datapoints_before:4d} data points from dataset {dataset_key} \\\"\\n        f\\\"due to missing residue definition in PDB structure.\\\"\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add wt residue environments to dataframes\n",
    "for dataset_key in ddg_dataset_dict.keys():\n",
    "    resenvs_ddg_data = []\n",
    "    for idx, row in ddg_dataset_dict[dataset_key].iterrows():\n",
    "        resenv_key = (\n",
    "            f\"{row['pdbid']}{row['chainid']}_\"\n",
    "            f\"{row['variant'][1:-1]}{row['variant'][0]}\"\n",
    "        )\n",
    "        try:\n",
    "            if \"symmetric\" in dataset_key:\n",
    "                dataset_key_adhoc_fix = \"symmetric\"\n",
    "            else:\n",
    "                dataset_key_adhoc_fix = dataset_key\n",
    "            resenv = resenv_datasets_look_up[dataset_key_adhoc_fix][resenv_key]\n",
    "            resenvs_ddg_data.append(resenv)\n",
    "        except KeyError:\n",
    "            resenvs_ddg_data.append(np.nan)\n",
    "    ddg_dataset_dict[dataset_key][\"resenv\"] = resenvs_ddg_data\n",
    "    n_datapoints_before = ddg_dataset_dict[dataset_key].shape[0]\n",
    "    ddg_dataset_dict[dataset_key].dropna(inplace=True)\n",
    "    n_datapoints_after = ddg_dataset_dict[dataset_key].shape[0]\n",
    "    print(\n",
    "        f\"dropped {n_datapoints_before - n_datapoints_after:4d} / \"\n",
    "        f\"{n_datapoints_before:4d} data points from dataset {dataset_key} \"\n",
    "        f\"due to missing residue definition in PDB structure.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
