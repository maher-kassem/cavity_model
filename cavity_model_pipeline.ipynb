{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"import glob\\nimport os\\nimport random\\nfrom typing import Callable, List, Union\\n\\nimport numpy as np\\nimport torch\\nfrom torch.utils.data import DataLoader, Dataset\\n\\nfrom cavity_model import (\\n    ResidueEnvironment,\\n    ResidueEnvironmentsDataset,\\n    ToTensor,\\n    CavityModel,\\n)\\n\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"import glob\\nimport os\\nimport random\\nfrom typing import Callable, List, Union\\n\\nimport numpy as np\\nimport torch\\nfrom torch.utils.data import DataLoader, Dataset\\n\\nfrom cavity_model import (\\n    ResidueEnvironment,\\n    ResidueEnvironmentsDataset,\\n    ToTensor,\\n    CavityModel,\\n)\\n\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "from typing import Callable, List, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from cavity_model import (\n",
    "    ResidueEnvironment,\n",
    "    ResidueEnvironmentsDataset,\n",
    "    ToTensor,\n",
    "    CavityModel,\n",
    ")\n",
    "\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded 4X2U.pdb to data/pdbs/raw/4X2U.pdb. 1/2.\n",
      "Successfully downloaded 2X96.pdb to data/pdbs/raw/2X96.pdb. 2/2.\n",
      "Successfully cleaned data/pdbs/raw/2X96.pdb and added it to data/pdbs/cleaned/. 1/2.\n",
      "Successfully cleaned data/pdbs/raw/4X2U.pdb and added it to data/pdbs/cleaned/. 2/2.\n",
      "Successfully parsed 2X96_clean.pdb and moved parsed file to data/pdbs/parsed. Finished 1/2.\n",
      "Successfully parsed 4X2U_clean.pdb and moved parsed file to data/pdbs/parsed. Finished 2/2.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# Run shell script that takes a .txt file with PDBIDs as input.\\n!./download_and_process_data.sh data/pdbids_002.txt\";\n",
       "                var nbb_formatted_code = \"# Run shell script that takes a .txt file with PDBIDs as input.\\n!./download_and_process_data.sh data/pdbids_002.txt\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run shell script that takes a .txt file with PDBIDs as input.\n",
    "!./download_and_process_data.sh data/pdbids_002.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"DEVICE = \\\"cuda\\\"  # \\\"cpu\\\" or \\\"cuda\\\"\\nBATCH_SIZE = 100\\nLEARNING_RATE = 3e-4\\nEPOCHS = 5\\nTRAIN_VAL_SPLIT = 0.8\";\n",
       "                var nbb_formatted_code = \"DEVICE = \\\"cuda\\\"  # \\\"cpu\\\" or \\\"cuda\\\"\\nBATCH_SIZE = 100\\nLEARNING_RATE = 3e-4\\nEPOCHS = 5\\nTRAIN_VAL_SPLIT = 0.8\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DEVICE = \"cuda\"  # \"cpu\" or \"cuda\"\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 3e-4\n",
    "EPOCHS = 5\n",
    "TRAIN_VAL_SPLIT = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse and train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data set includes 1 pdbs with 598 environments.\n",
      "Validation data set includes 1 pdbs with 889 environments.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"parsed_pdb_filenames = sorted(glob.glob(\\\"data/pdbs/parsed/*coord*\\\"))\\nrandom.shuffle(parsed_pdb_filenames)\\n\\nn_train_pdbs = int(len(parsed_pdb_filenames) * TRAIN_VAL_SPLIT)\\nfilenames_train = parsed_pdb_filenames[:n_train_pdbs]\\nfilenames_val = parsed_pdb_filenames[n_train_pdbs:]\\n\\nto_tensor_transformer = ToTensor(DEVICE)\\n\\ndataset_train = ResidueEnvironmentsDataset(\\n    filenames_train, transformer=to_tensor_transformer\\n)\\ndataset_val = ResidueEnvironmentsDataset(\\n    filenames_val, transformer=to_tensor_transformer\\n)\\n\\n\\ndataloader_train = DataLoader(\\n    dataset_train,\\n    batch_size=BATCH_SIZE,\\n    shuffle=True,\\n    collate_fn=to_tensor_transformer.collate_cat,\\n    drop_last=True,\\n)\\ndataloader_val = DataLoader(\\n    dataset_val,\\n    batch_size=BATCH_SIZE,\\n    shuffle=True,\\n    collate_fn=to_tensor_transformer.collate_cat,\\n    drop_last=True,\\n)\\n\\nprint(\\n    f\\\"Training data set includes {len(filenames_train)} pdbs with \\\"\\n    f\\\"{len(dataset_train)} environments.\\\"\\n)\\nprint(\\n    f\\\"Validation data set includes {len(filenames_val)} pdbs with \\\"\\n    f\\\"{len(dataset_val)} environments.\\\"\\n)\";\n",
       "                var nbb_formatted_code = \"parsed_pdb_filenames = sorted(glob.glob(\\\"data/pdbs/parsed/*coord*\\\"))\\nrandom.shuffle(parsed_pdb_filenames)\\n\\nn_train_pdbs = int(len(parsed_pdb_filenames) * TRAIN_VAL_SPLIT)\\nfilenames_train = parsed_pdb_filenames[:n_train_pdbs]\\nfilenames_val = parsed_pdb_filenames[n_train_pdbs:]\\n\\nto_tensor_transformer = ToTensor(DEVICE)\\n\\ndataset_train = ResidueEnvironmentsDataset(\\n    filenames_train, transformer=to_tensor_transformer\\n)\\ndataset_val = ResidueEnvironmentsDataset(\\n    filenames_val, transformer=to_tensor_transformer\\n)\\n\\n\\ndataloader_train = DataLoader(\\n    dataset_train,\\n    batch_size=BATCH_SIZE,\\n    shuffle=True,\\n    collate_fn=to_tensor_transformer.collate_cat,\\n    drop_last=True,\\n)\\ndataloader_val = DataLoader(\\n    dataset_val,\\n    batch_size=BATCH_SIZE,\\n    shuffle=True,\\n    collate_fn=to_tensor_transformer.collate_cat,\\n    drop_last=True,\\n)\\n\\nprint(\\n    f\\\"Training data set includes {len(filenames_train)} pdbs with \\\"\\n    f\\\"{len(dataset_train)} environments.\\\"\\n)\\nprint(\\n    f\\\"Validation data set includes {len(filenames_val)} pdbs with \\\"\\n    f\\\"{len(dataset_val)} environments.\\\"\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parsed_pdb_filenames = sorted(glob.glob(\"data/pdbs/parsed/*coord*\"))\n",
    "random.shuffle(parsed_pdb_filenames)\n",
    "\n",
    "n_train_pdbs = int(len(parsed_pdb_filenames) * TRAIN_VAL_SPLIT)\n",
    "filenames_train = parsed_pdb_filenames[:n_train_pdbs]\n",
    "filenames_val = parsed_pdb_filenames[n_train_pdbs:]\n",
    "\n",
    "to_tensor_transformer = ToTensor(DEVICE)\n",
    "\n",
    "dataset_train = ResidueEnvironmentsDataset(\n",
    "    filenames_train, transformer=to_tensor_transformer\n",
    ")\n",
    "dataset_val = ResidueEnvironmentsDataset(\n",
    "    filenames_val, transformer=to_tensor_transformer\n",
    ")\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=to_tensor_transformer.collate_cat,\n",
    "    drop_last=True,\n",
    ")\n",
    "dataloader_val = DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=to_tensor_transformer.collate_cat,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Training data set includes {len(filenames_train)} pdbs with \"\n",
    "    f\"{len(dataset_train)} environments.\"\n",
    ")\n",
    "print(\n",
    "    f\"Validation data set includes {len(filenames_val)} pdbs with \"\n",
    "    f\"{len(dataset_val)} environments.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1. Train loss: 1.292. Train Acc: 0.04. Val Acc: 0.06\n",
      "Epoch  2. Train loss: 0.787. Train Acc: 0.61. Val Acc: 0.06\n",
      "Epoch  3. Train loss: 0.552. Train Acc: 0.85. Val Acc: 0.07\n",
      "Epoch  4. Train loss: 0.406. Train Acc: 0.95. Val Acc: 0.06\n",
      "Epoch  5. Train loss: 0.302. Train Acc: 0.99. Val Acc: 0.07\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"def _train_step(\\n    cavity_model: CavityModel,\\n    optimizer: torch.optim.Adam,\\n    loss_function: torch.nn.CrossEntropyLoss,\\n) -> (torch.Tensor, float):\\n    cavity_model.train()\\n    optimizer.zero_grad()\\n    batch_y_pred = cavity_model(batch_x)\\n    loss_batch = loss_function(batch_y_pred, torch.argmax(batch_y, dim=-1))\\n    loss_batch.backward()\\n    optimizer.step()\\n    return (batch_y_pred, loss_batch.detach().cpu().item())\\n\\n\\n# Define model\\ncavity_model = CavityModel(DEVICE).to(DEVICE)\\nloss_function = torch.nn.CrossEntropyLoss()\\noptimizer = torch.optim.Adam(cavity_model.parameters(), lr=LEARNING_RATE)\\n\\n# Train loop\\nfor epoch in range(EPOCHS):\\n    loss_running_mean = 0.0\\n    labels_true = []\\n    labels_pred = []\\n    for batch_x, batch_y in dataloader_train:\\n        # Take train step\\n        batch_y_pred, loss_batch = _train_step(cavity_model, optimizer, loss_function)\\n\\n        # Exponential running mean for the loss\\n        loss_running_mean = loss_running_mean * 0.9 + loss_batch * 0.1\\n\\n        labels_true.append(torch.argmax(batch_y, dim=-1).detach().cpu().numpy())\\n        labels_pred.append(torch.argmax(batch_y_pred, dim=-1).detach().cpu().numpy())\\n    acc_train = np.mean((np.reshape(labels_true, -1) == np.reshape(labels_pred, -1)))\\n\\n    # Eval loop. Due to memory, we don't pass the whole data set to the model\\n    labels_true_val = []\\n    labels_pred_val = []\\n    for batch_x_val, batch_y_val in dataloader_val:\\n        cavity_model.eval()\\n        batch_y_pred_val = cavity_model(batch_x_val)\\n        labels_true_val.append(torch.argmax(batch_y_val, dim=-1).detach().cpu().numpy())\\n        labels_pred_val.append(\\n            torch.argmax(batch_y_pred_val, dim=-1).detach().cpu().numpy()\\n        )\\n    acc_val = np.mean(\\n        (np.reshape(labels_true_val, -1) == np.reshape(labels_pred_val, -1))\\n    )\\n\\n    print(\\n        f\\\"Epoch {epoch+1:2d}. Train loss: {loss_running_mean:5.3f}. \\\"\\n        f\\\"Train Acc: {acc_train:4.2f}. Val Acc: {acc_val:4.2f}\\\"\\n    )\";\n",
       "                var nbb_formatted_code = \"def _train_step(\\n    cavity_model: CavityModel,\\n    optimizer: torch.optim.Adam,\\n    loss_function: torch.nn.CrossEntropyLoss,\\n) -> (torch.Tensor, float):\\n    cavity_model.train()\\n    optimizer.zero_grad()\\n    batch_y_pred = cavity_model(batch_x)\\n    loss_batch = loss_function(batch_y_pred, torch.argmax(batch_y, dim=-1))\\n    loss_batch.backward()\\n    optimizer.step()\\n    return (batch_y_pred, loss_batch.detach().cpu().item())\\n\\n\\n# Define model\\ncavity_model = CavityModel(DEVICE).to(DEVICE)\\nloss_function = torch.nn.CrossEntropyLoss()\\noptimizer = torch.optim.Adam(cavity_model.parameters(), lr=LEARNING_RATE)\\n\\n# Train loop\\nfor epoch in range(EPOCHS):\\n    loss_running_mean = 0.0\\n    labels_true = []\\n    labels_pred = []\\n    for batch_x, batch_y in dataloader_train:\\n        # Take train step\\n        batch_y_pred, loss_batch = _train_step(cavity_model, optimizer, loss_function)\\n\\n        # Exponential running mean for the loss\\n        loss_running_mean = loss_running_mean * 0.9 + loss_batch * 0.1\\n\\n        labels_true.append(torch.argmax(batch_y, dim=-1).detach().cpu().numpy())\\n        labels_pred.append(torch.argmax(batch_y_pred, dim=-1).detach().cpu().numpy())\\n    acc_train = np.mean((np.reshape(labels_true, -1) == np.reshape(labels_pred, -1)))\\n\\n    # Eval loop. Due to memory, we don't pass the whole data set to the model\\n    labels_true_val = []\\n    labels_pred_val = []\\n    for batch_x_val, batch_y_val in dataloader_val:\\n        cavity_model.eval()\\n        batch_y_pred_val = cavity_model(batch_x_val)\\n        labels_true_val.append(torch.argmax(batch_y_val, dim=-1).detach().cpu().numpy())\\n        labels_pred_val.append(\\n            torch.argmax(batch_y_pred_val, dim=-1).detach().cpu().numpy()\\n        )\\n    acc_val = np.mean(\\n        (np.reshape(labels_true_val, -1) == np.reshape(labels_pred_val, -1))\\n    )\\n\\n    print(\\n        f\\\"Epoch {epoch+1:2d}. Train loss: {loss_running_mean:5.3f}. \\\"\\n        f\\\"Train Acc: {acc_train:4.2f}. Val Acc: {acc_val:4.2f}\\\"\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _train_step(\n",
    "    cavity_model: CavityModel,\n",
    "    optimizer: torch.optim.Adam,\n",
    "    loss_function: torch.nn.CrossEntropyLoss,\n",
    ") -> (torch.Tensor, float):\n",
    "    cavity_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    batch_y_pred = cavity_model(batch_x)\n",
    "    loss_batch = loss_function(batch_y_pred, torch.argmax(batch_y, dim=-1))\n",
    "    loss_batch.backward()\n",
    "    optimizer.step()\n",
    "    return (batch_y_pred, loss_batch.detach().cpu().item())\n",
    "\n",
    "\n",
    "# Define model\n",
    "cavity_model = CavityModel(DEVICE).to(DEVICE)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cavity_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_running_mean = 0.0\n",
    "    labels_true = []\n",
    "    labels_pred = []\n",
    "    for batch_x, batch_y in dataloader_train:\n",
    "        # Take train step\n",
    "        batch_y_pred, loss_batch = _train_step(cavity_model, optimizer, loss_function)\n",
    "\n",
    "        # Exponential running mean for the loss\n",
    "        loss_running_mean = loss_running_mean * 0.9 + loss_batch * 0.1\n",
    "\n",
    "        labels_true.append(torch.argmax(batch_y, dim=-1).detach().cpu().numpy())\n",
    "        labels_pred.append(torch.argmax(batch_y_pred, dim=-1).detach().cpu().numpy())\n",
    "    acc_train = np.mean((np.reshape(labels_true, -1) == np.reshape(labels_pred, -1)))\n",
    "\n",
    "    # Eval loop. Due to memory, we don't pass the whole data set to the model\n",
    "    labels_true_val = []\n",
    "    labels_pred_val = []\n",
    "    for batch_x_val, batch_y_val in dataloader_val:\n",
    "        cavity_model.eval()\n",
    "        batch_y_pred_val = cavity_model(batch_x_val)\n",
    "        labels_true_val.append(torch.argmax(batch_y_val, dim=-1).detach().cpu().numpy())\n",
    "        labels_pred_val.append(\n",
    "            torch.argmax(batch_y_pred_val, dim=-1).detach().cpu().numpy()\n",
    "        )\n",
    "    acc_val = np.mean(\n",
    "        (np.reshape(labels_true_val, -1) == np.reshape(labels_pred_val, -1))\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:2d}. Train loss: {loss_running_mean:5.3f}. \"\n",
    "        f\"Train Acc: {acc_train:4.2f}. Val Acc: {acc_val:4.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
