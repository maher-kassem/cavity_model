{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded 4X2U.pdb to data/raw/4X2U.pdb. 1/10.\n",
      "Successfully downloaded 2X96.pdb to data/raw/2X96.pdb. 2/10.\n",
      "Successfully downloaded 4MXD.pdb to data/raw/4MXD.pdb. 3/10.\n",
      "Successfully downloaded 3E9L.pdb to data/raw/3E9L.pdb. 4/10.\n",
      "Successfully downloaded 1UWC.pdb to data/raw/1UWC.pdb. 5/10.\n",
      "Successfully downloaded 4BGU.pdb to data/raw/4BGU.pdb. 6/10.\n",
      "Successfully downloaded 2YSW.pdb to data/raw/2YSW.pdb. 7/10.\n",
      "Successfully downloaded 4OW4.pdb to data/raw/4OW4.pdb. 8/10.\n",
      "Successfully downloaded 2V5E.pdb to data/raw/2V5E.pdb. 9/10.\n",
      "Successfully downloaded 1IXH.pdb to data/raw/1IXH.pdb. 10/10.\n",
      "Successfully cleaned data/raw/1IXH.pdb and added it to data/cleaned/. 1/10.\n",
      "Successfully cleaned data/raw/1UWC.pdb and added it to data/cleaned/. 2/10.\n",
      "Successfully cleaned data/raw/2V5E.pdb and added it to data/cleaned/. 3/10.\n",
      "Successfully cleaned data/raw/2X96.pdb and added it to data/cleaned/. 4/10.\n",
      "Successfully cleaned data/raw/2YSW.pdb and added it to data/cleaned/. 5/10.\n",
      "Successfully cleaned data/raw/3E9L.pdb and added it to data/cleaned/. 6/10.\n",
      "Successfully cleaned data/raw/4BGU.pdb and added it to data/cleaned/. 7/10.\n",
      "Successfully cleaned data/raw/4MXD.pdb and added it to data/cleaned/. 8/10.\n",
      "Successfully cleaned data/raw/4OW4.pdb and added it to data/cleaned/. 9/10.\n",
      "Successfully cleaned data/raw/4X2U.pdb and added it to data/cleaned/. 10/10.\n",
      "Successfully parsed 1IXH_clean.pdb and moved parsed file to data/parsed. Finished 1/10.\n",
      "Successfully parsed 1UWC_clean.pdb and moved parsed file to data/parsed. Finished 2/10.\n",
      "Successfully parsed 2V5E_clean.pdb and moved parsed file to data/parsed. Finished 3/10.\n",
      "Successfully parsed 2X96_clean.pdb and moved parsed file to data/parsed. Finished 4/10.\n",
      "Successfully parsed 2YSW_clean.pdb and moved parsed file to data/parsed. Finished 5/10.\n",
      "Successfully parsed 3E9L_clean.pdb and moved parsed file to data/parsed. Finished 6/10.\n",
      "Successfully parsed 4BGU_clean.pdb and moved parsed file to data/parsed. Finished 7/10.\n",
      "Successfully parsed 4MXD_clean.pdb and moved parsed file to data/parsed. Finished 8/10.\n",
      "Successfully parsed 4OW4_clean.pdb and moved parsed file to data/parsed. Finished 9/10.\n",
      "Successfully parsed 4X2U_clean.pdb and moved parsed file to data/parsed. Finished 10/10.\n"
     ]
    }
   ],
   "source": [
    "# Run shell script that takes a .txt file with PDBIDs as input.\n",
    "!./download_and_process_data.sh pdbids_010.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" # \"cpu\" or \"cuda\"\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.0003\n",
    "EPOCHS = 10\n",
    "TRAIN_VAL_SPLIT = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidueEnvironment:\n",
    "    \"\"\"\n",
    "    Residue environment class\n",
    "    \"\"\"\n",
    "    def __init__(self, coords_2d_arr: np.ndarray, atom_types: np.ndarray, aa_onehot: np.ndarray):\n",
    "        self._coords_2d_arr = coords_2d_arr\n",
    "        self._atom_types = atom_types\n",
    "        self._aa_onehot = aa_onehot\n",
    "        \n",
    "    @property\n",
    "    def coords_2d_arr(self):\n",
    "        return self._coords_2d_arr\n",
    "    \n",
    "    @property\n",
    "    def atom_types(self):\n",
    "        return self._atom_types\n",
    "    \n",
    "    @property\n",
    "    def aa_onehot(self):\n",
    "        return self._aa_onehot\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return (f\"<ResidueEnvironment objects with {self.coords_2d_arr.shape[0]} \"\n",
    "                f\"atoms and residue class {np.argmax(self.aa_onehot)}>\")\n",
    "\n",
    "        \n",
    "class ResidueEnvironmentsDataset(Dataset):\n",
    "    def __init__(self, npz_filenames: List[str], transform=None):\n",
    "        self._res_env_objects = self._parse_envs(npz_filenames)\n",
    "        self._transform = transform\n",
    "        \n",
    "    @property\n",
    "    def res_env_objects(self):\n",
    "        return self._res_env_objects\n",
    "    \n",
    "    @property\n",
    "    def transform(self):\n",
    "        return self._transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.res_env_objects)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.res_env_objects[idx]        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "    \n",
    "    def _parse_envs(self, npz_filenames):\n",
    "        res_env_objects = []\n",
    "        for i in range(len(npz_filenames)):\n",
    "            coordinate_features = np.load(npz_filenames[i])\n",
    "            atom_coords_prot_seq = coordinate_features[\"positions\"]\n",
    "            restype_onehots_prot_seq = coordinate_features[\"aa_onehot\"]\n",
    "            selector_prot_seq = coordinate_features[\"selector\"]\n",
    "            atom_types_flattened = coordinate_features[\"atom_types_numeric\"]\n",
    "            N_residues = selector_prot_seq.shape[0]\n",
    "            for resi_i in range(N_residues):\n",
    "                selector = selector_prot_seq[resi_i]\n",
    "                selector_masked = selector[selector>-1] # Remove Filler\n",
    "                coords_mask = atom_coords_prot_seq[resi_i, :, 0] != -99.0 # To remove filler\n",
    "                coords = atom_coords_prot_seq[resi_i][coords_mask]            \n",
    "                atom_types = atom_types_flattened[selector_masked]\n",
    "                restype_onehot = restype_onehots_prot_seq[resi_i]\n",
    "                res_env_objects.append(ResidueEnvironment(coords, atom_types, restype_onehot))\n",
    "        return res_env_objects\n",
    "\n",
    "\n",
    "class ToTensor:\n",
    "    def __call__(self, sample):        \n",
    "        sample_env = np.hstack([np.reshape(sample.atom_types, [-1, 1]),\n",
    "                               sample.coords_2d_arr])\n",
    "        \n",
    "        return {\"x_\": torch.tensor(sample_env, dtype=torch.float32).to(DEVICE), \n",
    "                \"y_\": torch.tensor(np.array(sample.aa_onehot), dtype=torch.float32).to(DEVICE)}\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_cat(batch):\n",
    "        target = torch.cat([torch.unsqueeze(b['y_'], 0) for b in batch], dim=0)\n",
    "            \n",
    "        # To collate the input, we need to add a column which \n",
    "        # specifies the environtment each atom belongs to\n",
    "        env_id_batch = []\n",
    "        for i, b in enumerate(batch):\n",
    "            n_atoms = b['x_'].shape[0]\n",
    "            env_id_arr = torch.zeros(n_atoms, dtype=torch.float32).to(DEVICE) + i\n",
    "            env_id_batch.append(torch.cat([torch.unsqueeze(env_id_arr, 1), b['x_']], dim=1))            \n",
    "        data = torch.cat(env_id_batch, dim=0)\n",
    "            \n",
    "        return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CavityModel(torch.nn.Module):\n",
    "    def __init__(self, device: str, sigma: float = 0.6):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.n_atom_types = 6\n",
    "        self.p = 1.0 # Bins pr. Anstrom\n",
    "        self.n = 18  # Grid dimension\n",
    "        self.sigma = sigma # width of gaussian\n",
    "        self.sigma_p = self.sigma*self.p\n",
    "        self.a = np.linspace(start=-self.n/2*self.p + self.p/2, \n",
    "                             stop=self.n/2*self.p - self.p/2, \n",
    "                             num=self.n) \n",
    "        self.xx, self.yy, self.zz = torch.tensor(np.meshgrid(self.a, self.a, self.a, indexing=\"ij\"),\n",
    "                                                 dtype = torch.float32).to(self.device)\n",
    "\n",
    "        self.conv1 = torch.nn.Sequential(torch.nn.Conv3d(6, 16, kernel_size=(3,3,3), stride=2, padding=1),\n",
    "                                         torch.nn.ReLU(), \n",
    "                                         torch.nn.BatchNorm3d(16))\n",
    "        self.conv2 = torch.nn.Sequential(torch.nn.Conv3d(16, 32, kernel_size=(3,3,3), stride=2, padding=0),\n",
    "                                         torch.nn.ReLU(), \n",
    "                                         torch.nn.BatchNorm3d(32))\n",
    "        self.conv3 = torch.nn.Sequential(torch.nn.Conv3d(32, 64, kernel_size=(3,3,3), stride=1, padding=1),\n",
    "                                         torch.nn.ReLU(), \n",
    "                                         torch.nn.BatchNorm3d(64),\n",
    "                                         torch.nn.Flatten())\n",
    "        self.dense1 = torch.nn.Sequential(torch.nn.Linear(in_features=4096, out_features=128),\n",
    "                                          torch.nn.ReLU(), \n",
    "                                          torch.nn.BatchNorm1d(128))\n",
    "        self.dense2 = torch.nn.Linear(in_features=128, out_features=21)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self._gaussian_blurring(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x \n",
    "\n",
    "    def _gaussian_blurring(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        current_batch_size = torch.unique(x[:, 0]).shape[0]\n",
    "        fields_torch = torch.zeros((current_batch_size, self.n_atom_types, self.n, self.n, self.n)).to(DEVICE)\n",
    "        for j in range(self.n_atom_types):\n",
    "            mask_j = x[:,1]==j\n",
    "            atom_type_j_data = x[mask_j]\n",
    "            if atom_type_j_data.shape[0] > 0:\n",
    "                pos = atom_type_j_data[:, 2:]\n",
    "                density = torch.exp(-((torch.reshape(self.xx, [-1, 1]) - pos[:,0])**2 +\\\n",
    "                                      (torch.reshape(self.yy, [-1, 1]) - pos[:,1])**2 +\\\n",
    "                                      (torch.reshape(self.zz, [-1, 1]) - pos[:,2])**2) / (2 * self.sigma_p**2))\n",
    "                # Normalize each atom to 1\n",
    "                density /= torch.sum(density, dim=0)\n",
    "                # Since column 0 of atom_type_j_data is sorted\n",
    "                # I can use a trick to detect the boundaries based\n",
    "                # on the change from one value to another.\n",
    "                change_mask_j = (atom_type_j_data[:,0][:-1] != atom_type_j_data[:,0][1:]) # detect change in column 0\n",
    "                # Add begin- and end indices\n",
    "                ranges_i = torch.cat([torch.tensor([0]),\n",
    "                                      torch.arange(atom_type_j_data.shape[0]-1)[change_mask_j]+1, \n",
    "                                      torch.tensor([atom_type_j_data.shape[0]]) ])\n",
    "                for i in range(ranges_i.shape[0]):\n",
    "                    if i < ranges_i.shape[0] - 1:\n",
    "                        index_0, index_1 = ranges_i[i], ranges_i[i+1]\n",
    "                        fields = torch.reshape(torch.sum(density[:,index_0:index_1], dim = 1), \n",
    "                                               [self.n, self.n, self.n])\n",
    "                        fields_torch[i,j,:,:,:] = fields\n",
    "        return fields_torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse and train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data set includes 8 pdbs with 4414 environments.\n",
      "Validation data setincludes 2 pdbs with 844 environments.\n"
     ]
    }
   ],
   "source": [
    "parsed_pdb_filenames = sorted(glob.glob(\"data/parsed/*coord*\"))\n",
    "random.shuffle(parsed_pdb_filenames)\n",
    "\n",
    "n_train_pdbs = int(len(parsed_pdb_filenames)*TRAIN_VAL_SPLIT)\n",
    "filenames_train = parsed_pdb_filenames[:n_train_pdbs]\n",
    "filenames_val = parsed_pdb_filenames[n_train_pdbs:]\n",
    "\n",
    "data_set_train = ResidueEnvironmentsDataset(filenames_train, transform=ToTensor())\n",
    "data_set_val = ResidueEnvironmentsDataset(filenames_val, transform=ToTensor())\n",
    "\n",
    "dataloader_train = DataLoader(data_set_train, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                              collate_fn=ToTensor.collate_cat, drop_last=True)\n",
    "dataloader_val = DataLoader(data_set_val, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                            collate_fn=ToTensor.collate_cat, drop_last=True)\n",
    "\n",
    "print(f\"Training data set includes {len(filenames_train)} pdbs with {len(data_set_train)} environments.\")\n",
    "print(f\"Validation data setincludes {len(filenames_val)} pdbs with {len(data_set_val)} environments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate model, loss and optimizer\n",
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1. Train loss: 2.221. Train Acc: 0.30. Val Acc: 0.11\n",
      "Epoch  2. Train loss: 1.257. Train Acc: 0.75. Val Acc: 0.20\n",
      "Epoch  3. Train loss: 0.736. Train Acc: 0.92. Val Acc: 0.20\n",
      "Epoch  4. Train loss: 0.395. Train Acc: 0.98. Val Acc: 0.20\n",
      "Epoch  5. Train loss: 0.205. Train Acc: 1.00. Val Acc: 0.21\n",
      "Epoch  6. Train loss: 0.115. Train Acc: 1.00. Val Acc: 0.21\n",
      "Epoch  7. Train loss: 0.073. Train Acc: 1.00. Val Acc: 0.21\n",
      "Epoch  8. Train loss: 0.052. Train Acc: 1.00. Val Acc: 0.21\n",
      "Epoch  9. Train loss: 0.040. Train Acc: 1.00. Val Acc: 0.22\n",
      "Epoch 10. Train loss: 0.032. Train Acc: 1.00. Val Acc: 0.21\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "cavity_model = CavityModel(DEVICE).to(DEVICE)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cavity_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train loop\n",
    "    loss_running_mean = 0.0\n",
    "    labels_true = []\n",
    "    labels_pred = []    \n",
    "    for batch_x, batch_y in dataloader_train:\n",
    "        cavity_model.train()\n",
    "        optimizer.zero_grad()    \n",
    "        batch_y_pred = cavity_model(batch_x)\n",
    "        loss_batch = loss(batch_y_pred, torch.argmax(batch_y, dim=-1))\n",
    "        loss_batch.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Exponential running mean for the loss\n",
    "        loss_running_mean = loss_running_mean*0.9 + loss_batch*0.1\n",
    "        \n",
    "        labels_true.append(torch.argmax(batch_y, dim=-1).detach().cpu().numpy())\n",
    "        labels_pred.append(torch.argmax(batch_y_pred, dim=-1).detach().cpu().numpy())\n",
    "    acc_train = np.mean((np.reshape(labels_true, -1) == np.reshape(labels_pred, -1)))\n",
    "    \n",
    "    # Eval loop. Due to memory, we don't want to pass the whole eval data set in one go\n",
    "    labels_true_val = []\n",
    "    labels_pred_val = []\n",
    "    for batch_x_val, batch_y_val in dataloader_val:\n",
    "        cavity_model.eval()\n",
    "        batch_y_pred_val = cavity_model(batch_x_val)\n",
    "        labels_true_val.append(torch.argmax(batch_y_val, dim=-1).detach().cpu().numpy())\n",
    "        labels_pred_val.append(torch.argmax(batch_y_pred_val, dim=-1).detach().cpu().numpy())\n",
    "    acc_val = np.mean((np.reshape(labels_true_val, -1) == np.reshape(labels_pred_val, -1)))\n",
    "\n",
    "    print(f\"Epoch {epoch+1:2d}. Train loss: {loss_running_mean:5.3f}. \"\n",
    "          f\"Train Acc: {acc_train:4.2f}. Val Acc: {acc_val:4.2f}\")   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
