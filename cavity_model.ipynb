{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded 4X2U.pdb to data/raw/4X2U.pdb. 1/50.\n",
      "Successfully downloaded 2X96.pdb to data/raw/2X96.pdb. 2/50.\n",
      "Successfully downloaded 4MXD.pdb to data/raw/4MXD.pdb. 3/50.\n",
      "Successfully downloaded 3E9L.pdb to data/raw/3E9L.pdb. 4/50.\n",
      "Successfully downloaded 1UWC.pdb to data/raw/1UWC.pdb. 5/50.\n",
      "Successfully downloaded 4BGU.pdb to data/raw/4BGU.pdb. 6/50.\n",
      "Successfully downloaded 2YSW.pdb to data/raw/2YSW.pdb. 7/50.\n",
      "Successfully downloaded 4OW4.pdb to data/raw/4OW4.pdb. 8/50.\n",
      "Successfully downloaded 2V5E.pdb to data/raw/2V5E.pdb. 9/50.\n",
      "Successfully downloaded 1IXH.pdb to data/raw/1IXH.pdb. 10/50.\n",
      "Successfully downloaded 3ZR9.pdb to data/raw/3ZR9.pdb. 11/50.\n",
      "Successfully downloaded 4O7Q.pdb to data/raw/4O7Q.pdb. 12/50.\n",
      "Successfully downloaded 3OBL.pdb to data/raw/3OBL.pdb. 13/50.\n",
      "Successfully downloaded 2YVP.pdb to data/raw/2YVP.pdb. 14/50.\n",
      "Successfully downloaded 1UNK.pdb to data/raw/1UNK.pdb. 15/50.\n",
      "Successfully downloaded 5B2H.pdb to data/raw/5B2H.pdb. 16/50.\n",
      "Successfully downloaded 5FEN.pdb to data/raw/5FEN.pdb. 17/50.\n",
      "Successfully downloaded 2CN4.pdb to data/raw/2CN4.pdb. 18/50.\n",
      "Successfully downloaded 1LDD.pdb to data/raw/1LDD.pdb. 19/50.\n",
      "Successfully downloaded 2PIA.pdb to data/raw/2PIA.pdb. 20/50.\n",
      "Successfully downloaded 3MX7.pdb to data/raw/3MX7.pdb. 21/50.\n",
      "Successfully downloaded 2XU3.pdb to data/raw/2XU3.pdb. 22/50.\n",
      "Successfully downloaded 4DPB.pdb to data/raw/4DPB.pdb. 23/50.\n",
      "Successfully downloaded 1F47.pdb to data/raw/1F47.pdb. 24/50.\n",
      "Successfully downloaded 3U4Z.pdb to data/raw/3U4Z.pdb. 25/50.\n",
      "Successfully downloaded 1IQQ.pdb to data/raw/1IQQ.pdb. 26/50.\n",
      "Successfully downloaded 4FIV.pdb to data/raw/4FIV.pdb. 27/50.\n",
      "Successfully downloaded 2QSK.pdb to data/raw/2QSK.pdb. 28/50.\n",
      "Successfully downloaded 1QQS.pdb to data/raw/1QQS.pdb. 29/50.\n",
      "Successfully downloaded 1DAB.pdb to data/raw/1DAB.pdb. 30/50.\n",
      "Successfully downloaded 1LXY.pdb to data/raw/1LXY.pdb. 31/50.\n",
      "Successfully downloaded 1Z2U.pdb to data/raw/1Z2U.pdb. 32/50.\n",
      "Successfully downloaded 2HLC.pdb to data/raw/2HLC.pdb. 33/50.\n",
      "Successfully downloaded 1TU7.pdb to data/raw/1TU7.pdb. 34/50.\n",
      "Successfully downloaded 2GDM.pdb to data/raw/2GDM.pdb. 35/50.\n",
      "Successfully downloaded 3RBA.pdb to data/raw/3RBA.pdb. 36/50.\n",
      "Successfully downloaded 1GYX.pdb to data/raw/1GYX.pdb. 37/50.\n",
      "Successfully downloaded 3EF4.pdb to data/raw/3EF4.pdb. 38/50.\n",
      "Successfully downloaded 1HRD.pdb to data/raw/1HRD.pdb. 39/50.\n",
      "Successfully downloaded 1HI9.pdb to data/raw/1HI9.pdb. 40/50.\n",
      "Successfully downloaded 4UPG.pdb to data/raw/4UPG.pdb. 41/50.\n",
      "Successfully downloaded 4ZUR.pdb to data/raw/4ZUR.pdb. 42/50.\n",
      "Successfully downloaded 1CPQ.pdb to data/raw/1CPQ.pdb. 43/50.\n",
      "Successfully downloaded 3AU0.pdb to data/raw/3AU0.pdb. 44/50.\n",
      "Successfully downloaded 3IPJ.pdb to data/raw/3IPJ.pdb. 45/50.\n",
      "Successfully downloaded 4BGV.pdb to data/raw/4BGV.pdb. 46/50.\n",
      "Successfully downloaded 5GSM.pdb to data/raw/5GSM.pdb. 47/50.\n",
      "Successfully downloaded 4RSX.pdb to data/raw/4RSX.pdb. 48/50.\n",
      "Successfully downloaded 1UAS.pdb to data/raw/1UAS.pdb. 49/50.\n",
      "Successfully downloaded 2CB5.pdb to data/raw/2CB5.pdb. 50/50.\n",
      "Successfully cleaned data/raw/1CPQ.pdb to data/cleaned/data/raw/1CPQ.pdb.pdb. 1/50.\n",
      "Successfully cleaned data/raw/1DAB.pdb to data/cleaned/data/raw/1DAB.pdb.pdb. 2/50.\n",
      "Successfully cleaned data/raw/1F47.pdb to data/cleaned/data/raw/1F47.pdb.pdb. 3/50.\n",
      "Successfully cleaned data/raw/1GYX.pdb to data/cleaned/data/raw/1GYX.pdb.pdb. 4/50.\n",
      "Successfully cleaned data/raw/1HI9.pdb to data/cleaned/data/raw/1HI9.pdb.pdb. 5/50.\n",
      "Successfully cleaned data/raw/1HRD.pdb to data/cleaned/data/raw/1HRD.pdb.pdb. 6/50.\n",
      "Successfully cleaned data/raw/1IQQ.pdb to data/cleaned/data/raw/1IQQ.pdb.pdb. 7/50.\n",
      "Successfully cleaned data/raw/1IXH.pdb to data/cleaned/data/raw/1IXH.pdb.pdb. 8/50.\n",
      "Successfully cleaned data/raw/1LDD.pdb to data/cleaned/data/raw/1LDD.pdb.pdb. 9/50.\n",
      "Successfully cleaned data/raw/1LXY.pdb to data/cleaned/data/raw/1LXY.pdb.pdb. 10/50.\n",
      "Successfully cleaned data/raw/1QQS.pdb to data/cleaned/data/raw/1QQS.pdb.pdb. 11/50.\n",
      "Successfully cleaned data/raw/1TU7.pdb to data/cleaned/data/raw/1TU7.pdb.pdb. 12/50.\n",
      "Successfully cleaned data/raw/1UAS.pdb to data/cleaned/data/raw/1UAS.pdb.pdb. 13/50.\n",
      "Successfully cleaned data/raw/1UNK.pdb to data/cleaned/data/raw/1UNK.pdb.pdb. 14/50.\n",
      "Successfully cleaned data/raw/1UWC.pdb to data/cleaned/data/raw/1UWC.pdb.pdb. 15/50.\n",
      "Successfully cleaned data/raw/1Z2U.pdb to data/cleaned/data/raw/1Z2U.pdb.pdb. 16/50.\n",
      "Successfully cleaned data/raw/2CB5.pdb to data/cleaned/data/raw/2CB5.pdb.pdb. 17/50.\n",
      "Successfully cleaned data/raw/2CN4.pdb to data/cleaned/data/raw/2CN4.pdb.pdb. 18/50.\n",
      "Successfully cleaned data/raw/2GDM.pdb to data/cleaned/data/raw/2GDM.pdb.pdb. 19/50.\n",
      "Successfully cleaned data/raw/2HLC.pdb to data/cleaned/data/raw/2HLC.pdb.pdb. 20/50.\n",
      "Successfully cleaned data/raw/2PIA.pdb to data/cleaned/data/raw/2PIA.pdb.pdb. 21/50.\n",
      "Successfully cleaned data/raw/2QSK.pdb to data/cleaned/data/raw/2QSK.pdb.pdb. 22/50.\n",
      "Successfully cleaned data/raw/2V5E.pdb to data/cleaned/data/raw/2V5E.pdb.pdb. 23/50.\n",
      "Successfully cleaned data/raw/2X96.pdb to data/cleaned/data/raw/2X96.pdb.pdb. 24/50.\n",
      "Successfully cleaned data/raw/2XU3.pdb to data/cleaned/data/raw/2XU3.pdb.pdb. 25/50.\n",
      "Successfully cleaned data/raw/2YSW.pdb to data/cleaned/data/raw/2YSW.pdb.pdb. 26/50.\n",
      "Successfully cleaned data/raw/2YVP.pdb to data/cleaned/data/raw/2YVP.pdb.pdb. 27/50.\n",
      "Successfully cleaned data/raw/3AU0.pdb to data/cleaned/data/raw/3AU0.pdb.pdb. 28/50.\n",
      "Successfully cleaned data/raw/3E9L.pdb to data/cleaned/data/raw/3E9L.pdb.pdb. 29/50.\n",
      "Successfully cleaned data/raw/3EF4.pdb to data/cleaned/data/raw/3EF4.pdb.pdb. 30/50.\n",
      "Successfully cleaned data/raw/3IPJ.pdb to data/cleaned/data/raw/3IPJ.pdb.pdb. 31/50.\n",
      "Successfully cleaned data/raw/3MX7.pdb to data/cleaned/data/raw/3MX7.pdb.pdb. 32/50.\n",
      "Successfully cleaned data/raw/3OBL.pdb to data/cleaned/data/raw/3OBL.pdb.pdb. 33/50.\n",
      "Successfully cleaned data/raw/3RBA.pdb to data/cleaned/data/raw/3RBA.pdb.pdb. 34/50.\n",
      "Successfully cleaned data/raw/3U4Z.pdb to data/cleaned/data/raw/3U4Z.pdb.pdb. 35/50.\n",
      "Successfully cleaned data/raw/3ZR9.pdb to data/cleaned/data/raw/3ZR9.pdb.pdb. 36/50.\n",
      "Successfully cleaned data/raw/4BGU.pdb to data/cleaned/data/raw/4BGU.pdb.pdb. 37/50.\n",
      "Successfully cleaned data/raw/4BGV.pdb to data/cleaned/data/raw/4BGV.pdb.pdb. 38/50.\n",
      "Successfully cleaned data/raw/4DPB.pdb to data/cleaned/data/raw/4DPB.pdb.pdb. 39/50.\n",
      "Successfully cleaned data/raw/4FIV.pdb to data/cleaned/data/raw/4FIV.pdb.pdb. 40/50.\n",
      "Successfully cleaned data/raw/4MXD.pdb to data/cleaned/data/raw/4MXD.pdb.pdb. 41/50.\n",
      "Successfully cleaned data/raw/4O7Q.pdb to data/cleaned/data/raw/4O7Q.pdb.pdb. 42/50.\n",
      "Successfully cleaned data/raw/4OW4.pdb to data/cleaned/data/raw/4OW4.pdb.pdb. 43/50.\n",
      "Successfully cleaned data/raw/4RSX.pdb to data/cleaned/data/raw/4RSX.pdb.pdb. 44/50.\n",
      "Successfully cleaned data/raw/4UPG.pdb to data/cleaned/data/raw/4UPG.pdb.pdb. 45/50.\n",
      "Successfully cleaned data/raw/4X2U.pdb to data/cleaned/data/raw/4X2U.pdb.pdb. 46/50.\n",
      "Successfully cleaned data/raw/4ZUR.pdb to data/cleaned/data/raw/4ZUR.pdb.pdb. 47/50.\n",
      "Successfully cleaned data/raw/5B2H.pdb to data/cleaned/data/raw/5B2H.pdb.pdb. 48/50.\n",
      "Successfully cleaned data/raw/5FEN.pdb to data/cleaned/data/raw/5FEN.pdb.pdb. 49/50.\n",
      "Successfully cleaned data/raw/5GSM.pdb to data/cleaned/data/raw/5GSM.pdb.pdb. 50/50.\n",
      "Successfully extracted environments from 1CPQ_clean.pdb. Finished 1/50.\n",
      "Successfully extracted environments from 1DAB_clean.pdb. Finished 2/50.\n",
      "Successfully extracted environments from 1F47_clean.pdb. Finished 3/50.\n",
      "Successfully extracted environments from 1GYX_clean.pdb. Finished 4/50.\n",
      "Successfully extracted environments from 1HI9_clean.pdb. Finished 5/50.\n",
      "Successfully extracted environments from 1HRD_clean.pdb. Finished 6/50.\n",
      "Successfully extracted environments from 1IQQ_clean.pdb. Finished 7/50.\n",
      "Successfully extracted environments from 1IXH_clean.pdb. Finished 8/50.\n",
      "Successfully extracted environments from 1LDD_clean.pdb. Finished 9/50.\n",
      "Successfully extracted environments from 1LXY_clean.pdb. Finished 10/50.\n",
      "Successfully extracted environments from 1QQS_clean.pdb. Finished 11/50.\n",
      "Successfully extracted environments from 1TU7_clean.pdb. Finished 12/50.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted environments from 1UAS_clean.pdb. Finished 13/50.\n",
      "Successfully extracted environments from 1UNK_clean.pdb. Finished 14/50.\n",
      "Successfully extracted environments from 1UWC_clean.pdb. Finished 15/50.\n",
      "Successfully extracted environments from 1Z2U_clean.pdb. Finished 16/50.\n",
      "Successfully extracted environments from 2CB5_clean.pdb. Finished 17/50.\n",
      "Successfully extracted environments from 2CN4_clean.pdb. Finished 18/50.\n",
      "Successfully extracted environments from 2GDM_clean.pdb. Finished 19/50.\n",
      "Successfully extracted environments from 2HLC_clean.pdb. Finished 20/50.\n",
      "Successfully extracted environments from 2PIA_clean.pdb. Finished 21/50.\n",
      "Successfully extracted environments from 2QSK_clean.pdb. Finished 22/50.\n",
      "Successfully extracted environments from 2V5E_clean.pdb. Finished 23/50.\n",
      "Successfully extracted environments from 2X96_clean.pdb. Finished 24/50.\n",
      "Successfully extracted environments from 2XU3_clean.pdb. Finished 25/50.\n",
      "Successfully extracted environments from 2YSW_clean.pdb. Finished 26/50.\n",
      "Successfully extracted environments from 2YVP_clean.pdb. Finished 27/50.\n",
      "Successfully extracted environments from 3AU0_clean.pdb. Finished 28/50.\n",
      "Successfully extracted environments from 3E9L_clean.pdb. Finished 29/50.\n",
      "Successfully extracted environments from 3EF4_clean.pdb. Finished 30/50.\n",
      "Successfully extracted environments from 3IPJ_clean.pdb. Finished 31/50.\n",
      "Successfully extracted environments from 3MX7_clean.pdb. Finished 32/50.\n",
      "Successfully extracted environments from 3OBL_clean.pdb. Finished 33/50.\n",
      "Successfully extracted environments from 3RBA_clean.pdb. Finished 34/50.\n",
      "Successfully extracted environments from 3U4Z_clean.pdb. Finished 35/50.\n",
      "Successfully extracted environments from 3ZR9_clean.pdb. Finished 36/50.\n",
      "Successfully extracted environments from 4BGU_clean.pdb. Finished 37/50.\n",
      "Successfully extracted environments from 4BGV_clean.pdb. Finished 38/50.\n",
      "Successfully extracted environments from 4DPB_clean.pdb. Finished 39/50.\n",
      "Successfully extracted environments from 4FIV_clean.pdb. Finished 40/50.\n",
      "Successfully extracted environments from 4MXD_clean.pdb. Finished 41/50.\n",
      "Successfully extracted environments from 4O7Q_clean.pdb. Finished 42/50.\n",
      "Successfully extracted environments from 4OW4_clean.pdb. Finished 43/50.\n",
      "Successfully extracted environments from 4RSX_clean.pdb. Finished 44/50.\n",
      "Successfully extracted environments from 4UPG_clean.pdb. Finished 45/50.\n",
      "Successfully extracted environments from 4X2U_clean.pdb. Finished 46/50.\n",
      "Successfully extracted environments from 4ZUR_clean.pdb. Finished 47/50.\n",
      "Successfully extracted environments from 5B2H_clean.pdb. Finished 48/50.\n",
      "Successfully extracted environments from 5FEN_clean.pdb. Finished 49/50.\n",
      "Successfully extracted environments from 5GSM_clean.pdb. Finished 50/50.\n"
     ]
    }
   ],
   "source": [
    "# Run shell script that takes a .txt file with PDBIDs as input.\n",
    "!./download_and_process_data.sh pdbids_050.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" # \"cpu\" or \"cuda\"\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.0003\n",
    "EPOCHS = 5\n",
    "TRAIN_VAL_SPLIT = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidueEnvironment:\n",
    "    \"\"\"\n",
    "    Residue environment class\n",
    "    \"\"\"\n",
    "    def __init__(self, coords_2d_arr: np.ndarray, atom_types: np.ndarray, aa_onehot: np.ndarray):\n",
    "        self._coords_2d_arr = coords_2d_arr\n",
    "        self._atom_types = atom_types\n",
    "        self._aa_onehot = aa_onehot\n",
    "        \n",
    "    @property\n",
    "    def coords_2d_arr(self):\n",
    "        return self._coords_2d_arr\n",
    "    \n",
    "    @property\n",
    "    def atom_types(self):\n",
    "        return self._atom_types\n",
    "    \n",
    "    @property\n",
    "    def aa_onehot(self):\n",
    "        return self._aa_onehot\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return (f\"<ResidueEnvironment objects with {self.coords_2d_arr.shape[0]} \"\n",
    "                f\"atoms and residue class {np.argmax(self.aa_onehot)}>\")\n",
    "\n",
    "        \n",
    "class ResidueEnvironmentsDataset(Dataset):\n",
    "    def __init__(self, npz_filenames: List[str], transform=None):\n",
    "        self._res_env_objects = self._parse_envs(npz_filenames)\n",
    "        self._transform = transform\n",
    "        \n",
    "    @property\n",
    "    def res_env_objects(self):\n",
    "        return self._res_env_objects\n",
    "    \n",
    "    @property\n",
    "    def transform(self):\n",
    "        return self._transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.res_env_objects)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.res_env_objects[idx]        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "    \n",
    "    def _parse_envs(self, npz_filenames):\n",
    "        res_env_objects = []\n",
    "        for i in range(len(npz_filenames)):\n",
    "            coordinate_features = np.load(npz_filenames[i])\n",
    "            atom_coords_prot_seq = coordinate_features[\"positions\"]\n",
    "            restype_onehots_prot_seq = coordinate_features[\"aa_onehot\"]\n",
    "            selector_prot_seq = coordinate_features[\"selector\"]\n",
    "            atom_types_flattened = coordinate_features[\"atom_types_numeric\"]\n",
    "            N_residues = selector_prot_seq.shape[0]\n",
    "            for resi_i in range(N_residues):\n",
    "                selector = selector_prot_seq[resi_i]\n",
    "                selector_masked = selector[selector>-1] # Remove Filler\n",
    "                coords_mask = atom_coords_prot_seq[resi_i, :, 0] != -99.0 # To remove filler\n",
    "                coords = atom_coords_prot_seq[resi_i][coords_mask]            \n",
    "                atom_types = atom_types_flattened[selector_masked]\n",
    "                restype_onehot = restype_onehots_prot_seq[resi_i]\n",
    "                res_env_objects.append(ResidueEnvironment(coords, atom_types, restype_onehot))\n",
    "        return res_env_objects\n",
    "\n",
    "\n",
    "class ToTensor:\n",
    "    def __call__(self, sample):        \n",
    "        sample_env = np.hstack([np.reshape(sample.atom_types, [-1, 1]),\n",
    "                               sample.coords_2d_arr])\n",
    "        \n",
    "        return {\"x_\": torch.tensor(sample_env, dtype=torch.float32).to(DEVICE), \n",
    "                \"y_\": torch.tensor(np.array(sample.aa_onehot), dtype=torch.float32).to(DEVICE)}\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_cat(batch):\n",
    "        target = torch.cat([torch.unsqueeze(b['y_'], 0) for b in batch], dim=0)\n",
    "            \n",
    "        # To collate the input, we need to add a column which \n",
    "        # specifies the environtment each atom belongs to\n",
    "        env_id_batch = []\n",
    "        for i, b in enumerate(batch):\n",
    "            n_atoms = b['x_'].shape[0]\n",
    "            env_id_arr = torch.zeros(n_atoms, dtype=torch.float32).to(DEVICE) + i\n",
    "            env_id_batch.append(torch.cat([torch.unsqueeze(env_id_arr, 1), b['x_']], dim=1))            \n",
    "        data = torch.cat(env_id_batch, dim=0)\n",
    "            \n",
    "        return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CavityModel(torch.nn.Module):\n",
    "    def __init__(self, device: str, sigma: float = 0.6):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.n_atom_types = 6\n",
    "        self.p = 1.0 # Bins pr. Anstrom\n",
    "        self.n = 18  # Grid dimension\n",
    "        self.sigma = sigma # width of gaussian\n",
    "        self.sigma_p = self.sigma*self.p\n",
    "        self.a = np.linspace(start=-self.n/2*self.p + self.p/2, \n",
    "                             stop=self.n/2*self.p - self.p/2, \n",
    "                             num=self.n) \n",
    "        self.xx, self.yy, self.zz = torch.tensor(np.meshgrid(self.a, self.a, self.a, indexing=\"ij\"),\n",
    "                                                 dtype = torch.float32).to(self.device)\n",
    "\n",
    "        self.conv1 = torch.nn.Sequential(torch.nn.Conv3d(6, 16, kernel_size=(3,3,3), stride=2, padding=1),\n",
    "                                         torch.nn.ReLU(), \n",
    "                                         torch.nn.BatchNorm3d(16))\n",
    "        self.conv2 = torch.nn.Sequential(torch.nn.Conv3d(16, 32, kernel_size=(3,3,3), stride=2, padding=0),\n",
    "                                         torch.nn.ReLU(), \n",
    "                                         torch.nn.BatchNorm3d(32))\n",
    "        self.conv3 = torch.nn.Sequential(torch.nn.Conv3d(32, 64, kernel_size=(3,3,3), stride=1, padding=1),\n",
    "                                         torch.nn.ReLU(), \n",
    "                                         torch.nn.BatchNorm3d(64),\n",
    "                                         torch.nn.Flatten())\n",
    "        self.dense1 = torch.nn.Sequential(torch.nn.Linear(in_features=4096, out_features=128),\n",
    "                                          torch.nn.ReLU(), \n",
    "                                          torch.nn.BatchNorm1d(128))\n",
    "        self.dense2 = torch.nn.Linear(in_features=128, out_features=21)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self._gaussian_blurring(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x \n",
    "\n",
    "    def _gaussian_blurring(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        current_batch_size = torch.unique(x[:, 0]).shape[0]\n",
    "        fields_torch = torch.zeros((current_batch_size, self.n_atom_types, self.n, self.n, self.n)).to(DEVICE)\n",
    "        for j in range(self.n_atom_types):\n",
    "            mask_j = x[:,1]==j\n",
    "            atom_type_j_data = x[mask_j]\n",
    "            if atom_type_j_data.shape[0] > 0:\n",
    "                pos = atom_type_j_data[:, 2:]\n",
    "                density = torch.exp(-((torch.reshape(self.xx, [-1, 1]) - pos[:,0])**2 +\\\n",
    "                                      (torch.reshape(self.yy, [-1, 1]) - pos[:,1])**2 +\\\n",
    "                                      (torch.reshape(self.zz, [-1, 1]) - pos[:,2])**2) / (2 * self.sigma_p**2))\n",
    "                # Normalize each atom to 1\n",
    "                density /= torch.sum(density, dim=0)\n",
    "                # Since column 0 of atom_type_j_data is sorted\n",
    "                # I can use a trick to detect the boundaries based\n",
    "                # on the change from one value to another.\n",
    "                change_mask_j = (atom_type_j_data[:,0][:-1] != atom_type_j_data[:,0][1:]) # detect change in column 0\n",
    "                # Add begin- and end indices\n",
    "                ranges_i = torch.cat([torch.tensor([0]),\n",
    "                                      torch.arange(atom_type_j_data.shape[0]-1)[change_mask_j]+1, \n",
    "                                      torch.tensor([atom_type_j_data.shape[0]]) ])\n",
    "                for i in range(ranges_i.shape[0]):\n",
    "                    if i < ranges_i.shape[0] - 1:\n",
    "                        index_0, index_1 = ranges_i[i], ranges_i[i+1]\n",
    "                        fields = torch.reshape(torch.sum(density[:,index_0:index_1], dim = 1), \n",
    "                                               [self.n, self.n, self.n])\n",
    "                        fields_torch[i,j,:,:,:] = fields\n",
    "        return fields_torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse and train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data set includes 40 pdbs with 17753 environments.\n",
      "Validation data setincludes 10 pdbs with 3621 environments.\n"
     ]
    }
   ],
   "source": [
    "parsed_pdb_filenames = sorted(glob.glob(\"data/parsed/*coord*\"))\n",
    "random.shuffle(parsed_pdb_filenames)\n",
    "\n",
    "n_train_pdbs = int(len(parsed_pdb_filenames)*TRAIN_VAL_SPLIT)\n",
    "filenames_train = parsed_pdb_filenames[:n_train_pdbs]\n",
    "filenames_val = parsed_pdb_filenames[n_train_pdbs:]\n",
    "\n",
    "data_set_train = ResidueEnvironmentsDataset(filenames_train, transform=ToTensor())\n",
    "data_set_val = ResidueEnvironmentsDataset(filenames_val, transform=ToTensor())\n",
    "\n",
    "dataloader_train = DataLoader(data_set_train, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                              collate_fn=ToTensor.collate_cat, drop_last=True)\n",
    "dataloader_val = DataLoader(data_set_val, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                            collate_fn=ToTensor.collate_cat, drop_last=True)\n",
    "\n",
    "print(f\"Training data set includes {len(filenames_train)} pdbs with {len(data_set_train)} environments.\")\n",
    "print(f\"Validation data setincludes {len(filenames_val)} pdbs with {len(data_set_val)} environments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate model, loss and optimizer\n",
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Train loss: 1.799. Train Acc: 0.37. Val Acc: 0.31\n",
      "Epoch 2. Train loss: 1.175. Train Acc: 0.69. Val Acc: 0.31\n",
      "Epoch 3. Train loss: 0.726. Train Acc: 0.85. Val Acc: 0.30\n",
      "Epoch 4. Train loss: 0.384. Train Acc: 0.95. Val Acc: 0.28\n",
      "Epoch 5. Train loss: 0.141. Train Acc: 0.99. Val Acc: 0.28\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "cavity_model = CavityModel(DEVICE).to(DEVICE)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cavity_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train loop\n",
    "    loss_running_mean = 0.0\n",
    "    labels_true = []\n",
    "    labels_pred = []    \n",
    "    for batch_x, batch_y in dataloader_train:\n",
    "        cavity_model.train()\n",
    "        optimizer.zero_grad()    \n",
    "        batch_y_pred = cavity_model(batch_x)\n",
    "        loss_batch = loss(batch_y_pred, torch.argmax(batch_y, dim=-1))\n",
    "        loss_batch.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Exponential running mean for the loss\n",
    "        loss_running_mean = loss_running_mean*0.9 + loss_batch*0.1\n",
    "        \n",
    "        labels_true.append(torch.argmax(batch_y, dim=-1).detach().cpu().numpy())\n",
    "        labels_pred.append(torch.argmax(batch_y_pred, dim=-1).detach().cpu().numpy())\n",
    "    acc_train = np.mean((np.reshape(labels_true, -1) == np.reshape(labels_pred, -1)))\n",
    "    \n",
    "    # Eval loop. Due to memory, we don't want to pass the whole eval data set in one go\n",
    "    labels_true_val = []\n",
    "    labels_pred_val = []\n",
    "    for batch_x_val, batch_y_val in dataloader_val:\n",
    "        cavity_model.eval()\n",
    "        batch_y_pred_val = cavity_model(batch_x_val)\n",
    "        labels_true_val.append(torch.argmax(batch_y_val, dim=-1).detach().cpu().numpy())\n",
    "        labels_pred_val.append(torch.argmax(batch_y_pred_val, dim=-1).detach().cpu().numpy())\n",
    "    acc_val = np.mean((np.reshape(labels_true_val, -1) == np.reshape(labels_pred_val, -1)))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}. Train loss: {loss_running_mean:5.3f}. \"\n",
    "          f\"Train Acc: {acc_train:4.2f}. Val Acc: {acc_val:4.2f}\")   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
