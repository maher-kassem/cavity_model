{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded 4X2U.pdb to data/raw/4X2U.pdb. 1/10.\n",
      "Successfully downloaded 2X96.pdb to data/raw/2X96.pdb. 2/10.\n",
      "Successfully downloaded 4MXD.pdb to data/raw/4MXD.pdb. 3/10.\n",
      "Successfully downloaded 3E9L.pdb to data/raw/3E9L.pdb. 4/10.\n",
      "Successfully downloaded 1UWC.pdb to data/raw/1UWC.pdb. 5/10.\n",
      "Successfully downloaded 4BGU.pdb to data/raw/4BGU.pdb. 6/10.\n",
      "Successfully downloaded 2YSW.pdb to data/raw/2YSW.pdb. 7/10.\n",
      "Successfully downloaded 4OW4.pdb to data/raw/4OW4.pdb. 8/10.\n",
      "Successfully downloaded 2V5E.pdb to data/raw/2V5E.pdb. 9/10.\n",
      "Successfully downloaded 1IXH.pdb to data/raw/1IXH.pdb. 10/10.\n",
      "Successfully cleaned data/raw/1IXH.pdb to data/cleaned/data/raw/1IXH.pdb.pdb. 1/10.\n",
      "Successfully cleaned data/raw/1UWC.pdb to data/cleaned/data/raw/1UWC.pdb.pdb. 2/10.\n",
      "Successfully cleaned data/raw/2V5E.pdb to data/cleaned/data/raw/2V5E.pdb.pdb. 3/10.\n",
      "Successfully cleaned data/raw/2X96.pdb to data/cleaned/data/raw/2X96.pdb.pdb. 4/10.\n",
      "Successfully cleaned data/raw/2YSW.pdb to data/cleaned/data/raw/2YSW.pdb.pdb. 5/10.\n",
      "Successfully cleaned data/raw/3E9L.pdb to data/cleaned/data/raw/3E9L.pdb.pdb. 6/10.\n",
      "Successfully cleaned data/raw/4BGU.pdb to data/cleaned/data/raw/4BGU.pdb.pdb. 7/10.\n",
      "Successfully cleaned data/raw/4MXD.pdb to data/cleaned/data/raw/4MXD.pdb.pdb. 8/10.\n",
      "Successfully cleaned data/raw/4OW4.pdb to data/cleaned/data/raw/4OW4.pdb.pdb. 9/10.\n",
      "Successfully cleaned data/raw/4X2U.pdb to data/cleaned/data/raw/4X2U.pdb.pdb. 10/10.\n",
      "Successfully extracted environments from 1IXH_clean.pdb. Finished 1/10.\n",
      "Successfully extracted environments from 1UWC_clean.pdb. Finished 2/10.\n",
      "Successfully extracted environments from 2V5E_clean.pdb. Finished 3/10.\n",
      "Successfully extracted environments from 2X96_clean.pdb. Finished 4/10.\n",
      "Successfully extracted environments from 2YSW_clean.pdb. Finished 5/10.\n",
      "Successfully extracted environments from 3E9L_clean.pdb. Finished 6/10.\n",
      "Successfully extracted environments from 4BGU_clean.pdb. Finished 7/10.\n",
      "Successfully extracted environments from 4MXD_clean.pdb. Finished 8/10.\n",
      "Successfully extracted environments from 4OW4_clean.pdb. Finished 9/10.\n",
      "Successfully extracted environments from 4X2U_clean.pdb. Finished 10/10.\n"
     ]
    }
   ],
   "source": [
    "# Run shell script that takes a .txt file with PDBIDs as input.\n",
    "!./download_and_process_data.sh pdbids_010.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\" # \"cpu\" or \"cuda\"\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.0003\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidueEnvironment():\n",
    "    \"\"\"\n",
    "    Residue environment class\n",
    "    \"\"\"\n",
    "    def __init__(self, coords_2d_arr: np.ndarray, \n",
    "                 atom_types: np.ndarray, aa_onehot: np.ndarray):\n",
    "        self.coords_2d_arr = coords_2d_arr\n",
    "        self.atom_types = atom_types\n",
    "        self.aa_onehot = aa_onehot\n",
    "\n",
    "class ResidueEnvironmentsDataset(Dataset):\n",
    "    def __init__(npz_filenames: List[str], transform=None):\n",
    "        \"\"\"\n",
    "        Load parsed pdb data in .npz format\n",
    "        \"\"\"\n",
    "        self.res_env_objects = []\n",
    "        for i in range(len(npz_filenames)):\n",
    "            coordinate_features = np.load(coordinate_features_filenames[i])\n",
    "            atom_coords_prot_seq = coordinate_features[\"positions\"]\n",
    "            restype_onehots_prot_seq = coordinate_features[\"aa_onehot\"]\n",
    "            selector_prot_seq = coordinate_features[\"selector\"]\n",
    "            atom_types_flattened = coordinate_features[\"atom_types_numeric\"]\n",
    "            N_residues = selector_prot_seq.shape[0]\n",
    "            for resi_i in range(N_residues):\n",
    "                selector = selector_prot_seq[resi_i]\n",
    "                selector_masked = selector[selector>-1] # Remove Filler\n",
    "                coords_mask = atom_coords_prot_seq[resi_i, :, 0] != -99.0 # To remove filler\n",
    "                coords = atom_coords_prot_seq[resi_i][coords_mask]            \n",
    "                atom_types = atom_types_flattened[selector_masked]\n",
    "                restype_onehot = restype_onehots_prot_seq[resi_i]\n",
    "                res_env_objects.append(ResidueEnvironment(coords, atom_types, restype_onehot))\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.res_env_objects)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            if torch.is_tensor(idx):\n",
    "                idx = idx.tolist()\n",
    "\n",
    "            img_name = os.path.join(self.root_dir,\n",
    "                                    self.landmarks_frame.iloc[idx, 0])\n",
    "            image = io.imread(img_name)\n",
    "            landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
    "            landmarks = np.array([landmarks])\n",
    "            landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "            sample = {'image': image, 'landmarks': landmarks}\n",
    "\n",
    "            if self.transform:\n",
    "                sample = self.transform(sample)\n",
    "\n",
    "            return sample\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = [1,2,3,4,5]\n",
    "print(u[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidueEnvironmentDensity():\n",
    "    \"\"\"\n",
    "    Environment class, which contains all the relevant information on the structural environment\n",
    "    \"\"\"\n",
    "    def __init__(self, coords_2d_arr: np.ndarray, atom_types, aa_onehot):\n",
    "        self.coords_2d_arr = coords_2d_arr\n",
    "        self.atom_types = atom_types\n",
    "        self.aa_onehot = aa_onehot\n",
    "        \n",
    "    def get_coords_2d_arr(self):\n",
    "        return self.coords_2d_arr\n",
    "    \n",
    "    def get_aa_onehot(self):\n",
    "        return self.aa_onehot\n",
    "    \n",
    "    def get_atom_types(self):\n",
    "        return self.atom_types\n",
    "\n",
    "def parse_data(coordinate_features_filenames):\n",
    "    \"\"\"\n",
    "    Function that parses environment files (\"coordinate_features.npz\") and \n",
    "    returns ResidueEnvironmentDensity objects\n",
    "    \"\"\"\n",
    "    env_objects = []\n",
    "    for i in range(len(coordinate_features_filenames)):\n",
    "        coordinate_features = np.load(coordinate_features_filenames[i])\n",
    "\n",
    "        atom_coords_prot_seq = coordinate_features[\"positions\"]\n",
    "        restype_onehots_prot_seq = coordinate_features[\"aa_onehot\"]\n",
    "        selector_prot_seq = coordinate_features[\"selector\"]\n",
    "        atom_types_flattened = coordinate_features[\"atom_types_numeric\"]\n",
    "        # Loop over protein sequence\n",
    "        N_residues = selector_prot_seq.shape[0]\n",
    "        for resi_i in range(N_residues):\n",
    "            # Loop over surrounding atoms in the environment from the perspective of the residues\n",
    "            selector = selector_prot_seq[resi_i]\n",
    "            selector_masked = selector[ selector > -1 ] # Remove Filler\n",
    "            coords_mask = atom_coords_prot_seq[resi_i, :, 0] != -99.0 # To remove filler\n",
    "\n",
    "            # Relevant data\n",
    "            coords = atom_coords_prot_seq[resi_i][ coords_mask ]            \n",
    "            atom_types = atom_types_flattened[selector_masked]\n",
    "            restype_onehot = restype_onehots_prot_seq[resi_i]\n",
    "            env_objects.append(ResidueEnvironmentDensity(coords, atom_types, restype_onehot))            \n",
    "    return env_objects\n",
    "\n",
    "def get_batch_x_and_batch_y(res_env_obj_list):\n",
    "    \"\"\"\n",
    "    Function that takes a list of ResidueEnvironmentDensity objects and returns\n",
    "    batches with dimensions that match the requirements of the CNN for x and y\n",
    "    \"\"\"\n",
    "    batch_aa_onehots = []\n",
    "    batch_coords = []\n",
    "    batch_atom_types = []\n",
    "    for i, res_env_obj in enumerate(res_env_obj_list):\n",
    "        batch_aa_onehots.append(res_env_obj.get_aa_onehot())\n",
    "        batch_coords.append(res_env_obj.get_coords_2d_arr())\n",
    "        batch_atom_types.append(res_env_obj.get_atom_types())\n",
    "    \n",
    "    env_data_all = []\n",
    "    for env_i in range(len(batch_coords)):\n",
    "        n_atoms = np.array(batch_coords[env_i]).shape[0]\n",
    "        env_i_vector = np.zeros(n_atoms)+i \n",
    "        atom_types = batch_atom_types[env_i]\n",
    "        coords = batch_coords[env_i]\n",
    "        env_data = np.hstack( [np.reshape(np.zeros(n_atoms)+env_i, [-1, 1]), np.reshape(atom_types, [-1, 1]), coords] )\n",
    "        env_data_all.append(env_data)\n",
    "        \n",
    "    env_data_all_stacked = np.vstack(env_data_all)\n",
    "    return env_data_all_stacked, np.array(batch_aa_onehots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CavityModel(torch.nn.Module):\n",
    "    def __init__(self, device: str, sigma: float = 0.6):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.n_atom_types = 6\n",
    "        self.p = 1.0 # Bins pr. Anstrom\n",
    "        self.n = 18  # Grid dimension\n",
    "        self.sigma = sigma # width of gaussian\n",
    "        self.sigma_p = self.sigma*self.p\n",
    "        self.a = np.linspace(start=-self.n/2*self.p + self.p/2, \n",
    "                             stop=self.n/2*self.p - self.p/2, \n",
    "                             num=self.n) \n",
    "        self.xx, self.yy, self.zz = torch.tensor(np.meshgrid(self.a, self.a, self.a, indexing=\"ij\"),\n",
    "                                                 dtype = torch.float32).to(self.device)\n",
    "\n",
    "        self.conv1 = torch.nn.Sequential(torch.nn.Conv3d(6, 16, kernel_size=(3,3,3), stride=2, padding=1),\n",
    "                                         torch.nn.ReLU(), \n",
    "                                         torch.nn.BatchNorm3d(16))\n",
    "        self.conv2 = torch.nn.Sequential(torch.nn.Conv3d(16, 32, kernel_size=(3,3,3), stride=2, padding=0),\n",
    "                                         torch.nn.ReLU(), \n",
    "                                         torch.nn.BatchNorm3d(32))\n",
    "        self.conv3 = torch.nn.Sequential(torch.nn.Conv3d(32, 64, kernel_size=(3,3,3), stride=1, padding=1),\n",
    "                                         torch.nn.ReLU(), \n",
    "                                         torch.nn.BatchNorm3d(64),\n",
    "                                         torch.nn.Flatten())\n",
    "        self.dense1 = torch.nn.Sequential(torch.nn.Linear(in_features=4096, out_features=128),\n",
    "                                          torch.nn.ReLU(), \n",
    "                                          torch.nn.BatchNorm1d(128))\n",
    "        self.dense2 = torch.nn.Linear(in_features=128, out_features=21)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self._gaussian_blurring(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x \n",
    "\n",
    "    def _gaussian_blurring(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        current_batch_size = torch.unique(x[:, 0]).shape[0]       \n",
    "        fields_torch = torch.zeros((current_batch_size, self.n_atom_types, self.n, self.n, self.n)).to(DEVICE)\n",
    "        for j in range(self.n_atom_types):\n",
    "            mask_j = x[:,1]==j\n",
    "            atom_type_j_data = x[mask_j]\n",
    "            if atom_type_j_data.shape[0] > 0:\n",
    "                pos = atom_type_j_data[:, 2:]\n",
    "                density = torch.exp(-((torch.reshape(self.xx, [-1, 1]) - pos[:,0])**2 +\\\n",
    "                                      (torch.reshape(self.yy, [-1, 1]) - pos[:,1])**2 +\\\n",
    "                                      (torch.reshape(self.zz, [-1, 1]) - pos[:,2])**2) / (2 * self.sigma_p**2))\n",
    "                # Normalize each atom to 1\n",
    "                density /= torch.sum(density, dim=0)\n",
    "                # Since column 0 of atom_type_j_data is sorted\n",
    "                # I can use a trick to detect the boundaries based\n",
    "                # on the change from one value to another.\n",
    "                change_mask_j = (atom_type_j_data[:,0][:-1] != atom_type_j_data[:,0][1:]) # detect change in column 0\n",
    "                # Add begin- and end indices\n",
    "                ranges_i = torch.cat([torch.tensor([0]),\n",
    "                                      torch.arange(atom_type_j_data.shape[0]-1)[change_mask_j]+1, \n",
    "                                      torch.tensor([atom_type_j_data.shape[0]]) ])\n",
    "                for i in range(ranges_i.shape[0]):\n",
    "                    if i < ranges_i.shape[0] - 1:\n",
    "                        index_0, index_1 = ranges_i[i], ranges_i[i+1]\n",
    "                        fields = torch.reshape(torch.sum(density[:,index_0:index_1], dim = 1), \n",
    "                                               [self.n, self.n, self.n])\n",
    "                        fields_torch[i,j,:,:,:] = fields\n",
    "        return fields_torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training environments: 3389\n",
      "Number of testing environments:  1869\n"
     ]
    }
   ],
   "source": [
    "# Parse environment files (one per protein)\n",
    "env_filenames_wild_card = \"data/parsed/*coord*\"\n",
    "env_filenames = sorted(glob.glob(env_filenames_wild_card))\n",
    "random.shuffle(env_filenames) # sort for good measure\n",
    "env_filenames_train = env_filenames[:8] # proteins for training\n",
    "env_filenames_test = env_filenames[8:] #  proteins for testing\n",
    "\n",
    "env_objects_train = parse_data(env_filenames_train)\n",
    "env_objects_test = parse_data(env_filenames_test)\n",
    "print (\"Number of training environments:\", len(env_objects_train))\n",
    "print (\"Number of testing environments: \", len(env_objects_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/1, step      0, loss(train):  3.05, loss(test):  3.05, accuracy(train):  0.05, accuracy(test):  0.08\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-189cbacca0a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0maa_pred_conv3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mbatch_y_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maa_pred_conv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mloss_batch_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/miniconda3/envs/cavity-model/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-19f0c046fa5a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gaussian_blurring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-19f0c046fa5a>\u001b[0m in \u001b[0;36m_gaussian_blurring\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 density = torch.exp(-((torch.reshape(self.xx, [-1, 1]) - pos[:,0])**2 +\\\n\u001b[1;32m     49\u001b[0m                                       \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                                       (torch.reshape(self.zz, [-1, 1]) - pos[:,2])**2) / (2 * self.sigma_p**2))\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0;31m# Normalize each atom to 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdensity\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdensity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "aa_pred_conv3d = CavityModel(DEVICE, SIGMA).to(DEVICE)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(aa_pred_conv3d.parameters(), lr = LEARNING_RATE)\n",
    "random.shuffle(env_objects_train)\n",
    "\n",
    "# Define random subsets of training and testing sets for evaluation.\n",
    "# This makes it simpler to fit in a single forward pass (memory-wise), \n",
    "# instead of iterating over all the data\n",
    "sample_size = 500\n",
    "train_eval_inds = np.random.choice(np.arange(len(env_objects_train)), size = sample_size, replace = False)\n",
    "test_eval_inds = np.random.choice(np.arange(len(env_objects_test)), size = sample_size, replace = False)\n",
    "env_objects_train_eval = [env_objects_train[ind] for ind in train_eval_inds]\n",
    "env_objects_test_eval = [env_objects_test[ind] for ind in test_eval_inds]\n",
    "x_train_torch, y_train_torch = [torch.tensor(tens, dtype = torch.float32, requires_grad = False).to(DEVICE) for tens in get_batch_x_and_batch_y(env_objects_train_eval)]\n",
    "x_test_torch, y_test_torch = [torch.tensor(tens, dtype = torch.float32, requires_grad = False).to(DEVICE) for tens in get_batch_x_and_batch_y(env_objects_test_eval)]\n",
    "\n",
    "# Train\n",
    "random.shuffle(env_objects_train)\n",
    "loss_train_list = []\n",
    "loss_test_list = []\n",
    "acc_train_list = []\n",
    "acc_test_list = []\n",
    "for epoch_i in range(EPOCHS):\n",
    "    for i in range(0, len(env_objects_train), BATCH_SIZE):\n",
    "        if i+BATCH_SIZE > len(env_objects_train): # skip last batch if it is smaller than batch size (small batch might mess up batch norm)\n",
    "            continue\n",
    "        # Define training batch\n",
    "        env_objs_batch_train = env_objects_train[i:i+BATCH_SIZE]\n",
    "        batch_x_train, batch_y_train = [torch.tensor(tens, dtype = torch.float32).to(DEVICE) for tens in get_batch_x_and_batch_y(env_objs_batch_train)]\n",
    "\n",
    "        # Set the parameter gradients to zero\n",
    "        # Forward pass, backward pass, optimize\n",
    "        aa_pred_conv3d.train()\n",
    "        optimizer.zero_grad()    \n",
    "        batch_y_pred = aa_pred_conv3d(batch_x_train)\n",
    "        labels = torch.argmax(batch_y_train, dim = -1)\n",
    "        loss_batch_train = loss(batch_y_pred, labels)\n",
    "        loss_batch_train.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Evaluate on the big subset of training and testing environments\n",
    "        if i % 1000 == 0:\n",
    "            aa_pred_conv3d.eval()\n",
    "            \n",
    "            # Training eval\n",
    "            y_pred_train = aa_pred_conv3d(x_train_torch)\n",
    "            labels_train_pred = torch.argmax(y_pred_train, dim = -1)\n",
    "            labels_train = torch.argmax(y_train_torch, dim = -1)\n",
    "            loss_train = loss(y_pred_train, labels_train).item()\n",
    "            loss_train_list.append(loss_train)\n",
    "            accuracy_train = torch.mean((labels_train == labels_train_pred).double()).item()\n",
    "            acc_train_list.append(accuracy_train)\n",
    "            \n",
    "            # Testing eval\n",
    "            y_pred_test = aa_pred_conv3d(x_test_torch)\n",
    "            labels_test_pred = torch.argmax(y_pred_test, dim = -1)\n",
    "            labels_test = torch.argmax(y_test_torch, dim = -1)\n",
    "            loss_test = loss(y_pred_test, labels_test).item()\n",
    "            loss_test_list.append(loss_test)\n",
    "            accuracy_test = torch.mean((labels_test == labels_test_pred).double()).item()\n",
    "            acc_test_list.append(accuracy_test)\n",
    "            \n",
    "            print (\"epoch {:1d}/{:1d}, step {:6d}, loss(train): {:5.2f}, loss(test): {:5.2f}, accuracy(train): {:5.2f}, accuracy(test): {:5.2f}\".format(epoch_i+1, EPOCHS, i, loss_train, loss_test, accuracy_train, accuracy_test))\n",
    "            \n",
    "    # shuffle after each epoch\n",
    "    random.shuffle(env_objects_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_arr = plt.subplots(2, sharex=True)\n",
    "ax_arr[0].plot(np.arange(len(loss_train_list)), loss_train_list, label = \"train\")\n",
    "ax_arr[0].plot(np.arange(len(loss_test_list)), loss_test_list, label = \"test\")\n",
    "ax_arr[1].plot(np.arange(len(acc_train_list)), acc_train_list, label = \"train\")\n",
    "ax_arr[1].plot(np.arange(len(acc_test_list)), acc_test_list, label = \"test\")\n",
    "ax_arr[0].set_ylabel(\"Cross Entropy loss\")\n",
    "ax_arr[1].set_ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "This demo demonstrates how to classify the missing amino acid from a structural environment where the atoms if the query amino acid have been purposefully removed.\n",
    "\n",
    "I train a simple Conv3D net to learn the shape/contact-pattern that the missing atoms have left behind.\n",
    "\n",
    "Given the very limited number of PDB structures (250) used in this demo, it is not surprising that we see much better performance on the training data compared to the testing data. Should you want to use the demo, please include more PDB structures. \n",
    "For reference, I use around 2k structures that have been homology reduces in sequence with much success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
