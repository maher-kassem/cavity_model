{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"from torch.utils.data import Dataset, DataLoader\\nfrom typing import List, Callable, Union\\nimport numpy as np\\nimport torch\\nimport glob\\nimport random\\nimport os\\n\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"from torch.utils.data import Dataset, DataLoader\\nfrom typing import List, Callable, Union\\nimport numpy as np\\nimport torch\\nimport glob\\nimport random\\nimport os\\n\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Callable, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded 4X2U.pdb to data/pdbs/raw/4X2U.pdb. 1/10.\n",
      "Successfully downloaded 2X96.pdb to data/pdbs/raw/2X96.pdb. 2/10.\n",
      "Successfully downloaded 4MXD.pdb to data/pdbs/raw/4MXD.pdb. 3/10.\n",
      "Successfully downloaded 3E9L.pdb to data/pdbs/raw/3E9L.pdb. 4/10.\n",
      "Successfully downloaded 1UWC.pdb to data/pdbs/raw/1UWC.pdb. 5/10.\n",
      "Successfully downloaded 4BGU.pdb to data/pdbs/raw/4BGU.pdb. 6/10.\n",
      "Successfully downloaded 2YSW.pdb to data/pdbs/raw/2YSW.pdb. 7/10.\n",
      "Successfully downloaded 4OW4.pdb to data/pdbs/raw/4OW4.pdb. 8/10.\n",
      "Successfully downloaded 2V5E.pdb to data/pdbs/raw/2V5E.pdb. 9/10.\n",
      "Successfully downloaded 1IXH.pdb to data/pdbs/raw/1IXH.pdb. 10/10.\n",
      "Successfully cleaned data/pdbs/raw/1IXH.pdb and added it to data/pdbs/cleaned/. 1/10.\n",
      "Successfully cleaned data/pdbs/raw/1UWC.pdb and added it to data/pdbs/cleaned/. 2/10.\n",
      "Successfully cleaned data/pdbs/raw/2V5E.pdb and added it to data/pdbs/cleaned/. 3/10.\n",
      "Successfully cleaned data/pdbs/raw/2X96.pdb and added it to data/pdbs/cleaned/. 4/10.\n",
      "Successfully cleaned data/pdbs/raw/2YSW.pdb and added it to data/pdbs/cleaned/. 5/10.\n",
      "Successfully cleaned data/pdbs/raw/3E9L.pdb and added it to data/pdbs/cleaned/. 6/10.\n",
      "Successfully cleaned data/pdbs/raw/4BGU.pdb and added it to data/pdbs/cleaned/. 7/10.\n",
      "Successfully cleaned data/pdbs/raw/4MXD.pdb and added it to data/pdbs/cleaned/. 8/10.\n",
      "Successfully cleaned data/pdbs/raw/4OW4.pdb and added it to data/pdbs/cleaned/. 9/10.\n",
      "Successfully cleaned data/pdbs/raw/4X2U.pdb and added it to data/pdbs/cleaned/. 10/10.\n",
      "Successfully parsed 1IXH_clean.pdb and moved parsed file to data/pdbs/parsed. Finished 1/10.\n",
      "Successfully parsed 1UWC_clean.pdb and moved parsed file to data/pdbs/parsed. Finished 2/10.\n",
      "Successfully parsed 2V5E_clean.pdb and moved parsed file to data/pdbs/parsed. Finished 3/10.\n",
      "Successfully parsed 2X96_clean.pdb and moved parsed file to data/pdbs/parsed. Finished 4/10.\n",
      "Successfully parsed 2YSW_clean.pdb and moved parsed file to data/pdbs/parsed. Finished 5/10.\n",
      "Successfully parsed 3E9L_clean.pdb and moved parsed file to data/pdbs/parsed. Finished 6/10.\n",
      "Successfully parsed 4BGU_clean.pdb and moved parsed file to data/pdbs/parsed. Finished 7/10.\n",
      "Successfully parsed 4MXD_clean.pdb and moved parsed file to data/pdbs/parsed. Finished 8/10.\n",
      "Successfully parsed 4OW4_clean.pdb and moved parsed file to data/pdbs/parsed. Finished 9/10.\n",
      "Successfully parsed 4X2U_clean.pdb and moved parsed file to data/pdbs/parsed. Finished 10/10.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# Run shell script that takes a .txt file with PDBIDs as input.\\n!./download_and_process_data.sh data/pdbids_010.txt\";\n",
       "                var nbb_formatted_code = \"# Run shell script that takes a .txt file with PDBIDs as input.\\n!./download_and_process_data.sh data/pdbids_010.txt\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run shell script that takes a .txt file with PDBIDs as input.\n",
    "!./download_and_process_data.sh data/pdbids_010.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"DEVICE = \\\"cuda\\\"  # \\\"cpu\\\" or \\\"cuda\\\"\\nBATCH_SIZE = 100\\nLEARNING_RATE = 3e-4\\nEPOCHS = 5\\nTRAIN_VAL_SPLIT = 0.8\";\n",
       "                var nbb_formatted_code = \"DEVICE = \\\"cuda\\\"  # \\\"cpu\\\" or \\\"cuda\\\"\\nBATCH_SIZE = 100\\nLEARNING_RATE = 3e-4\\nEPOCHS = 5\\nTRAIN_VAL_SPLIT = 0.8\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DEVICE = \"cuda\"  # \"cpu\" or \"cuda\"\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 3e-4\n",
    "EPOCHS = 5\n",
    "TRAIN_VAL_SPLIT = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"class ResidueEnvironment:\\n    \\\"\\\"\\\"\\n    Residue environment class used to hold necessarry information about the\\n    atoms of the environment such as atomic coordinates, atom types and the\\n    class of the missing central amino acid.\\n\\n    Parameters\\n    ----------\\n    xyz_coords: np.ndarray\\n        Numpy array with shape (n_atoms, 3) containing the x, y, z coordinates.\\n    atom_types: np.ndarray\\n        1D numpy array containing the atom types. Integer values in range(6).\\n    restypes_onehot: np.ndarray\\n        Numpy array with shape (n_atoms, 21) containing the amino acid\\n        class of the missing amino acid\\n    chain_id: str\\n        Chain id associated to ResidueEnvironment object\\n    pdb_residue_number: int\\n        Residue number associated with the ResidueEnvironment object\\n    pdb_id: str\\n        PDBID associated with the ResidueEnvironment object\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        xyz_coords: np.ndarray,\\n        atom_types: np.ndarray,\\n        restypes_onehot: np.ndarray,\\n        chain_id: str,\\n        pdb_residue_number: int,\\n        pdb_id: str,\\n    ):\\n        self._xyz_coords = xyz_coords\\n        self._atom_types = atom_types\\n        self._restypes_onehot = restypes_onehot\\n        self._chain_id = chain_id\\n        self._pdb_residue_number = pdb_residue_number\\n        self._pdb_id = pdb_id\\n\\n    @property\\n    def xyz_coords(self):\\n        return self._xyz_coords\\n\\n    @property\\n    def atom_types(self):\\n        return self._atom_types\\n\\n    @property\\n    def restypes_onehot(self):\\n        return self._restypes_onehot\\n\\n    @property\\n    def chain_id(self):\\n        return self._chain_id\\n\\n    @property\\n    def pdb_residue_number(self):\\n        return self._pdb_residue_number\\n\\n    @property\\n    def pdb_id(self):\\n        return self._pdb_id\\n\\n    def __repr__(self):\\n        return (\\n            f\\\"<ResidueEnvironment with {self.xyz_coords.shape[0]} atoms. \\\\n\\\"\\n            f\\\"pdb_id: {self.pdb_id}, \\\"\\n            f\\\"chain_id: {self.chain_id}, \\\"\\n            f\\\"pdb_residue_number: {self.pdb_residue_number},\\\\n\\\"\\n            f\\\"restype_one: {np.argmax(self.restypes_onehot)}>\\\"\\n        )\\n\\n\\nclass ResidueEnvironmentsDataset(Dataset):\\n    \\\"\\\"\\\"\\n    Residue environment dataset class\\n\\n    Parameters\\n    ----------\\n    input_data: Union[List[str], List[ResidueEnvironment]]\\n        List of parsed pdb filenames in .npz format or list of\\n        ResidueEnvironment objects\\n    transform: Callable\\n        A to-tensor transformer class\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        input_data: Union[List[str], List[ResidueEnvironment]],\\n        transformer: Callable = None,\\n    ):\\n        if all(isinstance(x, ResidueEnvironment) for x in input_data):\\n            self._res_env_objects = input_data\\n        elif all(isinstance(x, str) for x in input_data):\\n            self._res_env_objects = self._parse_envs(input_data)\\n        else:\\n            raise ValueError(\\n                \\\"Input data is not of type\\\" \\\"Union[List[str], List[ResidueEnvironment]]\\\"\\n            )\\n\\n        self.transformer = transformer\\n\\n    @property\\n    def res_env_objects(self):\\n        return self._res_env_objects\\n\\n    @property\\n    def transformer(self):\\n        return self._transformer\\n\\n    @transformer.setter\\n    def transformer(self, transformer):\\n        \\\"\\\"\\\"TODO: Think if a constraint to add later\\\"\\\"\\\"\\n        self._transformer = transformer\\n\\n    def __len__(self):\\n        return len(self.res_env_objects)\\n\\n    def __getitem__(self, idx):\\n        sample = self.res_env_objects[idx]\\n        if self.transformer:\\n            sample = self.transformer(sample)\\n        return sample\\n\\n    def _parse_envs(self, npz_filenames: List[str]) -> List[ResidueEnvironment]:\\n        \\\"\\\"\\\"\\n        TODO: Make this more readable\\n        \\\"\\\"\\\"\\n\\n        res_env_objects = []\\n        for i in range(len(npz_filenames)):\\n            coordinate_features = np.load(npz_filenames[i])\\n            atom_coords_prot_seq = coordinate_features[\\\"positions\\\"]\\n            restypes_onehots_prot_seq = coordinate_features[\\\"aa_onehot\\\"]\\n            selector_prot_seq = coordinate_features[\\\"selector\\\"]\\n            atom_types_flattened = coordinate_features[\\\"atom_types_numeric\\\"]\\n\\n            chain_ids = coordinate_features[\\\"chain_ids\\\"]\\n            pdb_residue_numbers = coordinate_features[\\\"residue_numbers\\\"]\\n            chain_boundary_indices = coordinate_features[\\\"chain_boundary_indices\\\"]\\n\\n            pdb_id = os.path.basename(npz_filenames[i])[0:4]\\n\\n            N_residues = selector_prot_seq.shape[0]\\n            for resi_i in range(N_residues):\\n                selector = selector_prot_seq[resi_i]\\n                selector_masked = selector[selector > -1]  # Remove Filler\\n                coords_mask = (\\n                    atom_coords_prot_seq[resi_i, :, 0] != -99.0\\n                )  # Remove filler\\n                coords = atom_coords_prot_seq[resi_i][coords_mask]\\n                atom_types = atom_types_flattened[selector_masked]\\n                restypes_onehot = restypes_onehots_prot_seq[resi_i]\\n\\n                pdb_residue_number = int(pdb_residue_numbers[resi_i])\\n                # Locate chain id\\n                for j in range(len(chain_ids)):\\n                    chain_boundary_0 = chain_boundary_indices[j]\\n                    chain_boundary_1 = chain_boundary_indices[j + 1]\\n                    if resi_i in range(chain_boundary_0, chain_boundary_1):\\n                        chain_id = str(chain_ids[j])\\n                        break\\n\\n                res_env_objects.append(\\n                    ResidueEnvironment(\\n                        coords,\\n                        atom_types,\\n                        restypes_onehot,\\n                        chain_id,\\n                        pdb_residue_number,\\n                        pdb_id,\\n                    )\\n                )\\n\\n        return res_env_objects\\n\\n\\nclass ToTensor:\\n    \\\"\\\"\\\" To-tensor transformer class\\\"\\\"\\\"\\n\\n    def __call__(self, sample: ResidueEnvironment):\\n        \\\"\\\"\\\"Converts single ResidueEnvironment object into x_ and y_\\\"\\\"\\\"\\n\\n        sample_env = np.hstack(\\n            [np.reshape(sample.atom_types, [-1, 1]), sample.xyz_coords]\\n        )\\n\\n        return {\\n            \\\"x_\\\": torch.tensor(sample_env, dtype=torch.float32).to(DEVICE),\\n            \\\"y_\\\": torch.tensor(\\n                np.array(sample.restypes_onehot), dtype=torch.float32\\n            ).to(DEVICE),\\n        }\\n\\n    @staticmethod\\n    def collate_cat(batch: List[ResidueEnvironment]):\\n        \\\"\\\"\\\"\\n        Collate method used by the dataloader to collate a\\n        batch of ResidueEnvironment objects.\\n        \\\"\\\"\\\"\\n        target = torch.cat([torch.unsqueeze(b[\\\"y_\\\"], 0) for b in batch], dim=0)\\n\\n        # To collate the input, we need to add a column which\\n        # specifies the environtment each atom belongs to\\n        env_id_batch = []\\n        for i, b in enumerate(batch):\\n            n_atoms = b[\\\"x_\\\"].shape[0]\\n            env_id_arr = torch.zeros(n_atoms, dtype=torch.float32).to(DEVICE) + i\\n            env_id_batch.append(\\n                torch.cat([torch.unsqueeze(env_id_arr, 1), b[\\\"x_\\\"]], dim=1)\\n            )\\n        data = torch.cat(env_id_batch, dim=0)\\n\\n        return data, target\";\n",
       "                var nbb_formatted_code = \"class ResidueEnvironment:\\n    \\\"\\\"\\\"\\n    Residue environment class used to hold necessarry information about the\\n    atoms of the environment such as atomic coordinates, atom types and the\\n    class of the missing central amino acid.\\n\\n    Parameters\\n    ----------\\n    xyz_coords: np.ndarray\\n        Numpy array with shape (n_atoms, 3) containing the x, y, z coordinates.\\n    atom_types: np.ndarray\\n        1D numpy array containing the atom types. Integer values in range(6).\\n    restypes_onehot: np.ndarray\\n        Numpy array with shape (n_atoms, 21) containing the amino acid\\n        class of the missing amino acid\\n    chain_id: str\\n        Chain id associated to ResidueEnvironment object\\n    pdb_residue_number: int\\n        Residue number associated with the ResidueEnvironment object\\n    pdb_id: str\\n        PDBID associated with the ResidueEnvironment object\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        xyz_coords: np.ndarray,\\n        atom_types: np.ndarray,\\n        restypes_onehot: np.ndarray,\\n        chain_id: str,\\n        pdb_residue_number: int,\\n        pdb_id: str,\\n    ):\\n        self._xyz_coords = xyz_coords\\n        self._atom_types = atom_types\\n        self._restypes_onehot = restypes_onehot\\n        self._chain_id = chain_id\\n        self._pdb_residue_number = pdb_residue_number\\n        self._pdb_id = pdb_id\\n\\n    @property\\n    def xyz_coords(self):\\n        return self._xyz_coords\\n\\n    @property\\n    def atom_types(self):\\n        return self._atom_types\\n\\n    @property\\n    def restypes_onehot(self):\\n        return self._restypes_onehot\\n\\n    @property\\n    def chain_id(self):\\n        return self._chain_id\\n\\n    @property\\n    def pdb_residue_number(self):\\n        return self._pdb_residue_number\\n\\n    @property\\n    def pdb_id(self):\\n        return self._pdb_id\\n\\n    def __repr__(self):\\n        return (\\n            f\\\"<ResidueEnvironment with {self.xyz_coords.shape[0]} atoms. \\\\n\\\"\\n            f\\\"pdb_id: {self.pdb_id}, \\\"\\n            f\\\"chain_id: {self.chain_id}, \\\"\\n            f\\\"pdb_residue_number: {self.pdb_residue_number},\\\\n\\\"\\n            f\\\"restype_one: {np.argmax(self.restypes_onehot)}>\\\"\\n        )\\n\\n\\nclass ResidueEnvironmentsDataset(Dataset):\\n    \\\"\\\"\\\"\\n    Residue environment dataset class\\n\\n    Parameters\\n    ----------\\n    input_data: Union[List[str], List[ResidueEnvironment]]\\n        List of parsed pdb filenames in .npz format or list of\\n        ResidueEnvironment objects\\n    transform: Callable\\n        A to-tensor transformer class\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        input_data: Union[List[str], List[ResidueEnvironment]],\\n        transformer: Callable = None,\\n    ):\\n        if all(isinstance(x, ResidueEnvironment) for x in input_data):\\n            self._res_env_objects = input_data\\n        elif all(isinstance(x, str) for x in input_data):\\n            self._res_env_objects = self._parse_envs(input_data)\\n        else:\\n            raise ValueError(\\n                \\\"Input data is not of type\\\" \\\"Union[List[str], List[ResidueEnvironment]]\\\"\\n            )\\n\\n        self.transformer = transformer\\n\\n    @property\\n    def res_env_objects(self):\\n        return self._res_env_objects\\n\\n    @property\\n    def transformer(self):\\n        return self._transformer\\n\\n    @transformer.setter\\n    def transformer(self, transformer):\\n        \\\"\\\"\\\"TODO: Think if a constraint to add later\\\"\\\"\\\"\\n        self._transformer = transformer\\n\\n    def __len__(self):\\n        return len(self.res_env_objects)\\n\\n    def __getitem__(self, idx):\\n        sample = self.res_env_objects[idx]\\n        if self.transformer:\\n            sample = self.transformer(sample)\\n        return sample\\n\\n    def _parse_envs(self, npz_filenames: List[str]) -> List[ResidueEnvironment]:\\n        \\\"\\\"\\\"\\n        TODO: Make this more readable\\n        \\\"\\\"\\\"\\n\\n        res_env_objects = []\\n        for i in range(len(npz_filenames)):\\n            coordinate_features = np.load(npz_filenames[i])\\n            atom_coords_prot_seq = coordinate_features[\\\"positions\\\"]\\n            restypes_onehots_prot_seq = coordinate_features[\\\"aa_onehot\\\"]\\n            selector_prot_seq = coordinate_features[\\\"selector\\\"]\\n            atom_types_flattened = coordinate_features[\\\"atom_types_numeric\\\"]\\n\\n            chain_ids = coordinate_features[\\\"chain_ids\\\"]\\n            pdb_residue_numbers = coordinate_features[\\\"residue_numbers\\\"]\\n            chain_boundary_indices = coordinate_features[\\\"chain_boundary_indices\\\"]\\n\\n            pdb_id = os.path.basename(npz_filenames[i])[0:4]\\n\\n            N_residues = selector_prot_seq.shape[0]\\n            for resi_i in range(N_residues):\\n                selector = selector_prot_seq[resi_i]\\n                selector_masked = selector[selector > -1]  # Remove Filler\\n                coords_mask = (\\n                    atom_coords_prot_seq[resi_i, :, 0] != -99.0\\n                )  # Remove filler\\n                coords = atom_coords_prot_seq[resi_i][coords_mask]\\n                atom_types = atom_types_flattened[selector_masked]\\n                restypes_onehot = restypes_onehots_prot_seq[resi_i]\\n\\n                pdb_residue_number = int(pdb_residue_numbers[resi_i])\\n                # Locate chain id\\n                for j in range(len(chain_ids)):\\n                    chain_boundary_0 = chain_boundary_indices[j]\\n                    chain_boundary_1 = chain_boundary_indices[j + 1]\\n                    if resi_i in range(chain_boundary_0, chain_boundary_1):\\n                        chain_id = str(chain_ids[j])\\n                        break\\n\\n                res_env_objects.append(\\n                    ResidueEnvironment(\\n                        coords,\\n                        atom_types,\\n                        restypes_onehot,\\n                        chain_id,\\n                        pdb_residue_number,\\n                        pdb_id,\\n                    )\\n                )\\n\\n        return res_env_objects\\n\\n\\nclass ToTensor:\\n    \\\"\\\"\\\" To-tensor transformer class\\\"\\\"\\\"\\n\\n    def __call__(self, sample: ResidueEnvironment):\\n        \\\"\\\"\\\"Converts single ResidueEnvironment object into x_ and y_\\\"\\\"\\\"\\n\\n        sample_env = np.hstack(\\n            [np.reshape(sample.atom_types, [-1, 1]), sample.xyz_coords]\\n        )\\n\\n        return {\\n            \\\"x_\\\": torch.tensor(sample_env, dtype=torch.float32).to(DEVICE),\\n            \\\"y_\\\": torch.tensor(\\n                np.array(sample.restypes_onehot), dtype=torch.float32\\n            ).to(DEVICE),\\n        }\\n\\n    @staticmethod\\n    def collate_cat(batch: List[ResidueEnvironment]):\\n        \\\"\\\"\\\"\\n        Collate method used by the dataloader to collate a\\n        batch of ResidueEnvironment objects.\\n        \\\"\\\"\\\"\\n        target = torch.cat([torch.unsqueeze(b[\\\"y_\\\"], 0) for b in batch], dim=0)\\n\\n        # To collate the input, we need to add a column which\\n        # specifies the environtment each atom belongs to\\n        env_id_batch = []\\n        for i, b in enumerate(batch):\\n            n_atoms = b[\\\"x_\\\"].shape[0]\\n            env_id_arr = torch.zeros(n_atoms, dtype=torch.float32).to(DEVICE) + i\\n            env_id_batch.append(\\n                torch.cat([torch.unsqueeze(env_id_arr, 1), b[\\\"x_\\\"]], dim=1)\\n            )\\n        data = torch.cat(env_id_batch, dim=0)\\n\\n        return data, target\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class ResidueEnvironment:\n",
    "    \"\"\"\n",
    "    Residue environment class used to hold necessarry information about the\n",
    "    atoms of the environment such as atomic coordinates, atom types and the\n",
    "    class of the missing central amino acid.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xyz_coords: np.ndarray\n",
    "        Numpy array with shape (n_atoms, 3) containing the x, y, z coordinates.\n",
    "    atom_types: np.ndarray\n",
    "        1D numpy array containing the atom types. Integer values in range(6).\n",
    "    restypes_onehot: np.ndarray\n",
    "        Numpy array with shape (n_atoms, 21) containing the amino acid\n",
    "        class of the missing amino acid\n",
    "    chain_id: str\n",
    "        Chain id associated to ResidueEnvironment object\n",
    "    pdb_residue_number: int\n",
    "        Residue number associated with the ResidueEnvironment object\n",
    "    pdb_id: str\n",
    "        PDBID associated with the ResidueEnvironment object\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        xyz_coords: np.ndarray,\n",
    "        atom_types: np.ndarray,\n",
    "        restypes_onehot: np.ndarray,\n",
    "        chain_id: str,\n",
    "        pdb_residue_number: int,\n",
    "        pdb_id: str,\n",
    "    ):\n",
    "        self._xyz_coords = xyz_coords\n",
    "        self._atom_types = atom_types\n",
    "        self._restypes_onehot = restypes_onehot\n",
    "        self._chain_id = chain_id\n",
    "        self._pdb_residue_number = pdb_residue_number\n",
    "        self._pdb_id = pdb_id\n",
    "\n",
    "    @property\n",
    "    def xyz_coords(self):\n",
    "        return self._xyz_coords\n",
    "\n",
    "    @property\n",
    "    def atom_types(self):\n",
    "        return self._atom_types\n",
    "\n",
    "    @property\n",
    "    def restypes_onehot(self):\n",
    "        return self._restypes_onehot\n",
    "\n",
    "    @property\n",
    "    def chain_id(self):\n",
    "        return self._chain_id\n",
    "\n",
    "    @property\n",
    "    def pdb_residue_number(self):\n",
    "        return self._pdb_residue_number\n",
    "\n",
    "    @property\n",
    "    def pdb_id(self):\n",
    "        return self._pdb_id\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"<ResidueEnvironment with {self.xyz_coords.shape[0]} atoms. \\n\"\n",
    "            f\"pdb_id: {self.pdb_id}, \"\n",
    "            f\"chain_id: {self.chain_id}, \"\n",
    "            f\"pdb_residue_number: {self.pdb_residue_number},\\n\"\n",
    "            f\"restype_one: {np.argmax(self.restypes_onehot)}>\"\n",
    "        )\n",
    "\n",
    "\n",
    "class ResidueEnvironmentsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Residue environment dataset class\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data: Union[List[str], List[ResidueEnvironment]]\n",
    "        List of parsed pdb filenames in .npz format or list of\n",
    "        ResidueEnvironment objects\n",
    "    transform: Callable\n",
    "        A to-tensor transformer class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_data: Union[List[str], List[ResidueEnvironment]],\n",
    "        transformer: Callable = None,\n",
    "    ):\n",
    "        if all(isinstance(x, ResidueEnvironment) for x in input_data):\n",
    "            self._res_env_objects = input_data\n",
    "        elif all(isinstance(x, str) for x in input_data):\n",
    "            self._res_env_objects = self._parse_envs(input_data)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Input data is not of type\" \"Union[List[str], List[ResidueEnvironment]]\"\n",
    "            )\n",
    "\n",
    "        self.transformer = transformer\n",
    "\n",
    "    @property\n",
    "    def res_env_objects(self):\n",
    "        return self._res_env_objects\n",
    "\n",
    "    @property\n",
    "    def transformer(self):\n",
    "        return self._transformer\n",
    "\n",
    "    @transformer.setter\n",
    "    def transformer(self, transformer):\n",
    "        \"\"\"TODO: Think if a constraint to add later\"\"\"\n",
    "        self._transformer = transformer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.res_env_objects)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.res_env_objects[idx]\n",
    "        if self.transformer:\n",
    "            sample = self.transformer(sample)\n",
    "        return sample\n",
    "\n",
    "    def _parse_envs(self, npz_filenames: List[str]) -> List[ResidueEnvironment]:\n",
    "        \"\"\"\n",
    "        TODO: Make this more readable\n",
    "        \"\"\"\n",
    "\n",
    "        res_env_objects = []\n",
    "        for i in range(len(npz_filenames)):\n",
    "            coordinate_features = np.load(npz_filenames[i])\n",
    "            atom_coords_prot_seq = coordinate_features[\"positions\"]\n",
    "            restypes_onehots_prot_seq = coordinate_features[\"aa_onehot\"]\n",
    "            selector_prot_seq = coordinate_features[\"selector\"]\n",
    "            atom_types_flattened = coordinate_features[\"atom_types_numeric\"]\n",
    "\n",
    "            chain_ids = coordinate_features[\"chain_ids\"]\n",
    "            pdb_residue_numbers = coordinate_features[\"residue_numbers\"]\n",
    "            chain_boundary_indices = coordinate_features[\"chain_boundary_indices\"]\n",
    "\n",
    "            pdb_id = os.path.basename(npz_filenames[i])[0:4]\n",
    "\n",
    "            N_residues = selector_prot_seq.shape[0]\n",
    "            for resi_i in range(N_residues):\n",
    "                selector = selector_prot_seq[resi_i]\n",
    "                selector_masked = selector[selector > -1]  # Remove Filler\n",
    "                coords_mask = (\n",
    "                    atom_coords_prot_seq[resi_i, :, 0] != -99.0\n",
    "                )  # Remove filler\n",
    "                coords = atom_coords_prot_seq[resi_i][coords_mask]\n",
    "                atom_types = atom_types_flattened[selector_masked]\n",
    "                restypes_onehot = restypes_onehots_prot_seq[resi_i]\n",
    "\n",
    "                pdb_residue_number = int(pdb_residue_numbers[resi_i])\n",
    "                # Locate chain id\n",
    "                for j in range(len(chain_ids)):\n",
    "                    chain_boundary_0 = chain_boundary_indices[j]\n",
    "                    chain_boundary_1 = chain_boundary_indices[j + 1]\n",
    "                    if resi_i in range(chain_boundary_0, chain_boundary_1):\n",
    "                        chain_id = str(chain_ids[j])\n",
    "                        break\n",
    "\n",
    "                res_env_objects.append(\n",
    "                    ResidueEnvironment(\n",
    "                        coords,\n",
    "                        atom_types,\n",
    "                        restypes_onehot,\n",
    "                        chain_id,\n",
    "                        pdb_residue_number,\n",
    "                        pdb_id,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return res_env_objects\n",
    "\n",
    "\n",
    "class ToTensor:\n",
    "    \"\"\" To-tensor transformer class\"\"\"\n",
    "\n",
    "    def __call__(self, sample: ResidueEnvironment):\n",
    "        \"\"\"Converts single ResidueEnvironment object into x_ and y_\"\"\"\n",
    "\n",
    "        sample_env = np.hstack(\n",
    "            [np.reshape(sample.atom_types, [-1, 1]), sample.xyz_coords]\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"x_\": torch.tensor(sample_env, dtype=torch.float32).to(DEVICE),\n",
    "            \"y_\": torch.tensor(\n",
    "                np.array(sample.restypes_onehot), dtype=torch.float32\n",
    "            ).to(DEVICE),\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_cat(batch: List[ResidueEnvironment]):\n",
    "        \"\"\"\n",
    "        Collate method used by the dataloader to collate a\n",
    "        batch of ResidueEnvironment objects.\n",
    "        \"\"\"\n",
    "        target = torch.cat([torch.unsqueeze(b[\"y_\"], 0) for b in batch], dim=0)\n",
    "\n",
    "        # To collate the input, we need to add a column which\n",
    "        # specifies the environtment each atom belongs to\n",
    "        env_id_batch = []\n",
    "        for i, b in enumerate(batch):\n",
    "            n_atoms = b[\"x_\"].shape[0]\n",
    "            env_id_arr = torch.zeros(n_atoms, dtype=torch.float32).to(DEVICE) + i\n",
    "            env_id_batch.append(\n",
    "                torch.cat([torch.unsqueeze(env_id_arr, 1), b[\"x_\"]], dim=1)\n",
    "            )\n",
    "        data = torch.cat(env_id_batch, dim=0)\n",
    "\n",
    "        return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"class CavityModel(torch.nn.Module):\\n    \\\"\\\"\\\"\\n    3D convolutional neural network to missing amino acid classification\\n\\n    Parameters\\n    ----------\\n    n_atom_types: int\\n        Number of atom types. (C, H, N, O, S, P)\\n    bins_per_angstrom: float\\n        Number of grid points per Anstrom.\\n    grid_dim: int\\n        Grid dimension\\n    sigma: float\\n        Standard deviation used for gaussian blurring\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        n_atom_types: int = 6,\\n        bins_per_angstrom: float = 1.0,\\n        grid_dim: int = 18,\\n        sigma: float = 0.6,\\n    ):\\n\\n        super().__init__()\\n\\n        self._n_atom_types = n_atom_types\\n        self._bins_per_angstrom = bins_per_angstrom\\n        self._grid_dim = grid_dim\\n        self._sigma = sigma\\n\\n        self._model()\\n\\n    @property\\n    def n_atom_types(self):\\n        return self._n_atom_types\\n\\n    @property\\n    def bins_per_angstrom(self):\\n        return self._bins_per_angstrom\\n\\n    @property\\n    def grid_dim(self):\\n        return self._grid_dim\\n\\n    @property\\n    def sigma(self):\\n        return self._sigma\\n\\n    @property\\n    def sigma_p(self):\\n        return self.sigma * self.bins_per_angstrom\\n\\n    @property\\n    def lin_spacing(self):\\n        lin_spacing = np.linspace(\\n            start=-self.grid_dim / 2 * self.bins_per_angstrom\\n            + self.bins_per_angstrom / 2,\\n            stop=self.grid_dim / 2 * self.bins_per_angstrom\\n            - self.bins_per_angstrom / 2,\\n            num=self.grid_dim,\\n        )\\n        return lin_spacing\\n\\n    def _model(self):\\n        self.xx, self.yy, self.zz = torch.tensor(\\n            np.meshgrid(\\n                self.lin_spacing, self.lin_spacing, self.lin_spacing, indexing=\\\"ij\\\"\\n            ),\\n            dtype=torch.float32,\\n        ).to(DEVICE)\\n\\n        self.conv1 = torch.nn.Sequential(\\n            torch.nn.Conv3d(6, 16, kernel_size=(3, 3, 3), stride=2, padding=1),\\n            torch.nn.ReLU(),\\n            torch.nn.BatchNorm3d(16),\\n        )\\n        self.conv2 = torch.nn.Sequential(\\n            torch.nn.Conv3d(16, 32, kernel_size=(3, 3, 3), stride=2, padding=0),\\n            torch.nn.ReLU(),\\n            torch.nn.BatchNorm3d(32),\\n        )\\n        self.conv3 = torch.nn.Sequential(\\n            torch.nn.Conv3d(32, 64, kernel_size=(3, 3, 3), stride=1, padding=1),\\n            torch.nn.ReLU(),\\n            torch.nn.BatchNorm3d(64),\\n            torch.nn.Flatten(),\\n        )\\n        self.dense1 = torch.nn.Sequential(\\n            torch.nn.Linear(in_features=4096, out_features=128),\\n            torch.nn.ReLU(),\\n            torch.nn.BatchNorm1d(128),\\n        )\\n        self.dense2 = torch.nn.Linear(in_features=128, out_features=21)\\n\\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\\n        x = self._gaussian_blurring(x)\\n        x = self.conv1(x)\\n        x = self.conv2(x)\\n        x = self.conv3(x)\\n        x = self.dense1(x)\\n        x = self.dense2(x)\\n        return x\\n\\n    def _gaussian_blurring(self, x: torch.Tensor) -> torch.Tensor:\\n        \\\"\\\"\\\"\\n        Method that takes 2d torch.Tensor describing the atoms of the batch.\\n\\n        Parameters\\n        ----------\\n        x: torch.Tensor\\n            Tensor for shape (n_atoms, 5). Each row represents an atom, where:\\n                column 0 describes the environment of the batch the\\n                atom belongs to\\n                column 1 describes the atom type\\n                column 2,3,4 are the x, y, z coordinates, respectively\\n\\n        Returns\\n        -------\\n        fields_torch: torch.Tensor\\n            Represents the structural environment with gaussian blurring\\n            and has shape (-1, self.grid_dim, self.grid_dim, self.grid_dim).\\n        \\\"\\\"\\\"\\n        current_batch_size = torch.unique(x[:, 0]).shape[0]\\n        fields_torch = torch.zeros(\\n            (\\n                current_batch_size,\\n                self.n_atom_types,\\n                self.grid_dim,\\n                self.grid_dim,\\n                self.grid_dim,\\n            )\\n        ).to(DEVICE)\\n        for j in range(self.n_atom_types):\\n            mask_j = x[:, 1] == j\\n            atom_type_j_data = x[mask_j]\\n            if atom_type_j_data.shape[0] > 0:\\n                pos = atom_type_j_data[:, 2:]\\n                density = torch.exp(\\n                    -(\\n                        (torch.reshape(self.xx, [-1, 1]) - pos[:, 0]) ** 2\\n                        + (torch.reshape(self.yy, [-1, 1]) - pos[:, 1]) ** 2\\n                        + (torch.reshape(self.zz, [-1, 1]) - pos[:, 2]) ** 2\\n                    )\\n                    / (2 * self.sigma_p ** 2)\\n                )\\n\\n                # Normalize each atom to 1\\n                density /= torch.sum(density, dim=0)\\n\\n                # Since column 0 of atom_type_j_data is sorted\\n                # I can use a trick to detect the boundaries based\\n                # on the change from one value to another.\\n                change_mask_j = (\\n                    atom_type_j_data[:, 0][:-1] != atom_type_j_data[:, 0][1:]\\n                )\\n\\n                # Add begin and end indices\\n                ranges_i = torch.cat(\\n                    [\\n                        torch.tensor([0]),\\n                        torch.arange(atom_type_j_data.shape[0] - 1)[change_mask_j] + 1,\\n                        torch.tensor([atom_type_j_data.shape[0]]),\\n                    ]\\n                )\\n\\n                # Fill tensor\\n                for i in range(ranges_i.shape[0]):\\n                    if i < ranges_i.shape[0] - 1:\\n                        index_0, index_1 = ranges_i[i], ranges_i[i + 1]\\n                        fields = torch.reshape(\\n                            torch.sum(density[:, index_0:index_1], dim=1),\\n                            [self.grid_dim, self.grid_dim, self.grid_dim],\\n                        )\\n                        fields_torch[i, j, :, :, :] = fields\\n        return fields_torch\";\n",
       "                var nbb_formatted_code = \"class CavityModel(torch.nn.Module):\\n    \\\"\\\"\\\"\\n    3D convolutional neural network to missing amino acid classification\\n\\n    Parameters\\n    ----------\\n    n_atom_types: int\\n        Number of atom types. (C, H, N, O, S, P)\\n    bins_per_angstrom: float\\n        Number of grid points per Anstrom.\\n    grid_dim: int\\n        Grid dimension\\n    sigma: float\\n        Standard deviation used for gaussian blurring\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        n_atom_types: int = 6,\\n        bins_per_angstrom: float = 1.0,\\n        grid_dim: int = 18,\\n        sigma: float = 0.6,\\n    ):\\n\\n        super().__init__()\\n\\n        self._n_atom_types = n_atom_types\\n        self._bins_per_angstrom = bins_per_angstrom\\n        self._grid_dim = grid_dim\\n        self._sigma = sigma\\n\\n        self._model()\\n\\n    @property\\n    def n_atom_types(self):\\n        return self._n_atom_types\\n\\n    @property\\n    def bins_per_angstrom(self):\\n        return self._bins_per_angstrom\\n\\n    @property\\n    def grid_dim(self):\\n        return self._grid_dim\\n\\n    @property\\n    def sigma(self):\\n        return self._sigma\\n\\n    @property\\n    def sigma_p(self):\\n        return self.sigma * self.bins_per_angstrom\\n\\n    @property\\n    def lin_spacing(self):\\n        lin_spacing = np.linspace(\\n            start=-self.grid_dim / 2 * self.bins_per_angstrom\\n            + self.bins_per_angstrom / 2,\\n            stop=self.grid_dim / 2 * self.bins_per_angstrom\\n            - self.bins_per_angstrom / 2,\\n            num=self.grid_dim,\\n        )\\n        return lin_spacing\\n\\n    def _model(self):\\n        self.xx, self.yy, self.zz = torch.tensor(\\n            np.meshgrid(\\n                self.lin_spacing, self.lin_spacing, self.lin_spacing, indexing=\\\"ij\\\"\\n            ),\\n            dtype=torch.float32,\\n        ).to(DEVICE)\\n\\n        self.conv1 = torch.nn.Sequential(\\n            torch.nn.Conv3d(6, 16, kernel_size=(3, 3, 3), stride=2, padding=1),\\n            torch.nn.ReLU(),\\n            torch.nn.BatchNorm3d(16),\\n        )\\n        self.conv2 = torch.nn.Sequential(\\n            torch.nn.Conv3d(16, 32, kernel_size=(3, 3, 3), stride=2, padding=0),\\n            torch.nn.ReLU(),\\n            torch.nn.BatchNorm3d(32),\\n        )\\n        self.conv3 = torch.nn.Sequential(\\n            torch.nn.Conv3d(32, 64, kernel_size=(3, 3, 3), stride=1, padding=1),\\n            torch.nn.ReLU(),\\n            torch.nn.BatchNorm3d(64),\\n            torch.nn.Flatten(),\\n        )\\n        self.dense1 = torch.nn.Sequential(\\n            torch.nn.Linear(in_features=4096, out_features=128),\\n            torch.nn.ReLU(),\\n            torch.nn.BatchNorm1d(128),\\n        )\\n        self.dense2 = torch.nn.Linear(in_features=128, out_features=21)\\n\\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\\n        x = self._gaussian_blurring(x)\\n        x = self.conv1(x)\\n        x = self.conv2(x)\\n        x = self.conv3(x)\\n        x = self.dense1(x)\\n        x = self.dense2(x)\\n        return x\\n\\n    def _gaussian_blurring(self, x: torch.Tensor) -> torch.Tensor:\\n        \\\"\\\"\\\"\\n        Method that takes 2d torch.Tensor describing the atoms of the batch.\\n\\n        Parameters\\n        ----------\\n        x: torch.Tensor\\n            Tensor for shape (n_atoms, 5). Each row represents an atom, where:\\n                column 0 describes the environment of the batch the\\n                atom belongs to\\n                column 1 describes the atom type\\n                column 2,3,4 are the x, y, z coordinates, respectively\\n\\n        Returns\\n        -------\\n        fields_torch: torch.Tensor\\n            Represents the structural environment with gaussian blurring\\n            and has shape (-1, self.grid_dim, self.grid_dim, self.grid_dim).\\n        \\\"\\\"\\\"\\n        current_batch_size = torch.unique(x[:, 0]).shape[0]\\n        fields_torch = torch.zeros(\\n            (\\n                current_batch_size,\\n                self.n_atom_types,\\n                self.grid_dim,\\n                self.grid_dim,\\n                self.grid_dim,\\n            )\\n        ).to(DEVICE)\\n        for j in range(self.n_atom_types):\\n            mask_j = x[:, 1] == j\\n            atom_type_j_data = x[mask_j]\\n            if atom_type_j_data.shape[0] > 0:\\n                pos = atom_type_j_data[:, 2:]\\n                density = torch.exp(\\n                    -(\\n                        (torch.reshape(self.xx, [-1, 1]) - pos[:, 0]) ** 2\\n                        + (torch.reshape(self.yy, [-1, 1]) - pos[:, 1]) ** 2\\n                        + (torch.reshape(self.zz, [-1, 1]) - pos[:, 2]) ** 2\\n                    )\\n                    / (2 * self.sigma_p ** 2)\\n                )\\n\\n                # Normalize each atom to 1\\n                density /= torch.sum(density, dim=0)\\n\\n                # Since column 0 of atom_type_j_data is sorted\\n                # I can use a trick to detect the boundaries based\\n                # on the change from one value to another.\\n                change_mask_j = (\\n                    atom_type_j_data[:, 0][:-1] != atom_type_j_data[:, 0][1:]\\n                )\\n\\n                # Add begin and end indices\\n                ranges_i = torch.cat(\\n                    [\\n                        torch.tensor([0]),\\n                        torch.arange(atom_type_j_data.shape[0] - 1)[change_mask_j] + 1,\\n                        torch.tensor([atom_type_j_data.shape[0]]),\\n                    ]\\n                )\\n\\n                # Fill tensor\\n                for i in range(ranges_i.shape[0]):\\n                    if i < ranges_i.shape[0] - 1:\\n                        index_0, index_1 = ranges_i[i], ranges_i[i + 1]\\n                        fields = torch.reshape(\\n                            torch.sum(density[:, index_0:index_1], dim=1),\\n                            [self.grid_dim, self.grid_dim, self.grid_dim],\\n                        )\\n                        fields_torch[i, j, :, :, :] = fields\\n        return fields_torch\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CavityModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    3D convolutional neural network to missing amino acid classification\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_atom_types: int\n",
    "        Number of atom types. (C, H, N, O, S, P)\n",
    "    bins_per_angstrom: float\n",
    "        Number of grid points per Anstrom.\n",
    "    grid_dim: int\n",
    "        Grid dimension\n",
    "    sigma: float\n",
    "        Standard deviation used for gaussian blurring\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_atom_types: int = 6,\n",
    "        bins_per_angstrom: float = 1.0,\n",
    "        grid_dim: int = 18,\n",
    "        sigma: float = 0.6,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self._n_atom_types = n_atom_types\n",
    "        self._bins_per_angstrom = bins_per_angstrom\n",
    "        self._grid_dim = grid_dim\n",
    "        self._sigma = sigma\n",
    "\n",
    "        self._model()\n",
    "\n",
    "    @property\n",
    "    def n_atom_types(self):\n",
    "        return self._n_atom_types\n",
    "\n",
    "    @property\n",
    "    def bins_per_angstrom(self):\n",
    "        return self._bins_per_angstrom\n",
    "\n",
    "    @property\n",
    "    def grid_dim(self):\n",
    "        return self._grid_dim\n",
    "\n",
    "    @property\n",
    "    def sigma(self):\n",
    "        return self._sigma\n",
    "\n",
    "    @property\n",
    "    def sigma_p(self):\n",
    "        return self.sigma * self.bins_per_angstrom\n",
    "\n",
    "    @property\n",
    "    def lin_spacing(self):\n",
    "        lin_spacing = np.linspace(\n",
    "            start=-self.grid_dim / 2 * self.bins_per_angstrom\n",
    "            + self.bins_per_angstrom / 2,\n",
    "            stop=self.grid_dim / 2 * self.bins_per_angstrom\n",
    "            - self.bins_per_angstrom / 2,\n",
    "            num=self.grid_dim,\n",
    "        )\n",
    "        return lin_spacing\n",
    "\n",
    "    def _model(self):\n",
    "        self.xx, self.yy, self.zz = torch.tensor(\n",
    "            np.meshgrid(\n",
    "                self.lin_spacing, self.lin_spacing, self.lin_spacing, indexing=\"ij\"\n",
    "            ),\n",
    "            dtype=torch.float32,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(6, 16, kernel_size=(3, 3, 3), stride=2, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm3d(16),\n",
    "        )\n",
    "        self.conv2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(16, 32, kernel_size=(3, 3, 3), stride=2, padding=0),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm3d(32),\n",
    "        )\n",
    "        self.conv3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(32, 64, kernel_size=(3, 3, 3), stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm3d(64),\n",
    "            torch.nn.Flatten(),\n",
    "        )\n",
    "        self.dense1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=4096, out_features=128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(128),\n",
    "        )\n",
    "        self.dense2 = torch.nn.Linear(in_features=128, out_features=21)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self._gaussian_blurring(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "\n",
    "    def _gaussian_blurring(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Method that takes 2d torch.Tensor describing the atoms of the batch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.Tensor\n",
    "            Tensor for shape (n_atoms, 5). Each row represents an atom, where:\n",
    "                column 0 describes the environment of the batch the\n",
    "                atom belongs to\n",
    "                column 1 describes the atom type\n",
    "                column 2,3,4 are the x, y, z coordinates, respectively\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        fields_torch: torch.Tensor\n",
    "            Represents the structural environment with gaussian blurring\n",
    "            and has shape (-1, self.grid_dim, self.grid_dim, self.grid_dim).\n",
    "        \"\"\"\n",
    "        current_batch_size = torch.unique(x[:, 0]).shape[0]\n",
    "        fields_torch = torch.zeros(\n",
    "            (\n",
    "                current_batch_size,\n",
    "                self.n_atom_types,\n",
    "                self.grid_dim,\n",
    "                self.grid_dim,\n",
    "                self.grid_dim,\n",
    "            )\n",
    "        ).to(DEVICE)\n",
    "        for j in range(self.n_atom_types):\n",
    "            mask_j = x[:, 1] == j\n",
    "            atom_type_j_data = x[mask_j]\n",
    "            if atom_type_j_data.shape[0] > 0:\n",
    "                pos = atom_type_j_data[:, 2:]\n",
    "                density = torch.exp(\n",
    "                    -(\n",
    "                        (torch.reshape(self.xx, [-1, 1]) - pos[:, 0]) ** 2\n",
    "                        + (torch.reshape(self.yy, [-1, 1]) - pos[:, 1]) ** 2\n",
    "                        + (torch.reshape(self.zz, [-1, 1]) - pos[:, 2]) ** 2\n",
    "                    )\n",
    "                    / (2 * self.sigma_p ** 2)\n",
    "                )\n",
    "\n",
    "                # Normalize each atom to 1\n",
    "                density /= torch.sum(density, dim=0)\n",
    "\n",
    "                # Since column 0 of atom_type_j_data is sorted\n",
    "                # I can use a trick to detect the boundaries based\n",
    "                # on the change from one value to another.\n",
    "                change_mask_j = (\n",
    "                    atom_type_j_data[:, 0][:-1] != atom_type_j_data[:, 0][1:]\n",
    "                )\n",
    "\n",
    "                # Add begin and end indices\n",
    "                ranges_i = torch.cat(\n",
    "                    [\n",
    "                        torch.tensor([0]),\n",
    "                        torch.arange(atom_type_j_data.shape[0] - 1)[change_mask_j] + 1,\n",
    "                        torch.tensor([atom_type_j_data.shape[0]]),\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                # Fill tensor\n",
    "                for i in range(ranges_i.shape[0]):\n",
    "                    if i < ranges_i.shape[0] - 1:\n",
    "                        index_0, index_1 = ranges_i[i], ranges_i[i + 1]\n",
    "                        fields = torch.reshape(\n",
    "                            torch.sum(density[:, index_0:index_1], dim=1),\n",
    "                            [self.grid_dim, self.grid_dim, self.grid_dim],\n",
    "                        )\n",
    "                        fields_torch[i, j, :, :, :] = fields\n",
    "        return fields_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse and train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data set includes 8 pdbs with 3725 environments.\n",
      "Validation data setincludes 2 pdbs with 1533 environments.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"parsed_pdb_filenames = sorted(glob.glob(\\\"data/pdbs/parsed/*coord*\\\"))\\nrandom.shuffle(parsed_pdb_filenames)\\n\\nn_train_pdbs = int(len(parsed_pdb_filenames) * TRAIN_VAL_SPLIT)\\nfilenames_train = parsed_pdb_filenames[:n_train_pdbs]\\nfilenames_val = parsed_pdb_filenames[n_train_pdbs:]\\n\\ndataset_train = ResidueEnvironmentsDataset(filenames_train, transformer=ToTensor())\\ndataset_val = ResidueEnvironmentsDataset(filenames_val, transformer=ToTensor())\\n\\ndataloader_train = DataLoader(\\n    dataset_train,\\n    batch_size=BATCH_SIZE,\\n    shuffle=True,\\n    collate_fn=ToTensor.collate_cat,\\n    drop_last=True,\\n)\\ndataloader_val = DataLoader(\\n    dataset_val,\\n    batch_size=BATCH_SIZE,\\n    shuffle=True,\\n    collate_fn=ToTensor.collate_cat,\\n    drop_last=True,\\n)\\n\\nprint(\\n    f\\\"Training data set includes {len(filenames_train)} pdbs with \\\"\\n    f\\\"{len(dataset_train)} environments.\\\"\\n)\\nprint(\\n    f\\\"Validation data setincludes {len(filenames_val)} pdbs with \\\"\\n    f\\\"{len(dataset_val)} environments.\\\"\\n)\";\n",
       "                var nbb_formatted_code = \"parsed_pdb_filenames = sorted(glob.glob(\\\"data/pdbs/parsed/*coord*\\\"))\\nrandom.shuffle(parsed_pdb_filenames)\\n\\nn_train_pdbs = int(len(parsed_pdb_filenames) * TRAIN_VAL_SPLIT)\\nfilenames_train = parsed_pdb_filenames[:n_train_pdbs]\\nfilenames_val = parsed_pdb_filenames[n_train_pdbs:]\\n\\ndataset_train = ResidueEnvironmentsDataset(filenames_train, transformer=ToTensor())\\ndataset_val = ResidueEnvironmentsDataset(filenames_val, transformer=ToTensor())\\n\\ndataloader_train = DataLoader(\\n    dataset_train,\\n    batch_size=BATCH_SIZE,\\n    shuffle=True,\\n    collate_fn=ToTensor.collate_cat,\\n    drop_last=True,\\n)\\ndataloader_val = DataLoader(\\n    dataset_val,\\n    batch_size=BATCH_SIZE,\\n    shuffle=True,\\n    collate_fn=ToTensor.collate_cat,\\n    drop_last=True,\\n)\\n\\nprint(\\n    f\\\"Training data set includes {len(filenames_train)} pdbs with \\\"\\n    f\\\"{len(dataset_train)} environments.\\\"\\n)\\nprint(\\n    f\\\"Validation data setincludes {len(filenames_val)} pdbs with \\\"\\n    f\\\"{len(dataset_val)} environments.\\\"\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parsed_pdb_filenames = sorted(glob.glob(\"data/pdbs/parsed/*coord*\"))\n",
    "random.shuffle(parsed_pdb_filenames)\n",
    "\n",
    "n_train_pdbs = int(len(parsed_pdb_filenames) * TRAIN_VAL_SPLIT)\n",
    "filenames_train = parsed_pdb_filenames[:n_train_pdbs]\n",
    "filenames_val = parsed_pdb_filenames[n_train_pdbs:]\n",
    "\n",
    "dataset_train = ResidueEnvironmentsDataset(filenames_train, transformer=ToTensor())\n",
    "dataset_val = ResidueEnvironmentsDataset(filenames_val, transformer=ToTensor())\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=ToTensor.collate_cat,\n",
    "    drop_last=True,\n",
    ")\n",
    "dataloader_val = DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=ToTensor.collate_cat,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Training data set includes {len(filenames_train)} pdbs with \"\n",
    "    f\"{len(dataset_train)} environments.\"\n",
    ")\n",
    "print(\n",
    "    f\"Validation data setincludes {len(filenames_val)} pdbs with \"\n",
    "    f\"{len(dataset_val)} environments.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1. Train loss: 2.492. Train Acc: 0.22. Val Acc: 0.11\n",
      "Epoch  2. Train loss: 1.487. Train Acc: 0.66. Val Acc: 0.22\n",
      "Epoch  3. Train loss: 0.960. Train Acc: 0.88. Val Acc: 0.25\n",
      "Epoch  4. Train loss: 0.559. Train Acc: 0.97. Val Acc: 0.24\n",
      "Epoch  5. Train loss: 0.296. Train Acc: 1.00. Val Acc: 0.23\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"def _train_step(\\n    cavity_model: CavityModel,\\n    optimizer: torch.optim.Adam,\\n    loss_function: torch.nn.CrossEntropyLoss,\\n) -> (torch.Tensor, float):\\n    cavity_model.train()\\n    optimizer.zero_grad()\\n    batch_y_pred = cavity_model(batch_x)\\n    loss_batch = loss_function(batch_y_pred, torch.argmax(batch_y, dim=-1))\\n    loss_batch.backward()\\n    optimizer.step()\\n    return (batch_y_pred, loss_batch.detach().cpu().item())\\n\\n\\n# Define model\\ncavity_model = CavityModel().to(DEVICE)\\nloss_function = torch.nn.CrossEntropyLoss()\\noptimizer = torch.optim.Adam(cavity_model.parameters(), lr=LEARNING_RATE)\\n\\n# Train loop\\nfor epoch in range(EPOCHS):\\n    loss_running_mean = 0.0\\n    labels_true = []\\n    labels_pred = []\\n    for batch_x, batch_y in dataloader_train:\\n        # Take train step\\n        batch_y_pred, loss_batch = _train_step(cavity_model, optimizer, loss_function)\\n\\n        # Exponential running mean for the loss\\n        loss_running_mean = loss_running_mean * 0.9 + loss_batch * 0.1\\n\\n        labels_true.append(torch.argmax(batch_y, dim=-1).detach().cpu().numpy())\\n        labels_pred.append(torch.argmax(batch_y_pred, dim=-1).detach().cpu().numpy())\\n    acc_train = np.mean((np.reshape(labels_true, -1) == np.reshape(labels_pred, -1)))\\n\\n    # Eval loop. Due to memory, we don't pass the whole data set to the model\\n    labels_true_val = []\\n    labels_pred_val = []\\n    for batch_x_val, batch_y_val in dataloader_val:\\n        cavity_model.eval()\\n        batch_y_pred_val = cavity_model(batch_x_val)\\n        labels_true_val.append(torch.argmax(batch_y_val, dim=-1).detach().cpu().numpy())\\n        labels_pred_val.append(\\n            torch.argmax(batch_y_pred_val, dim=-1).detach().cpu().numpy()\\n        )\\n    acc_val = np.mean(\\n        (np.reshape(labels_true_val, -1) == np.reshape(labels_pred_val, -1))\\n    )\\n\\n    print(\\n        f\\\"Epoch {epoch+1:2d}. Train loss: {loss_running_mean:5.3f}. \\\"\\n        f\\\"Train Acc: {acc_train:4.2f}. Val Acc: {acc_val:4.2f}\\\"\\n    )\";\n",
       "                var nbb_formatted_code = \"def _train_step(\\n    cavity_model: CavityModel,\\n    optimizer: torch.optim.Adam,\\n    loss_function: torch.nn.CrossEntropyLoss,\\n) -> (torch.Tensor, float):\\n    cavity_model.train()\\n    optimizer.zero_grad()\\n    batch_y_pred = cavity_model(batch_x)\\n    loss_batch = loss_function(batch_y_pred, torch.argmax(batch_y, dim=-1))\\n    loss_batch.backward()\\n    optimizer.step()\\n    return (batch_y_pred, loss_batch.detach().cpu().item())\\n\\n\\n# Define model\\ncavity_model = CavityModel().to(DEVICE)\\nloss_function = torch.nn.CrossEntropyLoss()\\noptimizer = torch.optim.Adam(cavity_model.parameters(), lr=LEARNING_RATE)\\n\\n# Train loop\\nfor epoch in range(EPOCHS):\\n    loss_running_mean = 0.0\\n    labels_true = []\\n    labels_pred = []\\n    for batch_x, batch_y in dataloader_train:\\n        # Take train step\\n        batch_y_pred, loss_batch = _train_step(cavity_model, optimizer, loss_function)\\n\\n        # Exponential running mean for the loss\\n        loss_running_mean = loss_running_mean * 0.9 + loss_batch * 0.1\\n\\n        labels_true.append(torch.argmax(batch_y, dim=-1).detach().cpu().numpy())\\n        labels_pred.append(torch.argmax(batch_y_pred, dim=-1).detach().cpu().numpy())\\n    acc_train = np.mean((np.reshape(labels_true, -1) == np.reshape(labels_pred, -1)))\\n\\n    # Eval loop. Due to memory, we don't pass the whole data set to the model\\n    labels_true_val = []\\n    labels_pred_val = []\\n    for batch_x_val, batch_y_val in dataloader_val:\\n        cavity_model.eval()\\n        batch_y_pred_val = cavity_model(batch_x_val)\\n        labels_true_val.append(torch.argmax(batch_y_val, dim=-1).detach().cpu().numpy())\\n        labels_pred_val.append(\\n            torch.argmax(batch_y_pred_val, dim=-1).detach().cpu().numpy()\\n        )\\n    acc_val = np.mean(\\n        (np.reshape(labels_true_val, -1) == np.reshape(labels_pred_val, -1))\\n    )\\n\\n    print(\\n        f\\\"Epoch {epoch+1:2d}. Train loss: {loss_running_mean:5.3f}. \\\"\\n        f\\\"Train Acc: {acc_train:4.2f}. Val Acc: {acc_val:4.2f}\\\"\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _train_step(\n",
    "    cavity_model: CavityModel,\n",
    "    optimizer: torch.optim.Adam,\n",
    "    loss_function: torch.nn.CrossEntropyLoss,\n",
    ") -> (torch.Tensor, float):\n",
    "    cavity_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    batch_y_pred = cavity_model(batch_x)\n",
    "    loss_batch = loss_function(batch_y_pred, torch.argmax(batch_y, dim=-1))\n",
    "    loss_batch.backward()\n",
    "    optimizer.step()\n",
    "    return (batch_y_pred, loss_batch.detach().cpu().item())\n",
    "\n",
    "\n",
    "# Define model\n",
    "cavity_model = CavityModel().to(DEVICE)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cavity_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_running_mean = 0.0\n",
    "    labels_true = []\n",
    "    labels_pred = []\n",
    "    for batch_x, batch_y in dataloader_train:\n",
    "        # Take train step\n",
    "        batch_y_pred, loss_batch = _train_step(cavity_model, optimizer, loss_function)\n",
    "\n",
    "        # Exponential running mean for the loss\n",
    "        loss_running_mean = loss_running_mean * 0.9 + loss_batch * 0.1\n",
    "\n",
    "        labels_true.append(torch.argmax(batch_y, dim=-1).detach().cpu().numpy())\n",
    "        labels_pred.append(torch.argmax(batch_y_pred, dim=-1).detach().cpu().numpy())\n",
    "    acc_train = np.mean((np.reshape(labels_true, -1) == np.reshape(labels_pred, -1)))\n",
    "\n",
    "    # Eval loop. Due to memory, we don't pass the whole data set to the model\n",
    "    labels_true_val = []\n",
    "    labels_pred_val = []\n",
    "    for batch_x_val, batch_y_val in dataloader_val:\n",
    "        cavity_model.eval()\n",
    "        batch_y_pred_val = cavity_model(batch_x_val)\n",
    "        labels_true_val.append(torch.argmax(batch_y_val, dim=-1).detach().cpu().numpy())\n",
    "        labels_pred_val.append(\n",
    "            torch.argmax(batch_y_pred_val, dim=-1).detach().cpu().numpy()\n",
    "        )\n",
    "    acc_val = np.mean(\n",
    "        (np.reshape(labels_true_val, -1) == np.reshape(labels_pred_val, -1))\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:2d}. Train loss: {loss_running_mean:5.3f}. \"\n",
    "        f\"Train Acc: {acc_train:4.2f}. Val Acc: {acc_val:4.2f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
