{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:70% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:70% !important; }</style>\"))\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidueEnvironmentDensity():\n",
    "    \"\"\"\n",
    "    Environment class, which contains all the relevant information on the structural environment\n",
    "    \"\"\"\n",
    "    def __init__(self, coords_2d_arr, atom_types, aa_onehot):\n",
    "        self.coords_2d_arr = coords_2d_arr\n",
    "        self.atom_types = atom_types\n",
    "        self.aa_onehot = aa_onehot\n",
    "    def get_coords_2d_arr(self):\n",
    "        return self.coords_2d_arr\n",
    "    def get_aa_onehot(self):\n",
    "        return self.aa_onehot\n",
    "    def get_atom_types(self):\n",
    "        return self.atom_types\n",
    "\n",
    "def parse_data(coordinate_features_filenames):\n",
    "    \"\"\"\n",
    "    Function that parses environment files (\"coordinate_features.npz\") and \n",
    "    returns ResidueEnvironmentDensity objects\n",
    "    \"\"\"\n",
    "    env_objects = []\n",
    "    for i in range(len(coordinate_features_filenames)):\n",
    "        coordinate_features = np.load(coordinate_features_filenames[i])\n",
    "\n",
    "        atom_coords_prot_seq = coordinate_features[\"positions\"]\n",
    "        restype_onehots_prot_seq = coordinate_features[\"aa_onehot\"]\n",
    "        selector_prot_seq = coordinate_features[\"selector\"]\n",
    "        atom_types_flattened = coordinate_features[\"atom_types_numeric\"]\n",
    "        # Loop over protein sequence\n",
    "        N_residues = selector_prot_seq.shape[0]\n",
    "        for resi_i in range(N_residues):\n",
    "            # Loop over surrounding atoms in the environment from the perspective of the residues\n",
    "            selector = selector_prot_seq[resi_i]\n",
    "            selector_masked = selector[ selector > -1 ] # Remove Filler\n",
    "            coords_mask = atom_coords_prot_seq[resi_i, :, 0] != -99.0 # To remove filler\n",
    "\n",
    "            # Relevant data\n",
    "            coords = atom_coords_prot_seq[resi_i][ coords_mask ]            \n",
    "            atom_types = atom_types_flattened[selector_masked]\n",
    "            restype_onehot = restype_onehots_prot_seq[resi_i]\n",
    "            env_objects.append(ResidueEnvironmentDensity(coords, atom_types, restype_onehot))            \n",
    "    return env_objects\n",
    "\n",
    "def get_batch_x_and_batch_y(res_env_obj_list):\n",
    "    \"\"\"\n",
    "    Function that takes a list of ResidueEnvironmentDensity objects and returns\n",
    "    batches with dimensions that match the requirements of the CNN for x and y\n",
    "    \"\"\"\n",
    "    batch_aa_onehots = []\n",
    "    batch_coords = []\n",
    "    batch_atom_types = []\n",
    "    for i, res_env_obj in enumerate(res_env_obj_list):\n",
    "        batch_aa_onehots.append(res_env_obj.get_aa_onehot())\n",
    "        batch_coords.append(res_env_obj.get_coords_2d_arr())\n",
    "        batch_atom_types.append(res_env_obj.get_atom_types())\n",
    "    \n",
    "    env_data_all = []\n",
    "    for env_i in range(len(batch_coords)):\n",
    "        n_atoms = np.array(batch_coords[env_i]).shape[0]\n",
    "        env_i_vector = np.zeros(n_atoms)+i \n",
    "        atom_types = batch_atom_types[env_i]\n",
    "        coords = batch_coords[env_i]\n",
    "        env_data = np.hstack( [np.reshape(np.zeros(n_atoms)+env_i, [-1, 1]), np.reshape(atom_types, [-1, 1]), coords] )\n",
    "        env_data_all.append(env_data)\n",
    "        \n",
    "    env_data_all_stacked = np.vstack(env_data_all)\n",
    "    return env_data_all_stacked, np.array(batch_aa_onehots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(torch.nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "    \n",
    "class AAPredConv3d(torch.nn.Module):\n",
    "    def __init__(self, device, sigma = 0.6):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.n_atom_types = 6\n",
    "        self.p = 1.0 # Bins pr. Anstrom\n",
    "        self.n = 18  # Grid dimension\n",
    "        self.sigma = sigma # width of gaussian\n",
    "        self.sigma_p = self.sigma*self.p\n",
    "        self.a = np.linspace(start = -self.n / 2 * self.p + self.p / 2, \n",
    "                             stop  = self.n / 2 * self.p - self.p / 2, \n",
    "                             num=self.n) # The torch equivalent does not work for this.\n",
    "        self.xx, self.yy, self.zz = torch.tensor(np.meshgrid(self.a, self.a, self.a, indexing=\"ij\"),\n",
    "                                                 dtype = torch.float32).to(self.device)\n",
    "        # Model \n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "                                         torch.nn.Conv3d(6, 16, kernel_size=(3,3,3), stride=2, padding=1),\n",
    "                                         torch.nn.ReLU(), \n",
    "                                         torch.nn.BatchNorm3d(16)\n",
    "                                        )\n",
    "        \n",
    "        self.conv2 = torch.nn.Sequential(\n",
    "                                         torch.nn.Conv3d(16, 32, kernel_size=(3,3,3), stride=2, padding=0),\n",
    "                                         torch.nn.ReLU(), \n",
    "                                         torch.nn.BatchNorm3d(32)\n",
    "                                        )\n",
    "        self.conv3 = torch.nn.Sequential(\n",
    "                                         torch.nn.Conv3d(32, 64, kernel_size=(3,3,3), stride=1, padding=1),\n",
    "                                         torch.nn.ReLU(), \n",
    "                                         torch.nn.BatchNorm3d(64),\n",
    "                                         Flatten()\n",
    "                                        )\n",
    "                                         \n",
    "        self.dense1 = torch.nn.Sequential(\n",
    "                                          torch.nn.Linear(in_features=4096, out_features=128),\n",
    "                                          torch.nn.ReLU(), \n",
    "                                          torch.nn.BatchNorm1d(128)\n",
    "                                         )\n",
    "        self.dense2 = torch.nn.Linear(in_features=128, out_features=21, bias = True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        current_batch_size = torch.unique(x[:, 0]).shape[0] # Used to build empty tensor fields_torch      \n",
    "        fields_torch = torch.zeros((current_batch_size, self.n_atom_types, self.n, self.n, self.n)).to(device)\n",
    "        \n",
    "        # This for-loop takes the atomic coordinates and places them in a grid\n",
    "        # with \"gaussian blurring\". The gaussian blurring gives a super-resolution effect\n",
    "        # compared to the standard binning of atom in a grid.\n",
    "        for j in range(self.n_atom_types):\n",
    "            mask_j = x[:,1]==j\n",
    "            atom_type_j_data = x[mask_j]\n",
    "            if atom_type_j_data.shape[0] > 0:\n",
    "                pos = atom_type_j_data[:, 2:]\n",
    "                density = torch.exp(-((torch.reshape(self.xx, [-1, 1]) - pos[:,0])**2 +\\\n",
    "                                      (torch.reshape(self.yy, [-1, 1]) - pos[:,1])**2 +\\\n",
    "                                      (torch.reshape(self.zz, [-1, 1]) - pos[:,2])**2) / (2 * self.sigma_p**2))\n",
    "                # Normalize each atom to 1\n",
    "                density /= torch.sum(density, dim=0)\n",
    "                # Since column 0 of atom_type_j_data is sorted\n",
    "                # I can use a trick to detect the boundaries based\n",
    "                # on the change from one value to another.\n",
    "                change_mask_j = (atom_type_j_data[:,0][:-1] != atom_type_j_data[:,0][1:]) # detect change in column 0\n",
    "                # Add begin- and end indices\n",
    "                ranges_i = torch.cat([torch.tensor([0]),\n",
    "                                      torch.arange(atom_type_j_data.shape[0]-1)[change_mask_j]+1, \n",
    "                                      torch.tensor([atom_type_j_data.shape[0]]) ])\n",
    "                for i in range(ranges_i.shape[0]):\n",
    "                    if i < ranges_i.shape[0] - 1:\n",
    "                        index_0, index_1 = ranges_i[i], ranges_i[i+1]\n",
    "                        fields = torch.reshape(torch.sum(density[:,index_0:index_1], dim = 1), \n",
    "                                               [self.n, self.n, self.n])\n",
    "                        fields_torch[i,j,:,:,:] = fields\n",
    "\n",
    "        # Convolutional layers\n",
    "        x = self.conv1(fields_torch)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        # Dense layers\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training environments: 101342\n",
      "Number of testing environments:  9658\n"
     ]
    }
   ],
   "source": [
    "# Parse environment files (one per protein)\n",
    "env_filenames_wild_card = \"data/transfer_learning_data/structural_environments/*coord*\"\n",
    "env_filenames = sorted(glob.glob(env_filenames_wild_card))\n",
    "random.shuffle(env_filenames) # sort for good measure\n",
    "env_filenames_train = env_filenames[:220] # proteins for training\n",
    "env_filenames_test = env_filenames[220:] #  proteins for testing\n",
    "\n",
    "env_objects_train = parse_data(env_filenames_train)\n",
    "env_objects_test = parse_data(env_filenames_test)\n",
    "print (\"Number of training environments:\", len(env_objects_train))\n",
    "print (\"Number of testing environments: \", len(env_objects_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/1, step      0, loss(train):  3.04, loss(test):  3.04, accuracy(train):  0.03, accuracy(test):  0.06\n",
      "epoch 1/1, step   1000, loss(train):  3.06, loss(test):  3.07, accuracy(train):  0.06, accuracy(test):  0.06\n",
      "epoch 1/1, step   2000, loss(train):  3.14, loss(test):  3.18, accuracy(train):  0.06, accuracy(test):  0.06\n",
      "epoch 1/1, step   3000, loss(train):  3.30, loss(test):  3.35, accuracy(train):  0.05, accuracy(test):  0.05\n",
      "epoch 1/1, step   4000, loss(train):  3.53, loss(test):  3.62, accuracy(train):  0.05, accuracy(test):  0.05\n",
      "epoch 1/1, step   5000, loss(train):  3.42, loss(test):  3.50, accuracy(train):  0.05, accuracy(test):  0.05\n",
      "epoch 1/1, step   6000, loss(train):  3.13, loss(test):  3.17, accuracy(train):  0.05, accuracy(test):  0.05\n",
      "epoch 1/1, step   7000, loss(train):  2.83, loss(test):  2.87, accuracy(train):  0.15, accuracy(test):  0.13\n",
      "epoch 1/1, step   8000, loss(train):  2.55, loss(test):  2.63, accuracy(train):  0.28, accuracy(test):  0.25\n",
      "epoch 1/1, step   9000, loss(train):  2.39, loss(test):  2.49, accuracy(train):  0.33, accuracy(test):  0.29\n",
      "epoch 1/1, step  10000, loss(train):  2.27, loss(test):  2.40, accuracy(train):  0.35, accuracy(test):  0.32\n",
      "epoch 1/1, step  11000, loss(train):  2.19, loss(test):  2.33, accuracy(train):  0.37, accuracy(test):  0.34\n",
      "epoch 1/1, step  12000, loss(train):  2.16, loss(test):  2.31, accuracy(train):  0.36, accuracy(test):  0.33\n",
      "epoch 1/1, step  13000, loss(train):  2.12, loss(test):  2.29, accuracy(train):  0.38, accuracy(test):  0.35\n",
      "epoch 1/1, step  14000, loss(train):  2.11, loss(test):  2.27, accuracy(train):  0.39, accuracy(test):  0.34\n",
      "epoch 1/1, step  15000, loss(train):  2.08, loss(test):  2.23, accuracy(train):  0.37, accuracy(test):  0.35\n",
      "epoch 1/1, step  16000, loss(train):  2.04, loss(test):  2.21, accuracy(train):  0.39, accuracy(test):  0.36\n",
      "epoch 1/1, step  17000, loss(train):  2.02, loss(test):  2.21, accuracy(train):  0.40, accuracy(test):  0.35\n",
      "epoch 1/1, step  18000, loss(train):  2.01, loss(test):  2.21, accuracy(train):  0.40, accuracy(test):  0.35\n",
      "epoch 1/1, step  19000, loss(train):  1.99, loss(test):  2.19, accuracy(train):  0.40, accuracy(test):  0.36\n",
      "epoch 1/1, step  20000, loss(train):  1.97, loss(test):  2.17, accuracy(train):  0.41, accuracy(test):  0.37\n",
      "epoch 1/1, step  21000, loss(train):  1.95, loss(test):  2.15, accuracy(train):  0.44, accuracy(test):  0.37\n",
      "epoch 1/1, step  22000, loss(train):  1.92, loss(test):  2.14, accuracy(train):  0.44, accuracy(test):  0.38\n",
      "epoch 1/1, step  23000, loss(train):  1.92, loss(test):  2.13, accuracy(train):  0.42, accuracy(test):  0.37\n",
      "epoch 1/1, step  24000, loss(train):  1.91, loss(test):  2.12, accuracy(train):  0.43, accuracy(test):  0.38\n",
      "epoch 1/1, step  25000, loss(train):  1.90, loss(test):  2.10, accuracy(train):  0.43, accuracy(test):  0.39\n",
      "epoch 1/1, step  26000, loss(train):  1.90, loss(test):  2.10, accuracy(train):  0.43, accuracy(test):  0.38\n",
      "epoch 1/1, step  27000, loss(train):  1.89, loss(test):  2.10, accuracy(train):  0.45, accuracy(test):  0.36\n",
      "epoch 1/1, step  28000, loss(train):  1.88, loss(test):  2.09, accuracy(train):  0.45, accuracy(test):  0.36\n",
      "epoch 1/1, step  29000, loss(train):  1.87, loss(test):  2.07, accuracy(train):  0.45, accuracy(test):  0.38\n",
      "epoch 1/1, step  30000, loss(train):  1.86, loss(test):  2.06, accuracy(train):  0.43, accuracy(test):  0.37\n",
      "epoch 1/1, step  31000, loss(train):  1.85, loss(test):  2.06, accuracy(train):  0.43, accuracy(test):  0.37\n",
      "epoch 1/1, step  32000, loss(train):  1.83, loss(test):  2.04, accuracy(train):  0.44, accuracy(test):  0.37\n",
      "epoch 1/1, step  33000, loss(train):  1.79, loss(test):  2.03, accuracy(train):  0.45, accuracy(test):  0.38\n",
      "epoch 1/1, step  34000, loss(train):  1.79, loss(test):  2.05, accuracy(train):  0.45, accuracy(test):  0.37\n",
      "epoch 1/1, step  35000, loss(train):  1.78, loss(test):  2.04, accuracy(train):  0.45, accuracy(test):  0.37\n",
      "epoch 1/1, step  36000, loss(train):  1.76, loss(test):  2.04, accuracy(train):  0.46, accuracy(test):  0.37\n",
      "epoch 1/1, step  37000, loss(train):  1.73, loss(test):  2.03, accuracy(train):  0.47, accuracy(test):  0.36\n",
      "epoch 1/1, step  38000, loss(train):  1.74, loss(test):  2.02, accuracy(train):  0.47, accuracy(test):  0.37\n",
      "epoch 1/1, step  39000, loss(train):  1.73, loss(test):  1.98, accuracy(train):  0.50, accuracy(test):  0.40\n",
      "epoch 1/1, step  40000, loss(train):  1.73, loss(test):  1.98, accuracy(train):  0.50, accuracy(test):  0.40\n",
      "epoch 1/1, step  41000, loss(train):  1.73, loss(test):  1.98, accuracy(train):  0.48, accuracy(test):  0.41\n",
      "epoch 1/1, step  42000, loss(train):  1.72, loss(test):  1.99, accuracy(train):  0.48, accuracy(test):  0.39\n",
      "epoch 1/1, step  43000, loss(train):  1.70, loss(test):  2.00, accuracy(train):  0.48, accuracy(test):  0.40\n",
      "epoch 1/1, step  44000, loss(train):  1.67, loss(test):  1.99, accuracy(train):  0.48, accuracy(test):  0.39\n",
      "epoch 1/1, step  45000, loss(train):  1.68, loss(test):  1.94, accuracy(train):  0.47, accuracy(test):  0.41\n",
      "epoch 1/1, step  46000, loss(train):  1.71, loss(test):  1.95, accuracy(train):  0.46, accuracy(test):  0.40\n",
      "epoch 1/1, step  47000, loss(train):  1.67, loss(test):  1.93, accuracy(train):  0.48, accuracy(test):  0.41\n",
      "epoch 1/1, step  48000, loss(train):  1.66, loss(test):  1.94, accuracy(train):  0.47, accuracy(test):  0.42\n",
      "epoch 1/1, step  49000, loss(train):  1.64, loss(test):  1.94, accuracy(train):  0.50, accuracy(test):  0.42\n",
      "epoch 1/1, step  50000, loss(train):  1.64, loss(test):  1.93, accuracy(train):  0.52, accuracy(test):  0.42\n",
      "epoch 1/1, step  51000, loss(train):  1.62, loss(test):  1.93, accuracy(train):  0.50, accuracy(test):  0.39\n",
      "epoch 1/1, step  52000, loss(train):  1.59, loss(test):  1.92, accuracy(train):  0.50, accuracy(test):  0.40\n",
      "epoch 1/1, step  53000, loss(train):  1.59, loss(test):  1.91, accuracy(train):  0.53, accuracy(test):  0.40\n",
      "epoch 1/1, step  54000, loss(train):  1.58, loss(test):  1.92, accuracy(train):  0.52, accuracy(test):  0.40\n",
      "epoch 1/1, step  55000, loss(train):  1.59, loss(test):  1.94, accuracy(train):  0.49, accuracy(test):  0.40\n",
      "epoch 1/1, step  56000, loss(train):  1.55, loss(test):  1.93, accuracy(train):  0.53, accuracy(test):  0.40\n",
      "epoch 1/1, step  57000, loss(train):  1.54, loss(test):  1.93, accuracy(train):  0.53, accuracy(test):  0.42\n",
      "epoch 1/1, step  58000, loss(train):  1.55, loss(test):  1.91, accuracy(train):  0.52, accuracy(test):  0.43\n",
      "epoch 1/1, step  59000, loss(train):  1.53, loss(test):  1.93, accuracy(train):  0.52, accuracy(test):  0.41\n",
      "epoch 1/1, step  60000, loss(train):  1.53, loss(test):  1.89, accuracy(train):  0.51, accuracy(test):  0.44\n",
      "epoch 1/1, step  61000, loss(train):  1.54, loss(test):  1.89, accuracy(train):  0.53, accuracy(test):  0.44\n",
      "epoch 1/1, step  62000, loss(train):  1.52, loss(test):  1.90, accuracy(train):  0.53, accuracy(test):  0.42\n",
      "epoch 1/1, step  63000, loss(train):  1.52, loss(test):  1.88, accuracy(train):  0.55, accuracy(test):  0.42\n",
      "epoch 1/1, step  64000, loss(train):  1.51, loss(test):  1.88, accuracy(train):  0.54, accuracy(test):  0.42\n",
      "epoch 1/1, step  65000, loss(train):  1.50, loss(test):  1.87, accuracy(train):  0.54, accuracy(test):  0.42\n",
      "epoch 1/1, step  66000, loss(train):  1.48, loss(test):  1.86, accuracy(train):  0.55, accuracy(test):  0.44\n",
      "epoch 1/1, step  67000, loss(train):  1.48, loss(test):  1.87, accuracy(train):  0.56, accuracy(test):  0.43\n",
      "epoch 1/1, step  68000, loss(train):  1.49, loss(test):  1.85, accuracy(train):  0.56, accuracy(test):  0.45\n",
      "epoch 1/1, step  69000, loss(train):  1.50, loss(test):  1.88, accuracy(train):  0.56, accuracy(test):  0.44\n",
      "epoch 1/1, step  70000, loss(train):  1.47, loss(test):  1.88, accuracy(train):  0.57, accuracy(test):  0.44\n",
      "epoch 1/1, step  71000, loss(train):  1.47, loss(test):  1.87, accuracy(train):  0.56, accuracy(test):  0.45\n",
      "epoch 1/1, step  72000, loss(train):  1.48, loss(test):  1.86, accuracy(train):  0.55, accuracy(test):  0.46\n",
      "epoch 1/1, step  73000, loss(train):  1.48, loss(test):  1.84, accuracy(train):  0.56, accuracy(test):  0.45\n",
      "epoch 1/1, step  74000, loss(train):  1.49, loss(test):  1.85, accuracy(train):  0.56, accuracy(test):  0.43\n",
      "epoch 1/1, step  75000, loss(train):  1.48, loss(test):  1.84, accuracy(train):  0.57, accuracy(test):  0.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/1, step  76000, loss(train):  1.47, loss(test):  1.84, accuracy(train):  0.56, accuracy(test):  0.43\n",
      "epoch 1/1, step  77000, loss(train):  1.45, loss(test):  1.83, accuracy(train):  0.57, accuracy(test):  0.43\n",
      "epoch 1/1, step  78000, loss(train):  1.42, loss(test):  1.80, accuracy(train):  0.57, accuracy(test):  0.44\n",
      "epoch 1/1, step  79000, loss(train):  1.44, loss(test):  1.83, accuracy(train):  0.57, accuracy(test):  0.45\n",
      "epoch 1/1, step  80000, loss(train):  1.42, loss(test):  1.82, accuracy(train):  0.57, accuracy(test):  0.45\n",
      "epoch 1/1, step  81000, loss(train):  1.42, loss(test):  1.84, accuracy(train):  0.57, accuracy(test):  0.45\n",
      "epoch 1/1, step  82000, loss(train):  1.41, loss(test):  1.83, accuracy(train):  0.58, accuracy(test):  0.44\n",
      "epoch 1/1, step  83000, loss(train):  1.41, loss(test):  1.84, accuracy(train):  0.59, accuracy(test):  0.43\n",
      "epoch 1/1, step  84000, loss(train):  1.40, loss(test):  1.81, accuracy(train):  0.58, accuracy(test):  0.44\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "device = \"cuda\"\n",
    "sigma = 0.6 # corresponds to the stdev of the \"gaussian blurring\" of the atoms\n",
    "batch_size = 100 # Number of environments\n",
    "learning_rate = 0.0003\n",
    "epochs = 1\n",
    "\n",
    "# Define model\n",
    "aa_pred_conv3d = AAPredConv3d(device, sigma = sigma).to(device)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(aa_pred_conv3d.parameters(), lr = learning_rate)\n",
    "random.shuffle(env_objects_train)\n",
    "\n",
    "# Define random subsets of training and testing sets for evaluation.\n",
    "# This makes it simpler to fit in a single forward pass (memory-wise), \n",
    "# instead of iterating over all the data\n",
    "sample_size = 500\n",
    "train_eval_inds = np.random.choice(np.arange(len(env_objects_train)), size = sample_size, replace = False)\n",
    "test_eval_inds = np.random.choice(np.arange(len(env_objects_test)), size = sample_size, replace = False)\n",
    "env_objects_train_eval = [env_objects_train[ind] for ind in train_eval_inds]\n",
    "env_objects_test_eval = [env_objects_test[ind] for ind in test_eval_inds]\n",
    "x_train_torch, y_train_torch = [torch.tensor(tens, dtype = torch.float32, requires_grad = False).to(device) for tens in get_batch_x_and_batch_y(env_objects_train_eval)]\n",
    "x_test_torch, y_test_torch = [torch.tensor(tens, dtype = torch.float32, requires_grad = False).to(device) for tens in get_batch_x_and_batch_y(env_objects_test_eval)]\n",
    "\n",
    "# Train\n",
    "random.shuffle(env_objects_train)\n",
    "loss_train_list = []\n",
    "loss_test_list = []\n",
    "acc_train_list = []\n",
    "acc_test_list = []\n",
    "for epoch_i in range(epochs):\n",
    "    for i in range(0, len(env_objects_train), batch_size):\n",
    "        if i+batch_size > len(env_objects_train): # skip last batch if it is smaller than batch size (small batch might mess up batch norm)\n",
    "            continue\n",
    "        # Define training batch\n",
    "        env_objs_batch_train = env_objects_train[i:i+batch_size]\n",
    "        batch_x_train, batch_y_train = [torch.tensor(tens, dtype = torch.float32).to(device) for tens in get_batch_x_and_batch_y(env_objs_batch_train)]\n",
    "\n",
    "        # Set the parameter gradients to zero\n",
    "        # Forward pass, backward pass, optimize\n",
    "        aa_pred_conv3d.train()\n",
    "        optimizer.zero_grad()    \n",
    "        batch_y_pred = aa_pred_conv3d(batch_x_train)\n",
    "        labels = torch.argmax(batch_y_train, dim = -1)\n",
    "        loss_batch_train = loss(batch_y_pred, labels)\n",
    "        loss_batch_train.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Evaluate on the big subset of training and testing environments\n",
    "        if i % 1000 == 0:\n",
    "            aa_pred_conv3d.eval()\n",
    "            \n",
    "            # Training eval\n",
    "            y_pred_train = aa_pred_conv3d(x_train_torch)\n",
    "            labels_train_pred = torch.argmax(y_pred_train, dim = -1)\n",
    "            labels_train = torch.argmax(y_train_torch, dim = -1)\n",
    "            loss_train = loss(y_pred_train, labels_train).item()\n",
    "            loss_train_list.append(loss_train)\n",
    "            accuracy_train = torch.mean((labels_train == labels_train_pred).double()).item()\n",
    "            acc_train_list.append(accuracy_train)\n",
    "            \n",
    "            # Testing eval\n",
    "            y_pred_test = aa_pred_conv3d(x_test_torch)\n",
    "            labels_test_pred = torch.argmax(y_pred_test, dim = -1)\n",
    "            labels_test = torch.argmax(y_test_torch, dim = -1)\n",
    "            loss_test = loss(y_pred_test, labels_test).item()\n",
    "            loss_test_list.append(loss_test)\n",
    "            accuracy_test = torch.mean((labels_test == labels_test_pred).double()).item()\n",
    "            acc_test_list.append(accuracy_test)\n",
    "            \n",
    "            print (\"epoch {:1d}/{:1d}, step {:6d}, loss(train): {:5.2f}, loss(test): {:5.2f}, accuracy(train): {:5.2f}, accuracy(test): {:5.2f}\".format(epoch_i+1, epochs, i, loss_train, loss_test, accuracy_train, accuracy_test))\n",
    "            \n",
    "    # shuffle after each epoch\n",
    "    random.shuffle(env_objects_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_arr = plt.subplots(2, sharex = True)\n",
    "ax_arr[0].plot(np.arange(len(loss_train_list)), loss_train_list, label = \"train\")\n",
    "ax_arr[0].plot(np.arange(len(loss_test_list)), loss_test_list, label = \"test\")\n",
    "ax_arr[1].plot(np.arange(len(acc_train_list)), acc_train_list, label = \"train\")\n",
    "ax_arr[1].plot(np.arange(len(acc_test_list)), acc_test_list, label = \"test\")\n",
    "ax_arr[0].set_ylabel(\"Cross Entropy loss\")\n",
    "ax_arr[1].set_ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "This demo demonstrates how to classify the missing amino acid from a structural environment where the atoms if the query amino acid have been purposefully removed.\n",
    "\n",
    "I train a simple Conv3D net to learn the shape/contact-pattern that the missing atoms have left behind.\n",
    "\n",
    "Given the very limited number of PDB structures (250) used in this demo, it is not surprising that we see much better performance on the training data compared to the testing data. Should you want to use the demo, please include more PDB structures. \n",
    "For reference, I use around 2k structures that have been homology reduces in sequence with much success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
